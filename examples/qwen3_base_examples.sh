#!/bin/bash
# Example commands for using Qwen3 models (both Base and post-trained versions)

echo "Qwen3 Model Examples"
echo "===================="
echo ""
echo "Available Qwen3 model versions:"
echo ""
echo "BASE MODELS (pretrained only - best for custom fine-tuning):"
echo "- Qwen/Qwen3-0.6B-Base (600M parameters)"
echo "- Qwen/Qwen3-1.7B-Base (1.7B parameters)"
echo "- Qwen/Qwen3-4B-Base (4B parameters)"
echo ""
echo "POST-TRAINED MODELS (already fine-tuned on diverse data):"
echo "- Qwen/Qwen3-0.6B (600M parameters)"
echo "- Qwen/Qwen3-1.7B (1.7B parameters)"
echo "- Qwen/Qwen3-4B (4B parameters)"
echo ""
echo "INSTRUCT MODEL (instruction-tuned):"
echo "- Qwen/Qwen3-4B-Instruct-2507"
echo ""
echo "Example training commands:"
echo ""

# Base model examples (pretrained only)
echo "# === BASE MODELS (Pretrained Only) ==="
echo "# Qwen3-0.6B-Base - best for custom domain adaptation"
echo "python train.py --model Qwen/Qwen3-0.6B-Base --dataset stanfordnlp/imdb --mode sft --use_lora --lora_r 4 --batch_size 16"
echo ""
echo "# Qwen3-1.7B-Base - balanced size for fine-tuning"
echo "python train.py --model Qwen/Qwen3-1.7B-Base --dataset openai/gsm8k --dataset_config main --mode sft --use_lora --lora_r 8 --batch_size 8"
echo ""
echo "# Qwen3-4B-Base - largest base model"
echo "python train.py --model Qwen/Qwen3-4B-Base --dataset virattt/financial-qa-10K --mode sft --use_lora --lora_r 16 --batch_size 4 --max_length 512"
echo ""

# Post-trained model examples
echo "# === POST-TRAINED MODELS (Already Fine-tuned) ==="
echo "# Qwen3-0.6B - may perform better on general tasks"
echo "python train.py --model Qwen/Qwen3-0.6B --dataset stanfordnlp/imdb --mode sft --use_lora --lora_r 4 --batch_size 16"
echo ""
echo "# Qwen3-1.7B - good starting point for task-specific fine-tuning"
echo "python train.py --model Qwen/Qwen3-1.7B --dataset glue --dataset_config sst2 --mode sft --use_lora --lora_r 8"
echo ""
echo "# Qwen3-4B - strongest general performance"
echo "python train.py --model Qwen/Qwen3-4B --dataset virattt/financial-qa-10K --mode sft --use_lora --lora_r 16 --batch_size 4"
echo ""

echo "# With Flash Attention (CUDA only)"
echo "python train.py --model Qwen/Qwen3-1.7B-Base --dataset stanfordnlp/imdb --mode sft --use_flash_attention --use_lora"
echo ""

echo "# With sequence packing for pretraining"
echo "python train.py --model Qwen/Qwen3-0.6B-Base --dataset wikitext --dataset_config wikitext-2-raw-v1 --mode pretrain --use_packing --max_length 2048"
echo ""

echo "# Multi-task fine-tuning"
echo "python train.py --model Qwen/Qwen3-1.7B-Base --datasets stanfordnlp/imdb glue --dataset_configs None sst2 --mixture_rates 0.6 0.4 --mode sft --use_lora"
echo ""

echo "# === CHOOSING THE RIGHT MODEL ==="
echo ""
echo "Use BASE models when:"
echo "- You want to fine-tune from scratch on your domain"
echo "- You have a specific task that differs from general language understanding"
echo "- You want maximum control over the training process"
echo ""
echo "Use POST-TRAINED models when:"
echo "- You want better zero-shot or few-shot performance"
echo "- Your task is similar to general language understanding"
echo "- You want to leverage existing diverse training"
echo ""
echo "Use INSTRUCT model (Qwen3-4B-Instruct-2507) when:"
echo "- You need strong instruction-following capabilities"
echo "- You're building a chatbot or assistant"
echo "- You want the best out-of-the-box performance"