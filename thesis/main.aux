\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\abx@aux@refcontext{nyt/global//global/global}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@cite{vaswani2017attention}
\abx@aux@segm{0}{0}{vaswani2017attention}
\abx@aux@cite{radford2019language}
\abx@aux@segm{0}{0}{radford2019language}
\abx@aux@cite{brown2020language}
\abx@aux@segm{0}{0}{brown2020language}
\abx@aux@cite{touvron2023llama}
\abx@aux@segm{0}{0}{touvron2023llama}
\abx@aux@cite{eu2016gdpr}
\abx@aux@segm{0}{0}{eu2016gdpr}
\abx@aux@cite{gururangan2020don}
\abx@aux@segm{0}{0}{gururangan2020don}
\abx@aux@cite{gao2020pile}
\abx@aux@segm{0}{0}{gao2020pile}
\abx@aux@cite{raffel2020exploring}
\abx@aux@segm{0}{0}{raffel2020exploring}
\abx@aux@cite{longpre2023pretrainer}
\abx@aux@segm{0}{0}{longpre2023pretrainer}
\abx@aux@cite{yang2024qwen2}
\abx@aux@segm{0}{0}{yang2024qwen2}
\abx@aux@cite{xia2023sheared}
\abx@aux@segm{0}{0}{xia2023sheared}
\abx@aux@cite{team2024gemma}
\abx@aux@segm{0}{0}{team2024gemma}
\abx@aux@cite{javaheripi2023phi}
\abx@aux@segm{0}{0}{javaheripi2023phi}
\abx@aux@cite{wu2023bloomberggpt}
\abx@aux@segm{0}{0}{wu2023bloomberggpt}
\abx@aux@cite{eu2016gdpr}
\abx@aux@segm{0}{0}{eu2016gdpr}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}\protected@file@percent }
\abx@aux@page{1}{1}
\abx@aux@page{2}{1}
\abx@aux@page{3}{1}
\abx@aux@page{4}{1}
\abx@aux@page{5}{1}
\abx@aux@page{6}{1}
\abx@aux@page{7}{1}
\abx@aux@page{8}{1}
\abx@aux@page{9}{1}
\abx@aux@page{10}{1}
\abx@aux@page{11}{1}
\abx@aux@page{12}{1}
\abx@aux@page{13}{1}
\abx@aux@page{14}{1}
\abx@aux@page{15}{1}
\abx@aux@cite{kaplan2020scaling}
\abx@aux@segm{0}{0}{kaplan2020scaling}
\abx@aux@cite{hoffmann2022training}
\abx@aux@segm{0}{0}{hoffmann2022training}
\abx@aux@cite{mccandlish2018empirical}
\abx@aux@segm{0}{0}{mccandlish2018empirical}
\abx@aux@page{16}{2}
\abx@aux@page{17}{2}
\abx@aux@page{18}{2}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.2}Research Questions}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.3}Contributions}{3}{section.1.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.4}Thesis Organization}{4}{section.1.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.5}Scope and Limitations}{5}{section.1.5}\protected@file@percent }
\abx@aux@cite{araci2019finbert}
\abx@aux@segm{0}{0}{araci2019finbert}
\abx@aux@cite{chen2021finqa}
\abx@aux@segm{0}{0}{chen2021finqa}
\abx@aux@cite{wu2023bloomberggpt}
\abx@aux@segm{0}{0}{wu2023bloomberggpt}
\abx@aux@cite{araci2019finbert}
\abx@aux@segm{0}{0}{araci2019finbert}
\abx@aux@cite{wu2023bloomberggpt}
\abx@aux@segm{0}{0}{wu2023bloomberggpt}
\abx@aux@cite{araci2019finbert}
\abx@aux@segm{0}{0}{araci2019finbert}
\abx@aux@cite{yang2020finbert}
\abx@aux@segm{0}{0}{yang2020finbert}
\abx@aux@cite{yang2023fingpt}
\abx@aux@segm{0}{0}{yang2023fingpt}
\abx@aux@cite{wu2023bloomberggpt}
\abx@aux@segm{0}{0}{wu2023bloomberggpt}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background and Related Work}{6}{chapter.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.1}Financial NLP}{6}{section.2.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Financial NLP Landscape}{6}{subsection.2.1.1}\protected@file@percent }
\abx@aux@page{19}{6}
\abx@aux@page{20}{6}
\abx@aux@page{21}{6}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Existing Financial Language Models}{6}{subsection.2.1.2}\protected@file@percent }
\abx@aux@page{22}{6}
\abx@aux@page{23}{6}
\abx@aux@page{24}{6}
\abx@aux@page{25}{6}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Domain-Specific Challenges}{6}{subsection.2.1.3}\protected@file@percent }
\abx@aux@page{26}{6}
\abx@aux@cite{radford2019language}
\abx@aux@segm{0}{0}{radford2019language}
\abx@aux@cite{brown2020language}
\abx@aux@segm{0}{0}{brown2020language}
\abx@aux@cite{vaswani2017attention}
\abx@aux@segm{0}{0}{vaswani2017attention}
\abx@aux@cite{touvron2023llama}
\abx@aux@segm{0}{0}{touvron2023llama}
\abx@aux@cite{kaplan2020scaling}
\abx@aux@segm{0}{0}{kaplan2020scaling}
\abx@aux@cite{hoffmann2022training}
\abx@aux@segm{0}{0}{hoffmann2022training}
\abx@aux@cite{tay2022ul2}
\abx@aux@segm{0}{0}{tay2022ul2}
\abx@aux@cite{mccandlish2018empirical}
\abx@aux@segm{0}{0}{mccandlish2018empirical}
\abx@aux@cite{rajbhandari2020zero}
\abx@aux@segm{0}{0}{rajbhandari2020zero}
\abx@aux@cite{kingma2014adam}
\abx@aux@segm{0}{0}{kingma2014adam}
\abx@aux@cite{narayanan2021efficient}
\abx@aux@segm{0}{0}{narayanan2021efficient}
\abx@aux@cite{hu2021lora}
\abx@aux@segm{0}{0}{hu2021lora}
\abx@aux@cite{bengio2009curriculum}
\abx@aux@segm{0}{0}{bengio2009curriculum}
\abx@aux@cite{wu2022opt}
\abx@aux@segm{0}{0}{wu2022opt}
\abx@aux@cite{longpre2023pretrainer}
\abx@aux@segm{0}{0}{longpre2023pretrainer}
\abx@aux@cite{raffel2020exploring}
\abx@aux@segm{0}{0}{raffel2020exploring}
\abx@aux@cite{wu2022opt}
\abx@aux@segm{0}{0}{wu2022opt}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.2}Language Model Pretraining}{7}{section.2.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Pretraining Objectives and Architecture}{7}{subsection.2.2.1}\protected@file@percent }
\abx@aux@page{27}{7}
\abx@aux@page{28}{7}
\abx@aux@page{29}{7}
\abx@aux@page{30}{7}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Scaling Laws and Model Size Effects}{7}{subsection.2.2.2}\protected@file@percent }
\abx@aux@page{31}{7}
\abx@aux@page{32}{7}
\abx@aux@page{33}{7}
\abx@aux@page{34}{7}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Computational and Memory Considerations}{7}{subsection.2.2.3}\protected@file@percent }
\abx@aux@page{35}{7}
\abx@aux@page{36}{7}
\abx@aux@page{37}{7}
\abx@aux@page{38}{7}
\abx@aux@cite{raffel2020exploring}
\abx@aux@segm{0}{0}{raffel2020exploring}
\abx@aux@cite{xie2023doremi}
\abx@aux@segm{0}{0}{xie2023doremi}
\abx@aux@cite{wu2023bloomberggpt}
\abx@aux@segm{0}{0}{wu2023bloomberggpt}
\abx@aux@cite{arivazhagan2019massively}
\abx@aux@segm{0}{0}{arivazhagan2019massively}
\abx@aux@cite{longpre2023pretrainer}
\abx@aux@segm{0}{0}{longpre2023pretrainer}
\abx@aux@cite{sanh2022multitask}
\abx@aux@segm{0}{0}{sanh2022multitask}
\abx@aux@cite{devlin2019bert}
\abx@aux@segm{0}{0}{devlin2019bert}
\abx@aux@cite{pan2010transfer}
\abx@aux@segm{0}{0}{pan2010transfer}
\abx@aux@cite{zhuang2020comprehensive}
\abx@aux@segm{0}{0}{zhuang2020comprehensive}
\abx@aux@cite{gururangan2020don}
\abx@aux@segm{0}{0}{gururangan2020don}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.3}Data Mixture Strategies}{8}{section.2.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Curriculum Learning and Sequential Mixing}{8}{subsection.2.3.1}\protected@file@percent }
\abx@aux@page{39}{8}
\abx@aux@page{40}{8}
\abx@aux@page{41}{8}
\abx@aux@page{42}{8}
\abx@aux@page{43}{8}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Simultaneous Mixture Approaches}{8}{subsection.2.3.2}\protected@file@percent }
\abx@aux@page{44}{8}
\abx@aux@page{45}{8}
\abx@aux@page{46}{8}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Domain Proportions and Sampling Strategies}{8}{subsection.2.3.3}\protected@file@percent }
\abx@aux@page{47}{8}
\abx@aux@page{48}{8}
\abx@aux@page{49}{8}
\abx@aux@cite{araci2019finbert}
\abx@aux@segm{0}{0}{araci2019finbert}
\abx@aux@cite{yang2020finbert}
\abx@aux@segm{0}{0}{yang2020finbert}
\abx@aux@cite{huang2023finbert}
\abx@aux@segm{0}{0}{huang2023finbert}
\abx@aux@cite{lee2022surgical}
\abx@aux@segm{0}{0}{lee2022surgical}
\abx@aux@cite{mccloskey1989catastrophic}
\abx@aux@segm{0}{0}{mccloskey1989catastrophic}
\abx@aux@cite{french1999catastrophic}
\abx@aux@segm{0}{0}{french1999catastrophic}
\abx@aux@cite{kirkpatrick2017overcoming}
\abx@aux@segm{0}{0}{kirkpatrick2017overcoming}
\abx@aux@cite{arivazhagan2019massively}
\abx@aux@segm{0}{0}{arivazhagan2019massively}
\abx@aux@cite{raffel2020exploring}
\abx@aux@segm{0}{0}{raffel2020exploring}
\abx@aux@cite{quinonero2009dataset}
\abx@aux@segm{0}{0}{quinonero2009dataset}
\abx@aux@cite{aharoni2020unsupervised}
\abx@aux@segm{0}{0}{aharoni2020unsupervised}
\abx@aux@cite{xie2023doremi}
\abx@aux@segm{0}{0}{xie2023doremi}
\abx@aux@cite{longpre2023pretrainer}
\abx@aux@segm{0}{0}{longpre2023pretrainer}
\abx@aux@cite{mitra2023orca2}
\abx@aux@segm{0}{0}{mitra2023orca2}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.4}Domain Adaptation and Transfer Learning}{9}{section.2.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Cross-Domain Transfer in Language Models}{9}{subsection.2.4.1}\protected@file@percent }
\abx@aux@page{50}{9}
\abx@aux@page{51}{9}
\abx@aux@page{52}{9}
\abx@aux@page{53}{9}
\abx@aux@page{54}{9}
\abx@aux@page{55}{9}
\abx@aux@page{56}{9}
\abx@aux@page{57}{9}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Catastrophic Forgetting and Stability}{9}{subsection.2.4.2}\protected@file@percent }
\abx@aux@page{58}{9}
\abx@aux@page{59}{9}
\abx@aux@page{60}{9}
\abx@aux@page{61}{9}
\abx@aux@page{62}{9}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Distribution Shift and Domain Mismatch}{9}{subsection.2.4.3}\protected@file@percent }
\abx@aux@page{63}{9}
\abx@aux@page{64}{9}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Related Empirical Studies}{10}{subsection.2.4.4}\protected@file@percent }
\abx@aux@page{65}{10}
\abx@aux@page{66}{10}
\abx@aux@page{67}{10}
\abx@aux@cite{yang2024qwen2}
\abx@aux@segm{0}{0}{yang2024qwen2}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{11}{chapter.3}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.1}Experimental Design Overview}{11}{section.3.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.2}Model Architecture}{12}{section.3.2}\protected@file@percent }
\abx@aux@page{68}{12}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.3}Datasets}{12}{section.3.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Financial Datasets}{12}{subsection.3.3.1}\protected@file@percent }
\abx@aux@cite{merity2016pointer}
\abx@aux@segm{0}{0}{merity2016pointer}
\abx@aux@cite{longpre2023pretrainer}
\abx@aux@segm{0}{0}{longpre2023pretrainer}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}WikiText}{13}{subsection.3.3.2}\protected@file@percent }
\abx@aux@page{69}{13}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Mixture Strategies}{13}{subsection.3.3.3}\protected@file@percent }
\abx@aux@page{70}{13}
\abx@aux@cite{kaplan2020scaling}
\abx@aux@segm{0}{0}{kaplan2020scaling}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.4}Training Setup and Hyperparameter Tuning}{14}{section.3.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Initial Configuration}{14}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Discovery of Reverse Scaling}{14}{subsection.3.4.2}\protected@file@percent }
\abx@aux@page{71}{14}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Systematic Learning Rate Adjustment}{15}{subsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Final Learning Rate Recommendations}{15}{subsection.3.4.4}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Learning rate recommendations by model size. Reduction factors follow approximate inverse square-root scaling relative to 0.6B baseline.\relax }}{15}{table.caption.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Other Hyperparameters}{15}{subsection.3.4.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.5}Evaluation Protocol}{16}{section.3.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Multi-Dataset Evaluation}{16}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Metrics}{16}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{CV computation details}{16}{section*.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{18}{chapter.4}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.1}Overview of Experimental Results}{18}{section.4.1}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Overview of 10 pretraining experiments. Perplexity reported for best-performing model size on the corresponding training dataset's test set. Epochs vary by model size to normalize token exposure ($\sim $100M tokens per model).\relax }}{18}{table.caption.8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:experiments_overview}{{4.1}{18}{Overview of 10 pretraining experiments. Perplexity reported for best-performing model size on the corresponding training dataset's test set. Epochs vary by model size to normalize token exposure ($\sim $100M tokens per model).\relax }{table.caption.8}{}}
\newlabel{tab:experiments_overview@cref}{{[table][1][4]4.1}{[1][18][]18}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.2}Data Mixture Effects: The Core Finding}{19}{section.4.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Mixed Financial Datasets}{19}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Mixed Wiki+Financial}{19}{subsection.4.2.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Mixed Financial Dataset: Model scaling behavior across 0.6B, 1.7B, and 4B parameters. Left panel shows perplexity (log scale) decreasing consistently with model size. Right panel shows cross-entropy loss following expected scaling pattern. Both metrics demonstrate normal scaling with 22.6\% total improvement from 0.6B to 4B.\relax }}{20}{figure.caption.9}\protected@file@percent }
\newlabel{fig:scaling_mixed_financial}{{4.1}{20}{Mixed Financial Dataset: Model scaling behavior across 0.6B, 1.7B, and 4B parameters. Left panel shows perplexity (log scale) decreasing consistently with model size. Right panel shows cross-entropy loss following expected scaling pattern. Both metrics demonstrate normal scaling with 22.6\% total improvement from 0.6B to 4B.\relax }{figure.caption.9}{}}
\newlabel{fig:scaling_mixed_financial@cref}{{[figure][1][4]4.1}{[1][19][]20}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Pure WikiText Baseline}{20}{subsection.4.2.3}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Mixed Financial Dataset: Evaluation Across Multiple Datasets\relax }}{21}{table.caption.10}\protected@file@percent }
\newlabel{tab:mixed_financial_results}{{4.2}{21}{Mixed Financial Dataset: Evaluation Across Multiple Datasets\relax }{table.caption.10}{}}
\newlabel{tab:mixed_financial_results@cref}{{[table][2][4]4.2}{[1][19][]21}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Key Takeaway}{21}{subsection.4.2.4}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Mixed Wiki+Financial Dataset: Scaling behavior shows normal pattern but with higher perplexity than pure financial mixture. The 15.1\% total improvement (0.6B to 4B) is smaller than pure financial (22.6\%), suggesting domain mixture creates competing optimization pressures that limit scaling benefits.\relax }}{22}{figure.caption.11}\protected@file@percent }
\newlabel{fig:scaling_mixed_wiki_financial}{{4.2}{22}{Mixed Wiki+Financial Dataset: Scaling behavior shows normal pattern but with higher perplexity than pure financial mixture. The 15.1\% total improvement (0.6B to 4B) is smaller than pure financial (22.6\%), suggesting domain mixture creates competing optimization pressures that limit scaling benefits.\relax }{figure.caption.11}{}}
\newlabel{fig:scaling_mixed_wiki_financial@cref}{{[figure][2][4]4.2}{[1][20][]22}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces WikiText Dataset: Severe reverse scaling phenomenon. The 1.7B model shows adjusted learning rate results (dashed line, squares) after fixing training collapse. The 4B model required 75\% LR reduction to stabilize. Clean, structured data amplifies learning rate sensitivity at larger scales.\relax }}{22}{figure.caption.13}\protected@file@percent }
\newlabel{fig:scaling_wikitext}{{4.3}{22}{WikiText Dataset: Severe reverse scaling phenomenon. The 1.7B model shows adjusted learning rate results (dashed line, squares) after fixing training collapse. The 4B model required 75\% LR reduction to stabilize. Clean, structured data amplifies learning rate sensitivity at larger scales.\relax }{figure.caption.13}{}}
\newlabel{fig:scaling_wikitext@cref}{{[figure][3][4]4.3}{[1][21][]22}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.3}Individual Dataset Analysis: Component Effects}{22}{section.4.3}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Mixed Wiki+Financial Dataset: Evaluation Across Multiple Datasets\relax }}{23}{table.caption.12}\protected@file@percent }
\newlabel{tab:mixed_wiki_financial_results}{{4.3}{23}{Mixed Wiki+Financial Dataset: Evaluation Across Multiple Datasets\relax }{table.caption.12}{}}
\newlabel{tab:mixed_wiki_financial_results@cref}{{[table][3][4]4.3}{[1][20][]23}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Large Datasets}{23}{subsection.4.3.1}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces WikiText Dataset: Impact of Learning Rate Adjustments\relax }}{24}{table.caption.14}\protected@file@percent }
\newlabel{tab:wikitext_lr_comparison}{{4.4}{24}{WikiText Dataset: Impact of Learning Rate Adjustments\relax }{table.caption.14}{}}
\newlabel{tab:wikitext_lr_comparison@cref}{{[table][4][4]4.4}{[1][21][]24}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Comparison of all three mixture strategies across model sizes. Mixed Financial (blue) consistently outperforms Mixed Wiki+Financial (orange) and WikiText (green) on financial evaluation metrics. The divergence increases with model size, demonstrating that in-domain diversity scales better than general-domain quality.\relax }}{24}{figure.caption.15}\protected@file@percent }
\newlabel{fig:scaling_comparison_all}{{4.4}{24}{Comparison of all three mixture strategies across model sizes. Mixed Financial (blue) consistently outperforms Mixed Wiki+Financial (orange) and WikiText (green) on financial evaluation metrics. The divergence increases with model size, demonstrating that in-domain diversity scales better than general-domain quality.\relax }{figure.caption.15}{}}
\newlabel{fig:scaling_comparison_all@cref}{{[figure][4][4]4.4}{[1][22][]24}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Financial News Articles Dataset: Excellent normal scaling with 21.7\% total improvement (0.6B to 4B). Large dataset size (197M tokens) provides sufficient diversity for stable training across all model sizes with minimal overtraining (2-3 epochs).\relax }}{25}{figure.caption.16}\protected@file@percent }
\newlabel{fig:scaling_news_articles}{{4.5}{25}{Financial News Articles Dataset: Excellent normal scaling with 21.7\% total improvement (0.6B to 4B). Large dataset size (197M tokens) provides sufficient diversity for stable training across all model sizes with minimal overtraining (2-3 epochs).\relax }{figure.caption.16}{}}
\newlabel{fig:scaling_news_articles@cref}{{[figure][5][4]4.5}{[1][24][]25}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces SEC Reports Dataset: Consistent normal scaling with 22.4\% total improvement. The 80M token corpus supports standalone pretraining with moderate overtraining (6-24 epochs). Strong transfer to similar long-form documents.\relax }}{25}{figure.caption.17}\protected@file@percent }
\newlabel{fig:scaling_sec_reports}{{4.6}{25}{SEC Reports Dataset: Consistent normal scaling with 22.4\% total improvement. The 80M token corpus supports standalone pretraining with moderate overtraining (6-24 epochs). Strong transfer to similar long-form documents.\relax }{figure.caption.17}{}}
\newlabel{fig:scaling_sec_reports@cref}{{[figure][6][4]4.6}{[1][24][]25}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Medium Datasets}{25}{subsection.4.3.2}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Financial News Dataset: Evaluation Across Multiple Datasets\relax }}{26}{table.caption.18}\protected@file@percent }
\newlabel{tab:news_articles_results}{{4.5}{26}{Financial News Dataset: Evaluation Across Multiple Datasets\relax }{table.caption.18}{}}
\newlabel{tab:news_articles_results@cref}{{[table][5][4]4.5}{[1][24][]26}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces SEC Reports Dataset: Evaluation Across Multiple Datasets\relax }}{27}{table.caption.19}\protected@file@percent }
\newlabel{tab:sec_reports_results}{{4.6}{27}{SEC Reports Dataset: Evaluation Across Multiple Datasets\relax }{table.caption.19}{}}
\newlabel{tab:sec_reports_results@cref}{{[table][6][4]4.6}{[1][24][]27}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Small Datasets}{27}{subsection.4.3.3}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces FinGPT Sentiment Dataset: Normal scaling with 22.1\% improvement despite moderate overtraining (12-30 epochs). Instruction-following format benefits from increased model capacity, showing strong transfer to similar task types.\relax }}{28}{figure.caption.20}\protected@file@percent }
\newlabel{fig:scaling_fingpt}{{4.7}{28}{FinGPT Sentiment Dataset: Normal scaling with 22.1\% improvement despite moderate overtraining (12-30 epochs). Instruction-following format benefits from increased model capacity, showing strong transfer to similar task types.\relax }{figure.caption.20}{}}
\newlabel{fig:scaling_fingpt@cref}{{[figure][7][4]4.7}{[1][26][]28}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Finance Alpaca Dataset: Consistent 21.8\% improvement across model sizes. Educational Q\&A format shows reliable scaling despite 13-25 epochs of training, but exhibits narrow task focus with 48\% cross-dataset variance.\relax }}{28}{figure.caption.21}\protected@file@percent }
\newlabel{fig:scaling_alpaca}{{4.8}{28}{Finance Alpaca Dataset: Consistent 21.8\% improvement across model sizes. Educational Q\&A format shows reliable scaling despite 13-25 epochs of training, but exhibits narrow task focus with 48\% cross-dataset variance.\relax }{figure.caption.21}{}}
\newlabel{fig:scaling_alpaca@cref}{{[figure][8][4]4.8}{[1][26][]28}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces FiQA Dataset: Strong normal scaling with 25.2\% total improvement. Despite small size (4M tokens), conversational Q\&A format produces stable training and excellent in-domain performance, though with high variance (52\%) on out-of-format tasks.\relax }}{29}{figure.caption.22}\protected@file@percent }
\newlabel{fig:scaling_fiqa}{{4.9}{29}{FiQA Dataset: Strong normal scaling with 25.2\% total improvement. Despite small size (4M tokens), conversational Q\&A format produces stable training and excellent in-domain performance, though with high variance (52\%) on out-of-format tasks.\relax }{figure.caption.22}{}}
\newlabel{fig:scaling_fiqa@cref}{{[figure][9][4]4.9}{[1][26][]29}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Financial QA 10K Dataset: Moderate reverse scaling resolved via learning rate adjustment. The 4B model (dashed line, squares) shows adjusted LR results with 10.3\% improvement, recovering expected scaling order. Extreme overtraining (67-100 epochs) causes 89\% cross-dataset variance.\relax }}{29}{figure.caption.26}\protected@file@percent }
\newlabel{fig:scaling_financial_qa}{{4.10}{29}{Financial QA 10K Dataset: Moderate reverse scaling resolved via learning rate adjustment. The 4B model (dashed line, squares) shows adjusted LR results with 10.3\% improvement, recovering expected scaling order. Extreme overtraining (67-100 epochs) causes 89\% cross-dataset variance.\relax }{figure.caption.26}{}}
\newlabel{fig:scaling_financial_qa@cref}{{[figure][10][4]4.10}{[1][28][]29}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Dataset Size vs Generalization}{29}{subsection.4.3.4}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces FinGPT Sentiment Dataset: Evaluation Across Multiple Datasets\relax }}{30}{table.caption.23}\protected@file@percent }
\newlabel{tab:fingpt_results}{{4.7}{30}{FinGPT Sentiment Dataset: Evaluation Across Multiple Datasets\relax }{table.caption.23}{}}
\newlabel{tab:fingpt_results@cref}{{[table][7][4]4.7}{[1][26][]30}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.4}Training Dynamics and Scaling Behavior}{30}{section.4.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Normal Scaling Pattern}{30}{subsection.4.4.1}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Finance Alpaca Dataset: Evaluation Across Multiple Datasets\relax }}{31}{table.caption.24}\protected@file@percent }
\newlabel{tab:alpaca_results}{{4.8}{31}{Finance Alpaca Dataset: Evaluation Across Multiple Datasets\relax }{table.caption.24}{}}
\newlabel{tab:alpaca_results@cref}{{[table][8][4]4.8}{[1][26][]31}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces FiQA Dataset: Evaluation Across Multiple Datasets\relax }}{32}{table.caption.25}\protected@file@percent }
\newlabel{tab:fiqa_results}{{4.9}{32}{FiQA Dataset: Evaluation Across Multiple Datasets\relax }{table.caption.25}{}}
\newlabel{tab:fiqa_results@cref}{{[table][9][4]4.9}{[1][26][]32}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Reverse Scaling Phenomenon}{32}{subsection.4.4.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Twitter Financial Sentiment Dataset: Severe reverse scaling phenomenon. The 4B model (dashed line, squares) required 75\% LR reduction to recover performance, achieving 31.6\% improvement. Extremely small dataset (0.3M tokens, 150-249 epochs) creates brittle optimization landscape with 89\% variance.\relax }}{33}{figure.caption.27}\protected@file@percent }
\newlabel{fig:scaling_twitter}{{4.11}{33}{Twitter Financial Sentiment Dataset: Severe reverse scaling phenomenon. The 4B model (dashed line, squares) required 75\% LR reduction to recover performance, achieving 31.6\% improvement. Extremely small dataset (0.3M tokens, 150-249 epochs) creates brittle optimization landscape with 89\% variance.\relax }{figure.caption.27}{}}
\newlabel{fig:scaling_twitter@cref}{{[figure][11][4]4.11}{[1][28][]33}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.10}{\ignorespaces Financial QA 10K Dataset: Impact of Learning Rate Adjustments\relax }}{33}{table.caption.28}\protected@file@percent }
\newlabel{tab:financial_qa_lr_comparison}{{4.10}{33}{Financial QA 10K Dataset: Impact of Learning Rate Adjustments\relax }{table.caption.28}{}}
\newlabel{tab:financial_qa_lr_comparison@cref}{{[table][10][4]4.10}{[1][28][]33}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.11}{\ignorespaces Twitter Financial Dataset: Impact of Learning Rate Adjustments\relax }}{34}{table.caption.29}\protected@file@percent }
\newlabel{tab:twitter_lr_comparison}{{4.11}{34}{Twitter Financial Dataset: Impact of Learning Rate Adjustments\relax }{table.caption.29}{}}
\newlabel{tab:twitter_lr_comparison@cref}{{[table][11][4]4.11}{[1][28][]34}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Learning Rate Sensitivity by Model Size}{34}{subsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Fixing Reverse Scaling}{35}{subsection.4.4.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.5}Model Stability Analysis}{36}{subsection.4.4.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.5}Domain Transfer and Generalization Patterns}{37}{section.4.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Cross-Dataset Evaluation}{37}{subsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Document Format and Task Type Effects}{38}{subsection.4.5.2}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.12}{\ignorespaces Financial News Evaluation: Performance Across Training Datasets\relax }}{39}{table.caption.30}\protected@file@percent }
\newlabel{tab:cross_financial_news}{{4.12}{39}{Financial News Evaluation: Performance Across Training Datasets\relax }{table.caption.30}{}}
\newlabel{tab:cross_financial_news@cref}{{[table][12][4]4.12}{[1][38][]39}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.13}{\ignorespaces SEC Reports Evaluation: Performance Across Training Datasets\relax }}{40}{table.caption.31}\protected@file@percent }
\newlabel{tab:cross_financial_repor}{{4.13}{40}{SEC Reports Evaluation: Performance Across Training Datasets\relax }{table.caption.31}{}}
\newlabel{tab:cross_financial_repor@cref}{{[table][13][4]4.13}{[1][38][]40}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Variance Comparison}{40}{subsection.4.5.3}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.14}{\ignorespaces Alpaca Evaluation: Performance Across Training Datasets\relax }}{41}{table.caption.32}\protected@file@percent }
\newlabel{tab:cross_alpaca}{{4.14}{41}{Alpaca Evaluation: Performance Across Training Datasets\relax }{table.caption.32}{}}
\newlabel{tab:cross_alpaca@cref}{{[table][14][4]4.14}{[1][39][]41}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.15}{\ignorespaces FinGPT Evaluation: Performance Across Training Datasets\relax }}{42}{table.caption.33}\protected@file@percent }
\newlabel{tab:cross_fingpt}{{4.15}{42}{FinGPT Evaluation: Performance Across Training Datasets\relax }{table.caption.33}{}}
\newlabel{tab:cross_fingpt@cref}{{[table][15][4]4.15}{[1][39][]42}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Domain-Specific vs General Knowledge Transfer}{42}{subsection.4.5.4}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.16}{\ignorespaces FiQA Evaluation: Performance Across Training Datasets\relax }}{43}{table.caption.34}\protected@file@percent }
\newlabel{tab:cross_fiqa}{{4.16}{43}{FiQA Evaluation: Performance Across Training Datasets\relax }{table.caption.34}{}}
\newlabel{tab:cross_fiqa@cref}{{[table][16][4]4.16}{[1][39][]43}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.17}{\ignorespaces Twitter Financial Evaluation: Performance Across Training Datasets\relax }}{44}{table.caption.35}\protected@file@percent }
\newlabel{tab:cross_twitter}{{4.17}{44}{Twitter Financial Evaluation: Performance Across Training Datasets\relax }{table.caption.35}{}}
\newlabel{tab:cross_twitter@cref}{{[table][17][4]4.17}{[1][39][]44}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.6}Summary and Key Results}{44}{section.4.6}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.18}{\ignorespaces Financial QA Evaluation: Performance Across Training Datasets\relax }}{45}{table.caption.36}\protected@file@percent }
\newlabel{tab:cross_financial_qa}{{4.18}{45}{Financial QA Evaluation: Performance Across Training Datasets\relax }{table.caption.36}{}}
\newlabel{tab:cross_financial_qa@cref}{{[table][18][4]4.18}{[1][41][]45}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.19}{\ignorespaces WikiText Evaluation: Performance Across Training Datasets\relax }}{46}{table.caption.37}\protected@file@percent }
\newlabel{tab:cross_wikitext}{{4.19}{46}{WikiText Evaluation: Performance Across Training Datasets\relax }{table.caption.37}{}}
\newlabel{tab:cross_wikitext@cref}{{[table][19][4]4.19}{[1][44][]46}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.20}{\ignorespaces Best configurations by application. *SEC's 18\% CV is in-domain only; cross-dataset CV is 32\%.\relax }}{46}{table.caption.38}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion}{48}{chapter.5}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.1}Key Empirical Findings}{48}{section.5.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.2}Interpretation of Data Interaction Effects}{49}{section.5.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Why WikiText Underperforms on Financial Tasks}{49}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Benefits of In-Domain Diversity}{50}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Domain Interference Patterns}{51}{subsection.5.2.3}\protected@file@percent }
\abx@aux@cite{kaplan2020scaling}
\abx@aux@segm{0}{0}{kaplan2020scaling}
\abx@aux@cite{hoffmann2022training}
\abx@aux@segm{0}{0}{hoffmann2022training}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Scale-Dependent Training Dynamics}{52}{subsection.5.2.4}\protected@file@percent }
\abx@aux@page{72}{52}
\abx@aux@page{73}{52}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.3}Practical Guidelines for Financial LM Pretraining}{52}{section.5.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Data Mixture Strategies by Use Case}{53}{subsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Model Size Selection}{53}{subsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Learning Rate Guidelines by Model Size}{54}{subsection.5.3.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}Token Budget Allocation}{54}{subsection.5.3.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.4}Limitations and Threats to Validity}{55}{section.5.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{56}{chapter.6}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.1}Summary of Contributions}{56}{section.6.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Data Mixture Guidelines for Financial NLP}{56}{subsection.6.1.1}\protected@file@percent }
\abx@aux@cite{kaplan2020scaling}
\abx@aux@segm{0}{0}{kaplan2020scaling}
\abx@aux@cite{hoffmann2022training}
\abx@aux@segm{0}{0}{hoffmann2022training}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Learning Rate Scaling Laws for Decoder-Only Transformers}{57}{subsection.6.1.2}\protected@file@percent }
\abx@aux@page{74}{57}
\abx@aux@page{75}{57}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Dataset Size Effects and Generalization}{57}{subsection.6.1.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Domain Transfer and Format Effects}{57}{subsection.6.1.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.5}Model Size Selection for Resource-Constrained Settings}{58}{subsection.6.1.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.6}Open-Source Reproducible Pipeline}{58}{subsection.6.1.6}\protected@file@percent }
\abx@aux@cite{kaplan2020scaling}
\abx@aux@segm{0}{0}{kaplan2020scaling}
\abx@aux@cite{hoffmann2022training}
\abx@aux@segm{0}{0}{hoffmann2022training}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.2}Implications for Practice and Research}{59}{section.6.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}For Practitioners: Actionable Deployment Guidelines}{59}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}For Researchers: Open Questions and Methodological Lessons}{59}{subsection.6.2.2}\protected@file@percent }
\abx@aux@page{76}{59}
\abx@aux@page{77}{59}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}For Industry: Privacy-Preserving Financial AI}{60}{subsection.6.2.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.3}Future Research Directions}{60}{section.6.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Scaling to Larger Models and Architectures}{60}{subsection.6.3.1}\protected@file@percent }
\abx@aux@cite{xie2023doremi}
\abx@aux@segm{0}{0}{xie2023doremi}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Advanced Mixture Optimization}{61}{subsection.6.3.2}\protected@file@percent }
\abx@aux@page{78}{61}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Comprehensive Downstream Evaluation}{61}{subsection.6.3.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Multi-Stage Pretraining Strategies}{61}{subsection.6.3.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.5}Theoretical Understanding of Learning Rate Scaling}{62}{subsection.6.3.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.4}Closing Remarks}{62}{section.6.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{Appendices}{}{section*.39}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {A}Experimental Details}{65}{appendix.A}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chp:experimental_details}{{A}{65}{Experimental Details}{appendix.A}{}}
\newlabel{chp:experimental_details@cref}{{[appendix][1][2147483647]A}{[1][65][]65}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {A.1}Complete Hyperparameter Tables}{65}{section.A.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {A.2}Additional Results Tables}{65}{section.A.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {A.3}Dataset Preprocessing Details}{65}{section.A.3}\protected@file@percent }
\abx@aux@page{79}{66}
\abx@aux@page{80}{66}
\abx@aux@page{81}{66}
\abx@aux@page{82}{66}
\abx@aux@page{83}{66}
\abx@aux@page{84}{66}
\abx@aux@page{85}{66}
\abx@aux@page{86}{66}
\abx@aux@page{87}{66}
\abx@aux@page{88}{66}
\abx@aux@page{89}{67}
\abx@aux@page{90}{67}
\abx@aux@page{91}{67}
\abx@aux@page{92}{67}
\abx@aux@page{93}{67}
\abx@aux@page{94}{67}
\abx@aux@page{95}{67}
\abx@aux@page{96}{67}
\abx@aux@page{97}{67}
\abx@aux@page{98}{67}
\abx@aux@page{99}{67}
\abx@aux@page{100}{67}
\abx@aux@page{101}{68}
\abx@aux@page{102}{68}
\abx@aux@page{103}{68}
\abx@aux@page{104}{68}
\abx@aux@page{105}{68}
\abx@aux@page{106}{68}
\abx@aux@page{107}{68}
\abx@aux@page{108}{68}
\abx@aux@page{109}{68}
\abx@aux@page{110}{68}
\abx@aux@page{111}{68}
\abx@aux@page{112}{68}
\abx@aux@page{113}{69}
\abx@aux@page{114}{69}
\abx@aux@page{115}{69}
\abx@aux@page{116}{69}
\abx@aux@page{117}{69}
\abx@aux@page{118}{69}
\abx@aux@page{119}{69}
\abx@aux@page{120}{69}
\abx@aux@read@bbl@mdfivesum{F8BB57610C837B7916C148AAC5336795}
\abx@aux@read@bblrerun
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{aharoni2020unsupervised}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{araci2019finbert}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{arivazhagan2019massively}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{bengio2009curriculum}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{brown2020language}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{devlin2019bert}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{french1999catastrophic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{gao2020pile}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{gururangan2020don}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{hoffmann2022training}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{hu2021lora}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{huang2023finbert}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{javaheripi2023phi}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{kaplan2020scaling}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{kingma2014adam}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{kirkpatrick2017overcoming}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{lee2022surgical}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{longpre2023pretrainer}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{mccandlish2018empirical}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{mccloskey1989catastrophic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{merity2016pointer}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{mitra2023orca2}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{narayanan2021efficient}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{pan2010transfer}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{quinonero2009dataset}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{radford2019language}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{raffel2020exploring}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{rajbhandari2020zero}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{eu2016gdpr}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{sanh2022multitask}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{tay2022ul2}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{team2024gemma}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{touvron2023llama}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{vaswani2017attention}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{wu2023bloomberggpt}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{xia2023sheared}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{xie2023doremi}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{yang2024qwen2}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{yang2023fingpt}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{yang2020finbert}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{wu2022opt}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{zhuang2020comprehensive}{nyt/global//global/global}
\gdef \@abspage@last{79}
