\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\abx@aux@refcontext{nyt/global//global/global}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@cite{vaswani2017attention}
\abx@aux@segm{0}{0}{vaswani2017attention}
\abx@aux@cite{radford2019language}
\abx@aux@segm{0}{0}{radford2019language}
\abx@aux@cite{brown2020language}
\abx@aux@segm{0}{0}{brown2020language}
\abx@aux@cite{touvron2023llama}
\abx@aux@segm{0}{0}{touvron2023llama}
\abx@aux@cite{eu2016gdpr}
\abx@aux@segm{0}{0}{eu2016gdpr}
\abx@aux@cite{gururangan2020don}
\abx@aux@segm{0}{0}{gururangan2020don}
\abx@aux@cite{gao2020pile}
\abx@aux@segm{0}{0}{gao2020pile}
\abx@aux@cite{raffel2020exploring}
\abx@aux@segm{0}{0}{raffel2020exploring}
\abx@aux@cite{longpre2023pretrainer}
\abx@aux@segm{0}{0}{longpre2023pretrainer}
\abx@aux@cite{yang2024qwen2}
\abx@aux@segm{0}{0}{yang2024qwen2}
\abx@aux@cite{xia2023sheared}
\abx@aux@segm{0}{0}{xia2023sheared}
\abx@aux@cite{wu2023bloomberggpt}
\abx@aux@segm{0}{0}{wu2023bloomberggpt}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}\protected@file@percent }
\abx@aux@page{1}{1}
\abx@aux@page{2}{1}
\abx@aux@page{3}{1}
\abx@aux@page{4}{1}
\abx@aux@page{5}{1}
\abx@aux@page{6}{1}
\abx@aux@page{7}{1}
\abx@aux@page{8}{1}
\abx@aux@page{9}{1}
\abx@aux@cite{eu2016gdpr}
\abx@aux@segm{0}{0}{eu2016gdpr}
\abx@aux@cite{kaplan2020scaling}
\abx@aux@segm{0}{0}{kaplan2020scaling}
\abx@aux@cite{hoffmann2022training}
\abx@aux@segm{0}{0}{hoffmann2022training}
\abx@aux@cite{mccandlish2018empirical}
\abx@aux@segm{0}{0}{mccandlish2018empirical}
\abx@aux@page{10}{2}
\abx@aux@page{11}{2}
\abx@aux@page{12}{2}
\abx@aux@page{13}{2}
\abx@aux@page{14}{2}
\abx@aux@page{15}{2}
\abx@aux@page{16}{2}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.2}Research Questions}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.3}Contributions}{4}{section.1.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.4}Thesis Organization}{6}{section.1.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1.5}Scope and Limitations}{7}{section.1.5}\protected@file@percent }
\abx@aux@cite{araci2019finbert}
\abx@aux@segm{0}{0}{araci2019finbert}
\abx@aux@cite{yang2020finqa}
\abx@aux@segm{0}{0}{yang2020finqa}
\abx@aux@cite{wu2023bloomberggpt}
\abx@aux@segm{0}{0}{wu2023bloomberggpt}
\abx@aux@cite{araci2019finbert}
\abx@aux@segm{0}{0}{araci2019finbert}
\abx@aux@cite{wu2023bloomberggpt}
\abx@aux@segm{0}{0}{wu2023bloomberggpt}
\abx@aux@cite{araci2019finbert}
\abx@aux@segm{0}{0}{araci2019finbert}
\abx@aux@cite{yang2020finbert}
\abx@aux@segm{0}{0}{yang2020finbert}
\abx@aux@cite{yang2023fingpt}
\abx@aux@segm{0}{0}{yang2023fingpt}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background and Related Work}{8}{chapter.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.1}Financial NLP}{8}{section.2.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Financial NLP Landscape}{8}{subsection.2.1.1}\protected@file@percent }
\abx@aux@page{17}{8}
\abx@aux@page{18}{8}
\abx@aux@page{19}{8}
\abx@aux@page{20}{8}
\abx@aux@cite{wu2023bloomberggpt}
\abx@aux@segm{0}{0}{wu2023bloomberggpt}
\abx@aux@cite{radford2019language}
\abx@aux@segm{0}{0}{radford2019language}
\abx@aux@cite{brown2020language}
\abx@aux@segm{0}{0}{brown2020language}
\abx@aux@cite{vaswani2017attention}
\abx@aux@segm{0}{0}{vaswani2017attention}
\abx@aux@cite{touvron2023llama}
\abx@aux@segm{0}{0}{touvron2023llama}
\abx@aux@cite{kaplan2020scaling}
\abx@aux@segm{0}{0}{kaplan2020scaling}
\abx@aux@cite{hoffmann2022training}
\abx@aux@segm{0}{0}{hoffmann2022training}
\abx@aux@cite{tay2022ul2}
\abx@aux@segm{0}{0}{tay2022ul2}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Existing Financial Language Models}{9}{subsection.2.1.2}\protected@file@percent }
\abx@aux@page{21}{9}
\abx@aux@page{22}{9}
\abx@aux@page{23}{9}
\abx@aux@page{24}{9}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Domain-Specific Challenges}{9}{subsection.2.1.3}\protected@file@percent }
\abx@aux@page{25}{9}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.2}Language Model Pretraining}{9}{section.2.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Pretraining Objectives and Architecture}{9}{subsection.2.2.1}\protected@file@percent }
\abx@aux@page{26}{9}
\abx@aux@page{27}{9}
\abx@aux@page{28}{9}
\abx@aux@page{29}{9}
\abx@aux@cite{mccandlish2018empirical}
\abx@aux@segm{0}{0}{mccandlish2018empirical}
\abx@aux@cite{rajbhandari2020zero}
\abx@aux@segm{0}{0}{rajbhandari2020zero}
\abx@aux@cite{narayanan2021efficient}
\abx@aux@segm{0}{0}{narayanan2021efficient}
\abx@aux@cite{bengio2009curriculum}
\abx@aux@segm{0}{0}{bengio2009curriculum}
\abx@aux@cite{wu2022opt}
\abx@aux@segm{0}{0}{wu2022opt}
\abx@aux@cite{longpre2023pretrainer}
\abx@aux@segm{0}{0}{longpre2023pretrainer}
\abx@aux@cite{raffel2020exploring}
\abx@aux@segm{0}{0}{raffel2020exploring}
\abx@aux@cite{wu2022opt}
\abx@aux@segm{0}{0}{wu2022opt}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Scaling Laws and Model Size Effects}{10}{subsection.2.2.2}\protected@file@percent }
\abx@aux@page{30}{10}
\abx@aux@page{31}{10}
\abx@aux@page{32}{10}
\abx@aux@page{33}{10}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Computational and Memory Considerations}{10}{subsection.2.2.3}\protected@file@percent }
\abx@aux@page{34}{10}
\abx@aux@page{35}{10}
\abx@aux@cite{raffel2020exploring}
\abx@aux@segm{0}{0}{raffel2020exploring}
\abx@aux@cite{xie2023doremi}
\abx@aux@segm{0}{0}{xie2023doremi}
\abx@aux@cite{wu2023bloomberggpt}
\abx@aux@segm{0}{0}{wu2023bloomberggpt}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.3}Data Mixture Strategies}{11}{section.2.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Curriculum Learning and Sequential Mixing}{11}{subsection.2.3.1}\protected@file@percent }
\abx@aux@page{36}{11}
\abx@aux@page{37}{11}
\abx@aux@page{38}{11}
\abx@aux@page{39}{11}
\abx@aux@page{40}{11}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Simultaneous Mixture Approaches}{11}{subsection.2.3.2}\protected@file@percent }
\abx@aux@page{41}{11}
\abx@aux@page{42}{11}
\abx@aux@page{43}{11}
\abx@aux@cite{arivazhagan2019massively}
\abx@aux@segm{0}{0}{arivazhagan2019massively}
\abx@aux@cite{longpre2023pretrainer}
\abx@aux@segm{0}{0}{longpre2023pretrainer}
\abx@aux@cite{sanh2022multitask}
\abx@aux@segm{0}{0}{sanh2022multitask}
\abx@aux@cite{devlin2019bert}
\abx@aux@segm{0}{0}{devlin2019bert}
\abx@aux@cite{gururangan2020don}
\abx@aux@segm{0}{0}{gururangan2020don}
\abx@aux@cite{araci2019finbert}
\abx@aux@segm{0}{0}{araci2019finbert}
\abx@aux@cite{yang2020finbert}
\abx@aux@segm{0}{0}{yang2020finbert}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Domain Proportions and Sampling Strategies}{12}{subsection.2.3.3}\protected@file@percent }
\abx@aux@page{44}{12}
\abx@aux@page{45}{12}
\abx@aux@page{46}{12}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2.4}Domain Adaptation and Transfer Learning}{12}{section.2.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Cross-Domain Transfer in Language Models}{12}{subsection.2.4.1}\protected@file@percent }
\abx@aux@page{47}{12}
\abx@aux@page{48}{12}
\abx@aux@page{49}{12}
\abx@aux@page{50}{12}
\abx@aux@cite{mccloskey1989catastrophic}
\abx@aux@segm{0}{0}{mccloskey1989catastrophic}
\abx@aux@cite{french1999catastrophic}
\abx@aux@segm{0}{0}{french1999catastrophic}
\abx@aux@cite{kirkpatrick2017overcoming}
\abx@aux@segm{0}{0}{kirkpatrick2017overcoming}
\abx@aux@cite{arivazhagan2019massively}
\abx@aux@segm{0}{0}{arivazhagan2019massively}
\abx@aux@cite{raffel2020exploring}
\abx@aux@segm{0}{0}{raffel2020exploring}
\abx@aux@cite{quinonero2009dataset}
\abx@aux@segm{0}{0}{quinonero2009dataset}
\abx@aux@cite{aharoni2020unsupervised}
\abx@aux@segm{0}{0}{aharoni2020unsupervised}
\abx@aux@cite{xie2023doremi}
\abx@aux@segm{0}{0}{xie2023doremi}
\abx@aux@cite{longpre2023pretrainer}
\abx@aux@segm{0}{0}{longpre2023pretrainer}
\abx@aux@cite{mitra2023orca2}
\abx@aux@segm{0}{0}{mitra2023orca2}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Catastrophic Forgetting and Stability}{13}{subsection.2.4.2}\protected@file@percent }
\abx@aux@page{51}{13}
\abx@aux@page{52}{13}
\abx@aux@page{53}{13}
\abx@aux@page{54}{13}
\abx@aux@page{55}{13}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Distribution Shift and Domain Mismatch}{13}{subsection.2.4.3}\protected@file@percent }
\abx@aux@page{56}{13}
\abx@aux@page{57}{13}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Related Empirical Studies}{13}{subsection.2.4.4}\protected@file@percent }
\abx@aux@page{58}{13}
\abx@aux@page{59}{13}
\abx@aux@page{60}{14}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methodology}{15}{chapter.3}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.1}Experimental Design Overview}{15}{section.3.1}\protected@file@percent }
\abx@aux@cite{yang2024qwen2}
\abx@aux@segm{0}{0}{yang2024qwen2}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.2}Model Architecture}{16}{section.3.2}\protected@file@percent }
\abx@aux@page{61}{16}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.3}Datasets}{17}{section.3.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Financial Datasets}{17}{subsection.3.3.1}\protected@file@percent }
\abx@aux@cite{merity2016pointer}
\abx@aux@segm{0}{0}{merity2016pointer}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}WikiText}{18}{subsection.3.3.2}\protected@file@percent }
\abx@aux@page{62}{18}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Mixture Strategies}{18}{subsection.3.3.3}\protected@file@percent }
\abx@aux@cite{longpre2023pretrainer}
\abx@aux@segm{0}{0}{longpre2023pretrainer}
\abx@aux@page{63}{19}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.4}Training Setup and Hyperparameter Tuning}{19}{section.3.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Initial Configuration}{19}{subsection.3.4.1}\protected@file@percent }
\abx@aux@cite{kaplan2020scaling}
\abx@aux@segm{0}{0}{kaplan2020scaling}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Discovery of Reverse Scaling}{20}{subsection.3.4.2}\protected@file@percent }
\abx@aux@page{64}{20}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Systematic Learning Rate Adjustment}{20}{subsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Final Learning Rate Recommendations}{21}{subsection.3.4.4}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Learning rate recommendations by model size. Reduction factors follow approximate inverse square-root scaling relative to 0.6B baseline.\relax }}{21}{table.caption.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Other Hyperparameters}{22}{subsection.3.4.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.5}Evaluation Protocol}{22}{section.3.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Multi-Dataset Evaluation}{22}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Metrics}{23}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{CV computation details}{23}{section*.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{25}{chapter.4}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.1}Overview of Experimental Results}{25}{section.4.1}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Overview of 10 pretraining experiments. Perplexity reported for best-performing model size on the corresponding training dataset's test set. Epochs vary by model size to normalize token exposure ($\sim $100M tokens per model).\relax }}{25}{table.caption.8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:experiments_overview}{{4.1}{25}{Overview of 10 pretraining experiments. Perplexity reported for best-performing model size on the corresponding training dataset's test set. Epochs vary by model size to normalize token exposure ($\sim $100M tokens per model).\relax }{table.caption.8}{}}
\newlabel{tab:experiments_overview@cref}{{[table][1][4]4.1}{[1][25][]25}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.2}Data Mixture Effects: The Core Finding}{26}{section.4.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Mixed Financial Datasets}{26}{subsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Mixed Financial Dataset: Model scaling behavior across 0.6B, 1.7B, and 4B parameters. Left panel shows perplexity (log scale) decreasing consistently with model size. Right panel shows cross-entropy loss following expected scaling pattern. Both metrics demonstrate normal scaling with 22.6\% total improvement from 0.6B to 4B.\relax }}{27}{figure.caption.9}\protected@file@percent }
\newlabel{fig:scaling_mixed_financial}{{4.1}{27}{Mixed Financial Dataset: Model scaling behavior across 0.6B, 1.7B, and 4B parameters. Left panel shows perplexity (log scale) decreasing consistently with model size. Right panel shows cross-entropy loss following expected scaling pattern. Both metrics demonstrate normal scaling with 22.6\% total improvement from 0.6B to 4B.\relax }{figure.caption.9}{}}
\newlabel{fig:scaling_mixed_financial@cref}{{[figure][1][4]4.1}{[1][27][]27}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Mixed Wiki+Financial}{27}{subsection.4.2.2}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Mixed Financial Dataset: Evaluation Across Multiple Datasets\relax }}{28}{table.caption.10}\protected@file@percent }
\newlabel{tab:mixed_financial_results}{{4.2}{28}{Mixed Financial Dataset: Evaluation Across Multiple Datasets\relax }{table.caption.10}{}}
\newlabel{tab:mixed_financial_results@cref}{{[table][2][4]4.2}{[1][27][]28}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Mixed Wiki+Financial Dataset: Scaling behavior shows normal pattern but with higher perplexity than pure financial mixture. The 15.1\% total improvement (0.6B to 4B) is smaller than pure financial (22.6\%), suggesting domain mixture creates competing optimization pressures that limit scaling benefits.\relax }}{29}{figure.caption.11}\protected@file@percent }
\newlabel{fig:scaling_mixed_wiki_financial}{{4.2}{29}{Mixed Wiki+Financial Dataset: Scaling behavior shows normal pattern but with higher perplexity than pure financial mixture. The 15.1\% total improvement (0.6B to 4B) is smaller than pure financial (22.6\%), suggesting domain mixture creates competing optimization pressures that limit scaling benefits.\relax }{figure.caption.11}{}}
\newlabel{fig:scaling_mixed_wiki_financial@cref}{{[figure][2][4]4.2}{[1][28][]29}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Pure WikiText Baseline}{29}{subsection.4.2.3}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Mixed Wiki+Financial Dataset: Evaluation Across Multiple Datasets\relax }}{30}{table.caption.12}\protected@file@percent }
\newlabel{tab:mixed_wiki_financial_results}{{4.3}{30}{Mixed Wiki+Financial Dataset: Evaluation Across Multiple Datasets\relax }{table.caption.12}{}}
\newlabel{tab:mixed_wiki_financial_results@cref}{{[table][3][4]4.3}{[1][28][]30}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Key Takeaway}{30}{subsection.4.2.4}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces WikiText Dataset: Severe reverse scaling phenomenon. The 1.7B model shows adjusted learning rate results (dashed line, squares) after fixing training collapse. The 4B model required 75\% LR reduction to stabilize. Clean, structured data amplifies learning rate sensitivity at larger scales.\relax }}{31}{figure.caption.13}\protected@file@percent }
\newlabel{fig:scaling_wikitext}{{4.3}{31}{WikiText Dataset: Severe reverse scaling phenomenon. The 1.7B model shows adjusted learning rate results (dashed line, squares) after fixing training collapse. The 4B model required 75\% LR reduction to stabilize. Clean, structured data amplifies learning rate sensitivity at larger scales.\relax }{figure.caption.13}{}}
\newlabel{fig:scaling_wikitext@cref}{{[figure][3][4]4.3}{[1][30][]31}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces WikiText Dataset: Impact of Learning Rate Adjustments\relax }}{31}{table.caption.14}\protected@file@percent }
\newlabel{tab:wikitext_lr_comparison}{{4.4}{31}{WikiText Dataset: Impact of Learning Rate Adjustments\relax }{table.caption.14}{}}
\newlabel{tab:wikitext_lr_comparison@cref}{{[table][4][4]4.4}{[1][30][]31}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Comparison of all three mixture strategies across model sizes. Mixed Financial (blue) consistently outperforms Mixed Wiki+Financial (orange) and WikiText (green) on financial evaluation metrics. The divergence increases with model size, demonstrating that in-domain diversity scales better than general-domain quality.\relax }}{32}{figure.caption.15}\protected@file@percent }
\newlabel{fig:scaling_comparison_all}{{4.4}{32}{Comparison of all three mixture strategies across model sizes. Mixed Financial (blue) consistently outperforms Mixed Wiki+Financial (orange) and WikiText (green) on financial evaluation metrics. The divergence increases with model size, demonstrating that in-domain diversity scales better than general-domain quality.\relax }{figure.caption.15}{}}
\newlabel{fig:scaling_comparison_all@cref}{{[figure][4][4]4.4}{[1][32][]32}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.3}Individual Dataset Analysis: Component Effects}{32}{section.4.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Large Datasets}{32}{subsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Financial News Articles Dataset: Excellent normal scaling with 21.7\% total improvement (0.6B to 4B). Large dataset size (197M tokens) provides sufficient diversity for stable training across all model sizes with minimal overtraining (2-3 epochs).\relax }}{34}{figure.caption.16}\protected@file@percent }
\newlabel{fig:scaling_news_articles}{{4.5}{34}{Financial News Articles Dataset: Excellent normal scaling with 21.7\% total improvement (0.6B to 4B). Large dataset size (197M tokens) provides sufficient diversity for stable training across all model sizes with minimal overtraining (2-3 epochs).\relax }{figure.caption.16}{}}
\newlabel{fig:scaling_news_articles@cref}{{[figure][5][4]4.5}{[1][33][]34}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces SEC Reports Dataset: Consistent normal scaling with 22.4\% total improvement. The 80M token corpus supports standalone pretraining with moderate overtraining (6-24 epochs). Strong transfer to similar long-form documents.\relax }}{34}{figure.caption.17}\protected@file@percent }
\newlabel{fig:scaling_sec_reports}{{4.6}{34}{SEC Reports Dataset: Consistent normal scaling with 22.4\% total improvement. The 80M token corpus supports standalone pretraining with moderate overtraining (6-24 epochs). Strong transfer to similar long-form documents.\relax }{figure.caption.17}{}}
\newlabel{fig:scaling_sec_reports@cref}{{[figure][6][4]4.6}{[1][33][]34}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Medium Datasets}{34}{subsection.4.3.2}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Financial News Dataset: Evaluation Across Multiple Datasets\relax }}{35}{table.caption.18}\protected@file@percent }
\newlabel{tab:news_articles_results}{{4.5}{35}{Financial News Dataset: Evaluation Across Multiple Datasets\relax }{table.caption.18}{}}
\newlabel{tab:news_articles_results@cref}{{[table][5][4]4.5}{[1][33][]35}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces SEC Reports Dataset: Evaluation Across Multiple Datasets\relax }}{36}{table.caption.19}\protected@file@percent }
\newlabel{tab:sec_reports_results}{{4.6}{36}{SEC Reports Dataset: Evaluation Across Multiple Datasets\relax }{table.caption.19}{}}
\newlabel{tab:sec_reports_results@cref}{{[table][6][4]4.6}{[1][33][]36}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Small Datasets}{36}{subsection.4.3.3}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces FinGPT Sentiment Dataset: Normal scaling with 22.1\% improvement despite moderate overtraining (12-30 epochs). Instruction-following format benefits from increased model capacity, showing strong transfer to similar task types.\relax }}{37}{figure.caption.20}\protected@file@percent }
\newlabel{fig:scaling_fingpt}{{4.7}{37}{FinGPT Sentiment Dataset: Normal scaling with 22.1\% improvement despite moderate overtraining (12-30 epochs). Instruction-following format benefits from increased model capacity, showing strong transfer to similar task types.\relax }{figure.caption.20}{}}
\newlabel{fig:scaling_fingpt@cref}{{[figure][7][4]4.7}{[1][36][]37}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Finance Alpaca Dataset: Consistent 21.8\% improvement across model sizes. Educational Q\&A format shows reliable scaling despite 13-25 epochs of training, but exhibits narrow task focus with 48\% cross-dataset variance.\relax }}{37}{figure.caption.21}\protected@file@percent }
\newlabel{fig:scaling_alpaca}{{4.8}{37}{Finance Alpaca Dataset: Consistent 21.8\% improvement across model sizes. Educational Q\&A format shows reliable scaling despite 13-25 epochs of training, but exhibits narrow task focus with 48\% cross-dataset variance.\relax }{figure.caption.21}{}}
\newlabel{fig:scaling_alpaca@cref}{{[figure][8][4]4.8}{[1][36][]37}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces FiQA Dataset: Strong normal scaling with 25.2\% total improvement. Despite small size (4M tokens), conversational Q\&A format produces stable training and excellent in-domain performance, though with high variance (52\%) on out-of-format tasks.\relax }}{38}{figure.caption.22}\protected@file@percent }
\newlabel{fig:scaling_fiqa}{{4.9}{38}{FiQA Dataset: Strong normal scaling with 25.2\% total improvement. Despite small size (4M tokens), conversational Q\&A format produces stable training and excellent in-domain performance, though with high variance (52\%) on out-of-format tasks.\relax }{figure.caption.22}{}}
\newlabel{fig:scaling_fiqa@cref}{{[figure][9][4]4.9}{[1][36][]38}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces FinGPT Sentiment Dataset: Evaluation Across Multiple Datasets\relax }}{39}{table.caption.23}\protected@file@percent }
\newlabel{tab:fingpt_results}{{4.7}{39}{FinGPT Sentiment Dataset: Evaluation Across Multiple Datasets\relax }{table.caption.23}{}}
\newlabel{tab:fingpt_results@cref}{{[table][7][4]4.7}{[1][36][]39}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Financial QA 10K Dataset: Moderate reverse scaling resolved via learning rate adjustment. The 4B model (dashed line, squares) shows adjusted LR results with 10.3\% improvement, recovering expected scaling order. Extreme overtraining (67-100 epochs) causes 89\% cross-dataset variance.\relax }}{39}{figure.caption.26}\protected@file@percent }
\newlabel{fig:scaling_financial_qa}{{4.10}{39}{Financial QA 10K Dataset: Moderate reverse scaling resolved via learning rate adjustment. The 4B model (dashed line, squares) shows adjusted LR results with 10.3\% improvement, recovering expected scaling order. Extreme overtraining (67-100 epochs) causes 89\% cross-dataset variance.\relax }{figure.caption.26}{}}
\newlabel{fig:scaling_financial_qa@cref}{{[figure][10][4]4.10}{[1][39][]39}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Dataset Size vs Generalization}{39}{subsection.4.3.4}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Finance Alpaca Dataset: Evaluation Across Multiple Datasets\relax }}{40}{table.caption.24}\protected@file@percent }
\newlabel{tab:alpaca_results}{{4.8}{40}{Finance Alpaca Dataset: Evaluation Across Multiple Datasets\relax }{table.caption.24}{}}
\newlabel{tab:alpaca_results@cref}{{[table][8][4]4.8}{[1][36][]40}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces FiQA Dataset: Evaluation Across Multiple Datasets\relax }}{41}{table.caption.25}\protected@file@percent }
\newlabel{tab:fiqa_results}{{4.9}{41}{FiQA Dataset: Evaluation Across Multiple Datasets\relax }{table.caption.25}{}}
\newlabel{tab:fiqa_results@cref}{{[table][9][4]4.9}{[1][36][]41}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.4}Training Dynamics and Scaling Behavior}{41}{section.4.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Normal Scaling Pattern}{41}{subsection.4.4.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Twitter Financial Sentiment Dataset: Severe reverse scaling phenomenon. The 4B model (dashed line, squares) required 75\% LR reduction to recover performance, achieving 31.6\% improvement. Extremely small dataset (0.3M tokens, 150-249 epochs) creates brittle optimization landscape with 89\% variance.\relax }}{42}{figure.caption.27}\protected@file@percent }
\newlabel{fig:scaling_twitter}{{4.11}{42}{Twitter Financial Sentiment Dataset: Severe reverse scaling phenomenon. The 4B model (dashed line, squares) required 75\% LR reduction to recover performance, achieving 31.6\% improvement. Extremely small dataset (0.3M tokens, 150-249 epochs) creates brittle optimization landscape with 89\% variance.\relax }{figure.caption.27}{}}
\newlabel{fig:scaling_twitter@cref}{{[figure][11][4]4.11}{[1][39][]42}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.10}{\ignorespaces Financial QA 10K Dataset: Impact of Learning Rate Adjustments\relax }}{42}{table.caption.28}\protected@file@percent }
\newlabel{tab:financial_qa_lr_comparison}{{4.10}{42}{Financial QA 10K Dataset: Impact of Learning Rate Adjustments\relax }{table.caption.28}{}}
\newlabel{tab:financial_qa_lr_comparison@cref}{{[table][10][4]4.10}{[1][39][]42}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.11}{\ignorespaces Twitter Financial Dataset: Impact of Learning Rate Adjustments\relax }}{43}{table.caption.29}\protected@file@percent }
\newlabel{tab:twitter_lr_comparison}{{4.11}{43}{Twitter Financial Dataset: Impact of Learning Rate Adjustments\relax }{table.caption.29}{}}
\newlabel{tab:twitter_lr_comparison@cref}{{[table][11][4]4.11}{[1][39][]43}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Reverse Scaling Phenomenon}{44}{subsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Learning Rate Sensitivity by Model Size}{45}{subsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Fixing Reverse Scaling}{47}{subsection.4.4.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.5}Model Stability Analysis}{48}{subsection.4.4.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.5}Domain Transfer and Generalization Patterns}{49}{section.4.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Cross-Dataset Evaluation}{49}{subsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Document Format and Task Type Effects}{51}{subsection.4.5.2}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.12}{\ignorespaces Financial News Evaluation: Performance Across Training Datasets\relax }}{52}{table.caption.30}\protected@file@percent }
\newlabel{tab:cross_financial_news}{{4.12}{52}{Financial News Evaluation: Performance Across Training Datasets\relax }{table.caption.30}{}}
\newlabel{tab:cross_financial_news@cref}{{[table][12][4]4.12}{[1][51][]52}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.13}{\ignorespaces SEC Reports Evaluation: Performance Across Training Datasets\relax }}{53}{table.caption.31}\protected@file@percent }
\newlabel{tab:cross_financial_repor}{{4.13}{53}{SEC Reports Evaluation: Performance Across Training Datasets\relax }{table.caption.31}{}}
\newlabel{tab:cross_financial_repor@cref}{{[table][13][4]4.13}{[1][51][]53}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Variance Comparison}{53}{subsection.4.5.3}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.14}{\ignorespaces Alpaca Evaluation: Performance Across Training Datasets\relax }}{54}{table.caption.32}\protected@file@percent }
\newlabel{tab:cross_alpaca}{{4.14}{54}{Alpaca Evaluation: Performance Across Training Datasets\relax }{table.caption.32}{}}
\newlabel{tab:cross_alpaca@cref}{{[table][14][4]4.14}{[1][52][]54}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.15}{\ignorespaces FinGPT Evaluation: Performance Across Training Datasets\relax }}{55}{table.caption.33}\protected@file@percent }
\newlabel{tab:cross_fingpt}{{4.15}{55}{FinGPT Evaluation: Performance Across Training Datasets\relax }{table.caption.33}{}}
\newlabel{tab:cross_fingpt@cref}{{[table][15][4]4.15}{[1][52][]55}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.16}{\ignorespaces FiQA Evaluation: Performance Across Training Datasets\relax }}{56}{table.caption.34}\protected@file@percent }
\newlabel{tab:cross_fiqa}{{4.16}{56}{FiQA Evaluation: Performance Across Training Datasets\relax }{table.caption.34}{}}
\newlabel{tab:cross_fiqa@cref}{{[table][16][4]4.16}{[1][52][]56}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Domain-Specific vs General Knowledge Transfer}{56}{subsection.4.5.4}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.17}{\ignorespaces Twitter Financial Evaluation: Performance Across Training Datasets\relax }}{57}{table.caption.35}\protected@file@percent }
\newlabel{tab:cross_twitter}{{4.17}{57}{Twitter Financial Evaluation: Performance Across Training Datasets\relax }{table.caption.35}{}}
\newlabel{tab:cross_twitter@cref}{{[table][17][4]4.17}{[1][53][]57}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.18}{\ignorespaces Financial QA Evaluation: Performance Across Training Datasets\relax }}{58}{table.caption.36}\protected@file@percent }
\newlabel{tab:cross_financial_qa}{{4.18}{58}{Financial QA Evaluation: Performance Across Training Datasets\relax }{table.caption.36}{}}
\newlabel{tab:cross_financial_qa@cref}{{[table][18][4]4.18}{[1][56][]58}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.19}{\ignorespaces WikiText Evaluation: Performance Across Training Datasets\relax }}{59}{table.caption.37}\protected@file@percent }
\newlabel{tab:cross_wikitext}{{4.19}{59}{WikiText Evaluation: Performance Across Training Datasets\relax }{table.caption.37}{}}
\newlabel{tab:cross_wikitext@cref}{{[table][19][4]4.19}{[1][59][]59}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4.6}Summary and Key Results}{60}{section.4.6}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4.20}{\ignorespaces Best configurations by application. *SEC's 18\% CV is in-domain only; cross-dataset CV is 32\%.\relax }}{61}{table.caption.38}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion}{63}{chapter.5}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.1}Key Empirical Findings}{63}{section.5.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.2}Interpretation of Data Interaction Effects}{65}{section.5.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Why WikiText Underperforms on Financial Tasks}{65}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Benefits of In-Domain Diversity}{66}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Domain Interference Patterns}{68}{subsection.5.2.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Scale-Dependent Training Dynamics}{69}{subsection.5.2.4}\protected@file@percent }
\abx@aux@cite{kaplan2020scaling}
\abx@aux@segm{0}{0}{kaplan2020scaling}
\abx@aux@cite{hoffmann2022training}
\abx@aux@segm{0}{0}{hoffmann2022training}
\abx@aux@page{65}{70}
\abx@aux@page{66}{70}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.3}Practical Guidelines for Financial LM Pretraining}{70}{section.5.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Data Mixture Strategies by Use Case}{70}{subsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Model Size Selection}{71}{subsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Learning Rate Guidelines by Model Size}{72}{subsection.5.3.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}Token Budget Allocation}{73}{subsection.5.3.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5.4}Limitations and Threats to Validity}{73}{section.5.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion}{75}{chapter.6}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.1}Summary of Contributions}{75}{section.6.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Data Mixture Guidelines for Financial NLP}{75}{subsection.6.1.1}\protected@file@percent }
\abx@aux@cite{kaplan2020scaling}
\abx@aux@segm{0}{0}{kaplan2020scaling}
\abx@aux@cite{hoffmann2022training}
\abx@aux@segm{0}{0}{hoffmann2022training}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Learning Rate Scaling Laws for Decoder-Only Transformers}{76}{subsection.6.1.2}\protected@file@percent }
\abx@aux@page{67}{76}
\abx@aux@page{68}{76}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Dataset Size Effects and Generalization}{77}{subsection.6.1.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Domain Transfer and Format Effects}{77}{subsection.6.1.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.5}Model Size Selection for Resource-Constrained Settings}{78}{subsection.6.1.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.6}Open-Source Reproducible Pipeline}{79}{subsection.6.1.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.2}Implications for Practice and Research}{79}{section.6.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}For Practitioners: Actionable Deployment Guidelines}{79}{subsection.6.2.1}\protected@file@percent }
\abx@aux@cite{kaplan2020scaling}
\abx@aux@segm{0}{0}{kaplan2020scaling}
\abx@aux@cite{hoffmann2022training}
\abx@aux@segm{0}{0}{hoffmann2022training}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}For Researchers: Open Questions and Methodological Lessons}{80}{subsection.6.2.2}\protected@file@percent }
\abx@aux@page{69}{80}
\abx@aux@page{70}{80}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}For Industry: Privacy-Preserving Financial AI}{81}{subsection.6.2.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.3}Future Research Directions}{81}{section.6.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Scaling to Larger Models and Architectures}{81}{subsection.6.3.1}\protected@file@percent }
\abx@aux@cite{xie2023doremi}
\abx@aux@segm{0}{0}{xie2023doremi}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Advanced Mixture Optimization}{82}{subsection.6.3.2}\protected@file@percent }
\abx@aux@page{71}{82}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Comprehensive Downstream Evaluation}{83}{subsection.6.3.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Multi-Stage Pretraining Strategies}{83}{subsection.6.3.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.5}Theoretical Understanding of Learning Rate Scaling}{84}{subsection.6.3.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6.4}Closing Remarks}{85}{section.6.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{Appendices}{}{section*.39}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\numberline {A}Experimental Details}{87}{appendix.A}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chp:experimental_details}{{A}{87}{Experimental Details}{appendix.A}{}}
\newlabel{chp:experimental_details@cref}{{[appendix][1][2147483647]A}{[1][87][]87}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {A.1}Complete Hyperparameter Tables}{87}{section.A.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {A.2}Additional Results Tables}{87}{section.A.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {A.3}Dataset Preprocessing Details}{87}{section.A.3}\protected@file@percent }
\abx@aux@page{72}{88}
\abx@aux@page{73}{88}
\abx@aux@page{74}{88}
\abx@aux@page{75}{88}
\abx@aux@page{76}{88}
\abx@aux@page{77}{88}
\abx@aux@page{78}{89}
\abx@aux@page{79}{89}
\abx@aux@page{80}{89}
\abx@aux@page{81}{89}
\abx@aux@page{82}{89}
\abx@aux@page{83}{89}
\abx@aux@page{84}{89}
\abx@aux@page{85}{89}
\abx@aux@page{86}{90}
\abx@aux@page{87}{90}
\abx@aux@page{88}{90}
\abx@aux@page{89}{90}
\abx@aux@page{90}{90}
\abx@aux@page{91}{90}
\abx@aux@page{92}{90}
\abx@aux@page{93}{90}
\abx@aux@page{94}{91}
\abx@aux@page{95}{91}
\abx@aux@page{96}{91}
\abx@aux@page{97}{91}
\abx@aux@page{98}{91}
\abx@aux@page{99}{91}
\abx@aux@page{100}{91}
\abx@aux@page{101}{92}
\abx@aux@page{102}{92}
\abx@aux@page{103}{92}
\abx@aux@page{104}{92}
\abx@aux@page{105}{92}
\abx@aux@page{106}{92}
\abx@aux@read@bbl@mdfivesum{F37B3304050D406071A29CC122DBA493}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{aharoni2020unsupervised}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{araci2019finbert}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{arivazhagan2019massively}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{bengio2009curriculum}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{brown2020language}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{yang2020finqa}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{devlin2019bert}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{french1999catastrophic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{gao2020pile}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{gururangan2020don}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{hoffmann2022training}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{kaplan2020scaling}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{kirkpatrick2017overcoming}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{longpre2023pretrainer}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{mccandlish2018empirical}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{mccloskey1989catastrophic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{merity2016pointer}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{mitra2023orca2}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{narayanan2021efficient}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{quinonero2009dataset}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{radford2019language}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{raffel2020exploring}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{rajbhandari2020zero}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{eu2016gdpr}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{sanh2022multitask}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{tay2022ul2}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{touvron2023llama}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{vaswani2017attention}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{wu2023bloomberggpt}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{xia2023sheared}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{xie2023doremi}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{yang2024qwen2}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{yang2023fingpt}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{yang2020finbert}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{wu2022opt}{nyt/global//global/global}
\gdef \@abspage@last{105}
