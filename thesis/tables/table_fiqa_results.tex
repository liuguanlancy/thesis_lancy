% FiQA Dataset: Evaluation Results
% Training: FiQA (FiQA dataset, 4M tokens)
% All models trained with LR=2e-5

\begin{table}[h]
\centering
\caption{FiQA Dataset: Evaluation Across Multiple Datasets}
\label{tab:fiqa_results}
\begin{tabular}{l|ccc|ccc}
\hline
\textbf{Eval Dataset} & \multicolumn{3}{c|}{\textbf{Cross-Entropy Loss}} & \multicolumn{3}{c}{\textbf{Perplexity}} \\n\cline{2-4} \cline{5-7}
  & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} \\
Alpaca & 4.14 & 2.56 & \textbf{1.96} & 62.97 & \textbf{12.96} & \textbf{7.12} \
 Financial News & 3.90 & 2.54 & \textbf{2.01} & 49.22 & \textbf{12.74} & \textbf{7.43} \
 Financial Qa & 4.64 & 2.60 & \textbf{1.84} & 103.4 & \textbf{13.53} & \textbf{6.32} \
 Financial Repor & 4.42 & 2.53 & \textbf{1.81} & 83.48 & \textbf{12.51} & \textbf{6.14} \
 Fingpt & 4.67 & 2.71 & \textbf{1.95} & 107.2 & \textbf{15.08} & \textbf{7.01} \
 Twitter & 4.66 & 2.65 & \textbf{1.88} & 105.3 & \textbf{14.10} & \textbf{6.58} \
 Wikitext & 4.52 & 2.63 & \textbf{1.91} & 92.13 & \textbf{13.81} & \textbf{6.72} \
\hline
\end{tabular}
\end{table}

