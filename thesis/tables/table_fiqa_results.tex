% FiQA Dataset: Evaluation Results
% Training: FiQA (FiQA dataset, 4M tokens)
% All models trained with LR=2e-5

\begin{table}[h]
\centering
\caption[FiQA: Evaluation Results]{FiQA Dataset: Evaluation Across Multiple Datasets}
\label{tab:fiqa_results}
\begin{tabular}{l|ccc|ccc}
\hline
\textbf{Eval Dataset} & \multicolumn{3}{c|}{\textbf{Cross-Entropy Loss}} & \multicolumn{3}{c}{\textbf{Perplexity}} \\
\cline{2-4} \cline{5-7}
  & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} \\
Alpaca & 4.14 & 2.56 & 1.96 & 62.97 & 12.96 & 7.12 \\
Financial News & 3.90 & 2.54 & 2.01 & 49.22 & 12.74 & 7.43 \\
Financial QA & 4.64 & 2.60 & 1.84 & 103.4 & 13.53 & 6.32 \\
SEC Reports & 4.42 & 2.53 & 1.81 & 83.48 & 12.51 & 6.14 \\
\textbf{FiQA} & \textbf{4.17} & \textbf{2.56} & \textbf{1.96} & \textbf{64.75} & \textbf{12.99} & \textbf{7.08} \\
FinGPT & 4.67 & 2.71 & 1.95 & 107.2 & 15.08 & 7.01 \\
Twitter & 4.66 & 2.65 & 1.88 & 105.3 & 14.10 & 6.58 \\
Wikitext & 4.52 & 2.63 & 1.91 & 92.13 & 13.81 & 6.72 \\
\hline
\textbf{Average} & \textbf{4.39} & \textbf{2.60} & \textbf{1.92} & \textbf{83.57} & \textbf{13.47} & \textbf{6.80} \\
\hline
\end{tabular}
\end{table}
