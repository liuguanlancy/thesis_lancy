% Financial News Dataset: Evaluation Results
% Training: Financial News (Financial news articles, 197M tokens)
% All models trained with LR=2e-5

\begin{table}[h]
\centering
\caption[Financial News: Evaluation Results]{Financial News Dataset: Evaluation Across Multiple Datasets}
\label{tab:news_articles_results}
\begin{tabular}{l|ccc|ccc}
\hline
\textbf{Eval Dataset} & \multicolumn{3}{c|}{\textbf{Cross-Entropy Loss}} & \multicolumn{3}{c}{\textbf{Perplexity}} \\
\cline{2-4} \cline{5-7}
  & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} \\
Alpaca & 4.57 & 3.61 & \textbf{3.39} & 96.31 & \textbf{36.92} & \textbf{29.75} \\
Financial Qa & 5.11 & 3.90 & \textbf{3.66} & 166.1 & \textbf{49.53} & \textbf{38.90} \\
Financial Repor & 4.85 & 3.73 & \textbf{3.51} & 127.7 & \textbf{41.68} & \textbf{33.46} \\
Fingpt & 5.08 & 3.90 & \textbf{3.64} & 160.9 & \textbf{49.56} & \textbf{38.03} \\
Fiqa & 4.62 & 3.65 & \textbf{3.46} & 101.3 & \textbf{38.68} & \textbf{31.69} \\
Twitter & 5.11 & 3.91 & \textbf{3.66} & 165.2 & \textbf{49.88} & \textbf{38.98} \\
Wikitext & 4.95 & 3.81 & \textbf{3.54} & 140.7 & \textbf{45.17} & \textbf{34.33} \\
\hline
\end{tabular}
\end{table}

