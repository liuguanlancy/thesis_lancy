% Financial News Dataset: Evaluation Results
% Training: Financial News (Financial news articles, 197M tokens)
% All models trained with LR=2e-5

\begin{table}[h]
\centering
\caption{Financial News Dataset: Evaluation Across Multiple Datasets}
\label{tab:news_articles_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ccc|ccc}
\toprule
\multirow{2}{*}{\textbf{Eval Dataset}} &
\multicolumn{3}{c|}{\textbf{Cross-Entropy Loss}} &
\multicolumn{3}{c}{\textbf{Perplexity}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B}\\
\midrule
Alpaca & 4.57 & 3.61 & \textbf{3.39} & 96.31 & 36.92 & \textbf{29.75}\\
Financial Qa & 5.11 & 3.90 & \textbf{3.66} & 166.1 & 49.53 & \textbf{38.90}\\
Financial Repor & 4.85 & 3.73 & \textbf{3.51} & 127.7 & 41.68 & \textbf{33.46}\\
Fingpt & 5.08 & 3.90 & \textbf{3.64} & 160.9 & 49.56 & \textbf{38.03}\\
Fiqa & 4.62 & 3.65 & \textbf{3.46} & 101.3 & 38.68 & \textbf{31.69}\\
Twitter & 5.11 & 3.91 & \textbf{3.66} & 165.2 & 49.88 & \textbf{38.98}\\
Wikitext & 4.95 & 3.81 & \textbf{3.54} & 140.7 & 45.17 & \textbf{34.33}\\
\bottomrule
\end{tabular}
}
\end{table}

