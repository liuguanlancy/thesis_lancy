% SEC Reports Dataset: Evaluation Results
% Training: SEC Reports (SEC 10-K/10-Q filings, 80M tokens)
% All models trained with LR=2e-5

\begin{table}[h]
\centering
\caption{SEC Reports Dataset: Evaluation Across Multiple Datasets}
\label{tab:sec_reports_results}
\begin{tabular}{l|ccc|ccc}
\hline
\textbf{Eval Dataset} & \multicolumn{3}{c|}{\textbf{Cross-Entropy Loss}} & \multicolumn{3}{c}{\textbf{Perplexity}} \\n\cline{2-4} \cline{5-7}
  & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} \\
Alpaca & 3.86 & 3.14 & \textbf{2.92} & 47.65 & \textbf{23.04} & \textbf{18.54} \
 Financial News & 3.71 & 3.08 & \textbf{2.81} & 40.85 & \textbf{21.65} & \textbf{16.67} \
 Financial Qa & 3.90 & 3.08 & \textbf{2.86} & 49.30 & \textbf{21.77} & \textbf{17.39} \
 Fingpt & 3.97 & 3.15 & \textbf{2.93} & 53.18 & \textbf{23.41} & \textbf{18.68} \
 Fiqa & 3.85 & 3.14 & \textbf{2.96} & 47.22 & \textbf{23.15} & \textbf{19.34} \
 Twitter & 3.94 & 3.13 & \textbf{2.90} & 51.30 & \textbf{22.86} & \textbf{18.12} \
 Wikitext & 3.89 & 3.10 & \textbf{2.88} & 49.02 & \textbf{22.21} & \textbf{17.72} \
\hline
\end{tabular}
\end{table}

