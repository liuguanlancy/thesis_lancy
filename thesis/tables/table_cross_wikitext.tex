% Cross-Dataset Comparison: WikiText as Evaluation Dataset
% Shows which training dataset performs best on WikiText
% Bold values indicate best performance for each model size

\begin{table}[htbp]
\centering
\caption[WikiText Evaluation: Cross-Dataset Performance]{WikiText Evaluation: Performance Across Training Datasets}
\label{tab:cross_wikitext}
\begin{tabular}{l|ccc|ccc}
\hline
\textbf{Training Dataset} & \multicolumn{3}{c|}{\textbf{Cross-Entropy Loss}} & \multicolumn{3}{c}{\textbf{Perplexity}} \\
\cline{2-4} \cline{5-7}
  & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} \\
Alpaca (2e-5) & 4.63 & 2.94 & 2.18 & 102.41 & 18.85 & 8.88  \\
Financial QA (2e-5) & 2.24 & \textbf{2.11} & 2.19 & 9.41 & \textbf{8.23} & 8.89  \\
Financial QA (1.7B: 1e-5, 4B: 5e-6) & 2.24 & 2.21 & 2.08 & 9.41 & 9.08 & 8.00  \\
FinGPT (2e-5) & 3.66 & 2.44 & 1.99 & 38.70 & 11.46 & 7.29  \\
FiQA (2e-5) & 4.52 & 2.63 & \textbf{1.91} & 92.13 & 13.81 & \textbf{6.72}  \\
Mixed Wiki+Financial (2e-5) & 4.41 & 3.74 & 3.32 & 82.10 & 41.95 & 27.72  \\
Financial News (2e-5) & 4.95 & 3.81 & 3.54 & 140.71 & 45.17 & 34.33  \\
SEC Reports (2e-5) & 3.89 & 3.10 & 2.88 & 49.02 & 22.21 & 17.72  \\
Twitter Financial (2e-5) & 2.69 & 2.47 & 2.88 & 14.74 & 11.78 & 17.85  \\
Twitter Financial (1.7B: 1e-5, 4B: 5e-6) & 2.69 & 2.30 & 2.49 & 14.74 & 9.94 & 12.02  \\
WikiText (2e-5) & \textbf{1.56} & 3.42 & 3.30 & \textbf{4.78} & 30.63 & 27.19  \\
WikiText (1.7B: 5e-6, 4B: 3e-6) & \textbf{1.56} & 3.88 & 3.65 & \textbf{4.78} & 48.44 & 38.60  \\
\hline
\end{tabular}
\end{table}

