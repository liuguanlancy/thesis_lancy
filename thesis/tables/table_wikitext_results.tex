% WikiText Dataset: Evaluation Results
% Training: WikiText (WikiText-103, 100M tokens)
% All models trained with LR=2e-5

\begin{table}[h]
\centering
\caption[WikiText: Evaluation Results]{WikiText Dataset: Evaluation Across Multiple Datasets}
\label{tab:wikitext_results}
\begin{tabular}{l|ccc|ccc}
\hline
\textbf{Eval Dataset} & \multicolumn{3}{c|}{\textbf{Cross-Entropy Loss}} & \multicolumn{3}{c}{\textbf{Perplexity}} \\
\cline{2-4} \cline{5-7}
  & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} \\
Alpaca & 2.22 & 3.24 & 3.48 & 9.23 & 25.51 & 32.38 \\
Financial News & 2.62 & 2.93 & 3.37 & 13.70 & 18.78 & 29.19 \\
Financial Qa & 3.40 & 10.67 & 3.37 & 29.90 & $\infty$ & 29.08 \\
Financial Repor & 1.39 & 3.27 & 3.44 & 3.99 & 26.46 & 31.23 \\
Fingpt & 1.30 & 2.11 & 3.57 & 3.67 & 8.27 & 35.50 \\
Fiqa & 2.07 & 3.14 & 3.53 & 7.89 & 23.15 & 34.03 \\
Twitter & 1.45 & 2.78 & 3.52 & 4.26 & 16.06 & 33.71 \\
\textbf{Wikitext} & \textbf{1.56} & \textbf{3.42} & \textbf{3.30} & \textbf{4.78} & \textbf{30.63} & \textbf{27.19} \\
\hline
\textbf{Average} & \textbf{2.00} & \textbf{3.95} & \textbf{3.45} & \textbf{9.68} & \textbf{143.61} & \textbf{31.54} \\
\hline
\end{tabular}
\end{table}

