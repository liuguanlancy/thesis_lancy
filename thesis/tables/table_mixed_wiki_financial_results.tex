% Mixed Wiki+Financial Dataset: Evaluation Results
% Training: Mixed Wiki+Financial (WikiText + 7 financial datasets, 343.35M tokens)
% All models trained with LR=2e-5

\begin{table}[h]
\centering
\caption[Mixed Wiki+Financial: Evaluation Results]{Mixed Wiki+Financial Dataset: Evaluation Across Multiple Datasets}
\label{tab:mixed_wiki_financial_results}
\begin{tabular}{l|ccc|ccc}
\hline
\textbf{Eval Dataset} & \multicolumn{3}{c|}{\textbf{Cross-Entropy Loss}} & \multicolumn{3}{c}{\textbf{Perplexity}} \\
\cline{2-4} \cline{5-7}
  & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} \\
Alpaca & 4.07 & 3.48 & 3.15 & 58.56 & 32.38 & 23.23 \\
Financial News & 3.65 & 3.13 & 2.77 & 38.68 & 22.79 & 15.91 \\
Financial QA & 4.58 & 3.87 & 3.46 & 97.49 & 47.94 & 31.76 \\
SEC Reports & 4.35 & 3.69 & 3.33 & 77.57 & 40.17 & 27.91 \\
FinGPT & 4.44 & 3.75 & 3.37 & 84.43 & 42.50 & 28.92 \\
FiQA & 4.14 & 3.56 & 3.24 & 63.03 & 35.04 & 25.61 \\
Twitter & 4.59 & 3.88 & 3.48 & 98.13 & 48.42 & 32.48 \\
Wikitext & 4.41 & 3.74 & 3.32 & 82.10 & 41.95 & 27.72 \\
\hline
\textbf{Average} & \textbf{4.28} & \textbf{3.64} & \textbf{3.26} & \textbf{75.00} & \textbf{38.90} & \textbf{26.69} \\
\hline
\end{tabular}
\end{table}
