% Mixed Wiki+Financial Dataset: Evaluation Results
% Training: Mixed Wiki+Financial (WikiText + 7 financial datasets, ~400M tokens)
% All models trained with LR=2e-5

\begin{table}[h]
\centering
\caption{Mixed Wiki+Financial Dataset: Evaluation Across Multiple Datasets}
\label{tab:mixed_wiki_financial_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ccc|ccc}
\toprule
\multirow{2}{*}{\textbf{Eval Dataset}} &
\multicolumn{3}{c|}{\textbf{Cross-Entropy Loss}} &
\multicolumn{3}{c}{\textbf{Perplexity}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B}\\
\midrule
Alpaca & 4.07 & 3.48 & \textbf{3.15} & 58.56 & 32.38 & \textbf{23.23}\\
Financial News & 3.65 & 3.13 & \textbf{2.77} & 38.68 & 22.79 & \textbf{15.91}\\
Financial Qa & 4.58 & 3.87 & \textbf{3.46} & 97.49 & 47.94 & \textbf{31.76}\\
Financial Repor & 4.35 & 3.69 & \textbf{3.33} & 77.57 & 40.17 & \textbf{27.91}\\
Fingpt & 4.44 & 3.75 & \textbf{3.37} & 84.43 & 42.50 & \textbf{28.92}\\
Fiqa & 4.14 & 3.56 & \textbf{3.24} & 63.03 & 35.04 & \textbf{25.61}\\
Twitter & 4.59 & 3.88 & \textbf{3.48} & 98.13 & 48.42 & \textbf{32.48}\\
Wikitext & 4.41 & 3.74 & \textbf{3.32} & 82.10 & 41.95 & \textbf{27.72}\\
\bottomrule
\end{tabular}
}
\end{table}

