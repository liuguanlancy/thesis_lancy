% Finance Alpaca Dataset: Evaluation Results
% Training: Finance Alpaca (gbharti/finance-alpaca, 17M tokens)
% All models trained with LR=2e-5

\begin{table}[h]
\centering
\caption{Finance Alpaca Dataset: Evaluation Across Multiple Datasets}
\label{tab:alpaca_results}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|ccc|ccc}
\toprule
\multirow{2}{*}{\textbf{Eval Dataset}} &
\multicolumn{3}{c|}{\textbf{Cross-Entropy Loss}} &
\multicolumn{3}{c}{\textbf{Perplexity}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B}\\
\midrule
Financial News & 3.92 & 2.71 & \textbf{2.15} & 50.40 & 15.05 & \textbf{8.58}\\
Financial Qa & 4.77 & 2.95 & \textbf{2.15} & 117.4 & 19.11 & \textbf{8.56}\\
Financial Repor & 4.54 & 2.85 & \textbf{2.11} & 93.56 & 17.26 & \textbf{8.25}\\
Fingpt & 4.71 & 2.99 & \textbf{2.22} & 111.7 & 19.85 & \textbf{9.18}\\
Fiqa & 4.29 & 2.87 & \textbf{2.22} & 73.12 & 17.63 & \textbf{9.22}\\
Twitter & 4.78 & 2.99 & \textbf{2.19} & 118.7 & 19.82 & \textbf{8.97}\\
Wikitext & 4.63 & 2.94 & \textbf{2.18} & 102.4 & 18.85 & \textbf{8.88}\\
\bottomrule
\end{tabular}
}
\end{table}

