% Finance Alpaca Dataset: Evaluation Results
% Training: Finance Alpaca (gbharti/finance-alpaca, 17M tokens)
% All models trained with LR=2e-5

\begin{table}[h]
\centering
\caption[Finance Alpaca: Evaluation Results]{Finance Alpaca Dataset: Evaluation Across Multiple Datasets}
\label{tab:alpaca_results}
\begin{tabular}{l|ccc|ccc}
\hline
\textbf{Eval Dataset} & \multicolumn{3}{c|}{\textbf{Cross-Entropy Loss}} & \multicolumn{3}{c}{\textbf{Perplexity}} \\
\cline{2-4} \cline{5-7}
  & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} \\
\textbf{Alpaca} & \textbf{4.15} & \textbf{2.75} & \textbf{2.11} & \textbf{63.73} & \textbf{15.61} & \textbf{8.22} \\
Financial News & 3.92 & 2.71 & 2.15 & 50.40 & 15.05 & 8.58 \\
Financial Qa & 4.77 & 2.95 & 2.15 & 117.4 & 19.11 & 8.56 \\
Financial Repor & 4.54 & 2.85 & 2.11 & 93.56 & 17.26 & 8.25 \\
Fingpt & 4.71 & 2.99 & 2.22 & 111.7 & 19.85 & 9.18 \\
Fiqa & 4.29 & 2.87 & 2.22 & 73.12 & 17.63 & 9.22 \\
Twitter & 4.78 & 2.99 & 2.19 & 118.7 & 19.82 & 8.97 \\
Wikitext & 4.63 & 2.94 & 2.18 & 102.4 & 18.85 & 8.88 \\
\hline
\textbf{Average} & \textbf{4.47} & \textbf{2.88} & \textbf{2.17} & \textbf{91.37} & \textbf{17.90} & \textbf{8.73} \\
\hline
\end{tabular}
\end{table}

