% FinGPT Sentiment Dataset: Evaluation Results
% Training: FinGPT Sentiment (FinGPT/fingpt-sentiment-train, 19M tokens)
% All models trained with LR=2e-5

\begin{table}[h]
\centering
\caption[FinGPT Sentiment: Evaluation Results]{FinGPT Sentiment Dataset: Evaluation Across Multiple Datasets}
\label{tab:fingpt_results}
\begin{tabular}{l|ccc|ccc}
\hline
\textbf{Eval Dataset} & \multicolumn{3}{c|}{\textbf{Cross-Entropy Loss}} & \multicolumn{3}{c}{\textbf{Perplexity}} \\
\cline{2-4} \cline{5-7}
  & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} & \textbf{0.6B} & \textbf{1.7B} & \textbf{4B} \\
Alpaca & 3.57 & 2.55 & 2.11 & 35.55 & 12.78 & 8.27 \\
Financial News & 3.36 & 2.45 & 2.07 & 28.72 & 11.58 & 7.92 \\
Financial QA & 3.66 & 2.38 & 1.83 & 38.96 & 10.85 & 6.24 \\
SEC Reports & 3.53 & 2.31 & 1.82 & 33.97 & 10.12 & 6.20 \\
\textbf{FinGPT} & \textbf{3.49} & \textbf{2.26} & \textbf{1.73} & \textbf{32.78} & \textbf{9.56} & \textbf{5.67} \\
FiQA & 3.57 & 2.55 & 2.10 & 35.64 & 12.79 & 8.16 \\
Twitter & 3.68 & 2.40 & 1.87 & 39.54 & 11.05 & 6.46 \\
Wikitext & 3.66 & 2.44 & 1.99 & 38.70 & 11.46 & 7.29 \\
\hline
\textbf{Average} & \textbf{3.56} & \textbf{2.42} & \textbf{1.94} & \textbf{35.48} & \textbf{11.27} & \textbf{7.03} \\
\hline
\end{tabular}
\end{table}
