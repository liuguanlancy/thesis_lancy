@article{devlin2018bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{yang2024qwen2,
  title={Qwen2 Technical Report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@misc{qwen3,
  title={Qwen3: Open Large Language Models},
  author={{Qwen Team}},
  howpublished={GitHub repository},
  year={2024},
  url={https://github.com/QwenLM/Qwen},
  note={Accessed 2025-10-03}
}

@article{gururangan2020don,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020}
}

@article{xia2023sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}

@article{araci2019finbert,
  title={Finbert: Financial sentiment analysis with pre-trained language models},
  author={Araci, Dogu},
  journal={arXiv preprint arXiv:1908.10063},
  year={2019}
}

@article{liu2023fingpt,
  title={FinGPT: Open-source financial large language models},
  author={Liu, Xiao-Yang and Chen, Guoxuan and Wang, Hongyang and Zhang, Daochen and Ruan, Jiechao},
  journal={arXiv preprint arXiv:2306.06031},
  year={2023}
}

@inproceedings{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{smith2017cyclical,
  title={Cyclical learning rates for training neural networks},
  author={Smith, Leslie N},
  journal={2017 IEEE winter conference on applications of computer vision (WACV)},
  pages={464--472},
  year={2017}
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is All you Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems 30},
  pages={5998--6008},
  year={2017},
  url={https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@inproceedings{devlin2019bert,
  title={{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of NAACL-HLT 2019},
  pages={4171--4186},
  year={2019},
  doi={10.18653/v1/n19-1423},
  url={https://doi.org/10.18653/v1/n19-1423}
}

@article{tay2022ul2,
  title={UL2: Unifying Language Learning Paradigms},
  author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Shakeri, Siamak and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Zhou, Denny and Houlsby, Neil and Metzler, Donald},
  journal={arXiv preprint arXiv:2205.05131},
  year={2022},
  url={https://arxiv.org/abs/2205.05131}
}

@article{mccandlish2018empirical,
  title={An Empirical Model of Large-Batch Training},
  author={McCandlish, Sam and Kaplan, Jared and Amodei, Dario and OpenAI Dota Team},
  journal={arXiv preprint arXiv:1812.06162},
  year={2018},
  url={https://arxiv.org/abs/1812.06162}
}

@inproceedings{rajbhandari2020zero,
  title={{ZeRO}: Memory optimizations Toward Training Trillion Parameter Models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  publisher={IEEE},
  doi={10.1109/SC41405.2020.00024},
  url={https://doi.org/10.1109/SC41405.2020.00024}
}

@article{narayanan2021efficient,
  title={Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-{LM}},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay Anand and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
  journal={arXiv preprint arXiv:2104.04473},
  year={2021},
  url={https://arxiv.org/abs/2104.04473}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={41--48},
  year={2009},
  publisher={ACM},
  doi={10.1145/1553374.1553380}
}

@article{wu2022opt,
  title={{OPT}: Open Pre-trained Transformer Language Models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022},
  url={https://arxiv.org/abs/2205.01068}
}

@article{xie2023doremi,
  title={DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining},
  author={Xie, Sang Michael and Pham, Hieu and Dong, Xuanyi and Du, Nan and Liu, Hanxiao and Lu, Yifeng and Liang, Percy and Le, Quoc V. and Ma, Tengyu and Yu, Adams Wei},
  journal={arXiv preprint arXiv:2305.10429},
  year={2023},
  url={https://arxiv.org/abs/2305.10429}
}

@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={140:1--140:67},
  year={2020},
  url={https://jmlr.org/papers/v21/20-074.html}
}

@article{arivazhagan2019massively,
  title={Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges},
  author={Arivazhagan, Naveen and Bapna, Ankur and Firat, Orhan and Lepikhin, Dmitry and Johnson, Melvin and Krikun, Maxim and Chen, Mia Xu and Cao, Yuan and Foster, George F. and Cherry, Colin and Macherey, Wolfgang and Chen, Zhifeng and Wu, Yonghui},
  journal={arXiv preprint arXiv:1907.05019},
  year={2019},
  url={http://arxiv.org/abs/1907.05019}
}

@article{longpre2023pretrainer,
  title={A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, \& Toxicity},
  author={Longpre, Shayne and Hou, Yao and Deshpande, Aakanksha and He, He and Sellam, Thibault and Tamkin, Alex and Petrov, Slav and Zhou, Denny and Wei, Jason and Tay, Yi and Le, Quoc V. and others},
  journal={arXiv preprint arXiv:2305.13169},
  year={2023},
  url={https://arxiv.org/abs/2305.13169}
}

@inbook{mccloskey1989catastrophic,
  title={Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
  author={McCloskey, Michael and Cohen, Neal J.},
  booktitle={Psychology of Learning and Motivation},
  pages={109--165},
  year={1989},
  publisher={Elsevier},
  doi={10.1016/S0079-7421(08)60536-8}
}

@article{french1999catastrophic,
  title={Catastrophic forgetting in connectionist networks},
  author={French, Robert M},
  journal={Trends in Cognitive Sciences},
  volume={3},
  number={4},
  pages={128--135},
  year={1999},
  publisher={Elsevier},
  doi={10.1016/S1364-6613(99)01294-2}
}

@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  journal={Proceedings of the National Academy of Sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  doi={10.1073/pnas.1611835114}
}

@book{quinonero2009dataset,
  title={Dataset Shift in Machine Learning},
  editor={Qui{\~n}onero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
  publisher={MIT Press},
  year={2008},
  doi={10.7551/mitpress/9780262170055.001.0001},
  url={https://doi.org/10.7551/mitpress/9780262170055.001.0001}
}

@article{aharoni2020unsupervised,
  title={Unsupervised Domain Clusters in Pretrained Language Models},
  author={Aharoni, Roee and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2004.02105},
  year={2020},
  url={https://arxiv.org/abs/2004.02105}
}

@article{wu2023bloomberggpt,
  title={BloombergGPT: A Large Language Model for Finance},
  author={Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David S. and Mann, Gideon},
  journal={arXiv preprint arXiv:2303.17564},
  year={2023},
  url={https://arxiv.org/abs/2303.17564}
}

@article{chen2021finqa,
  title={FinQA: A Dataset of Numerical Reasoning over Financial Data},
  author={Chen, Zhiyu and Chen, Wenhu and Smiley, Charese and Shah, Sameena and Borova, Iana and Langdon, Dylan and Moussa, Reema and Beane, Matt and Huang, Ting-Hao and Routledge, Bryan and Wang, William Yang},
  journal={arXiv preprint arXiv:2109.00122},
  year={2021},
  url={https://arxiv.org/abs/2109.00122}
}

@article{yang2020finbert,
  title={FinBERT: A Pretrained Language Model for Financial Communications},
  author={Yang, Yi and UY, Mark Christopher Siy and Huang, Allen},
  journal={arXiv preprint arXiv:2006.08097},
  year={2020},
  url={https://arxiv.org/abs/2006.08097}
}

@article{yang2023fingpt,
  title={FinGPT: Open-Source Financial Large Language Models},
  author={Yang, Hongyang and Liu, Xiao{-}Yang and Wang, Christina Dan},
  journal={arXiv preprint arXiv:2306.06031},
  year={2023},
  url={https://arxiv.org/abs/2306.06031}
}

@article{sanh2022multitask,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Le Scao, Teven and Raja, Arun and others},
  journal={arXiv preprint arXiv:2110.08207},
  year={2022},
  url={https://arxiv.org/abs/2110.08207}
}

@article{mitra2023orca2,
  title={Orca 2: Teaching Small Language Models How to Reason},
  author={Mitra, Arindam and Del Corro, Luciano and Mahajan, Shweti and Codas, Andres and Simoes, Clarisse and Agarwal, Sahaj and Chen, Xuxi and Razdaibiedina, Anastasia and Jones, Erik and Aggarwal, Kriti and Palangi, Hamid and Zheng, Guoqing and Rosset, Corby and Khanpour, Hamed and Awadallah, Ahmed},
  journal={arXiv preprint arXiv:2311.11045},
  year={2023},
  url={https://arxiv.org/abs/2311.11045}
}

@article{gao2020pile,
  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sidney and Anthony, Laurence and Golding, Xenia and Hoppe, Horace and Foster, Connor and Phang, Jason and He, Anish and Thite, Aman and Nabeshima, Andy and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2021},
  url={https://arxiv.org/abs/2101.00027}
}

@misc{eu2016gdpr,
  title={{Regulation (EU) 2016/679} of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data (General Data Protection Regulation)},
  howpublished={Official Journal of the European Union},
  year={2016},
  url={https://eur-lex.europa.eu/eli/reg/2016/679/oj}
}

@article{kingma2014adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P. and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014},
  url={https://arxiv.org/abs/1412.6980}
}

@article{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021},
  url={https://arxiv.org/abs/2106.09685}
}

@inproceedings{pan2010transfer,
  title={A Survey on Transfer Learning},
  author={Pan, Sinno Jialin and Yang, Qiang},
  booktitle={IEEE Transactions on Knowledge and Data Engineering},
  volume={22},
  pages={1345--1359},
  year={2010},
  doi={10.1109/TKDE.2009.191}
}

@article{zhuang2020comprehensive,
  title={A Comprehensive Survey on Transfer Learning},
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE},
  volume={109},
  pages={43--76},
  year={2021},
  doi={10.1109/JPROC.2020.3004555}
}

@article{huang2023finbert,
  title={FinBERT: A Large Language Model for Extracting Information from Financial Text},
  author={Huang, Allen H. and Wang, Hui and Yang, Yi},
  journal={Contemporary Accounting Research},
  volume={40},
  number={2},
  pages={806--841},
  year={2023},
  doi={10.1111/1911-3846.12832}
}

@misc{team2024gemma,
  title={Gemma: Open Models Based on Gemini Research and Technology},
  author={Gemma Team and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024},
  url={https://arxiv.org/abs/2403.08295}
}

@misc{javaheripi2023phi,
  title={Phi-2: The surprising power of small language models},
  author={Javaheripi, Mojan and Bubeck, S{\'e}bastien and Abdin, Marah and Aneja, Jyoti and Bubeck, Sebastien and Mendes, Caio C{\'e}sar Teodoro and Chen, Weizhu and Del Giorno, Allie and Eldan, Ronen and Gopi, Sivakanth and others},
  howpublished={Microsoft Research Blog},
  year={2023},
  url={https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/}
}

@article{lee2022surgical,
  title={Surgical Fine-Tuning Improves Adaptation to Distribution Shifts},
  author={Lee, Yoonho and Chen, Annie S. and Tajwar, Fahim and Kumar, Ananya and Yao, Huaxiu and Liang, Percy and Finn, Chelsea},
  journal={arXiv preprint arXiv:2210.11466},
  year={2022},
  url={https://arxiv.org/abs/2210.11466}
}
