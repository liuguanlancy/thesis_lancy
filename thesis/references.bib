@article{brown2020language,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {1877--1901},
  year    = {2020}
}


@article{radford2019language,
  title   = {Language models are unsupervised multitask learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  volume  = {1},
  number  = {8},
  pages   = {9},
  year    = {2019}
}


@article{kaplan2020scaling,
  title   = {Scaling laws for neural language models},
  author  = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal = {arXiv preprint arXiv:2001.08361},
  year    = {2020}
}


@inproceedings{hoffmann2022training,
  title     = {Training Compute-Optimal Large Language Models},
  author    = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {35},
  pages     = {30016--30030},
  year      = {2022},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/c1e2faff6f588870935f114ebe04a3e5-Abstract-Conference.html}
}


@article{touvron2023llama,
  title   = {Llama: Open and efficient foundation language models},
  author  = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal = {arXiv preprint arXiv:2302.13971},
  year    = {2023}
}


@article{yang2024qwen2,
  title   = {Qwen2 technical report},
  author  = {Team, Qwen and others},
  journal = {arXiv preprint arXiv:2407.10671},
  volume  = {2},
  pages   = {3},
  year    = {2024}
}

@article{qwen3,
  title   = {Qwen3 technical report},
  author  = {Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and others},
  journal = {arXiv preprint arXiv:2505.09388},
  year    = {2025}
}


@article{gururangan2020don,
  title   = {Don't stop pretraining: Adapt language models to domains and tasks},
  author  = {Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal = {arXiv preprint arXiv:2004.10964},
  year    = {2020}
}


@article{xia2023sheared,
  title   = {Sheared llama: Accelerating language model pre-training via structured pruning},
  author  = {Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal = {arXiv preprint arXiv:2310.06694},
  year    = {2023}
}


@article{araci2019finbert,
  title   = {Finbert: Financial sentiment analysis with pre-trained language models},
  author  = {Araci, Dogu},
  journal = {arXiv preprint arXiv:1908.10063},
  year    = {2019}
}

@article{merity2016pointer,
  title   = {Pointer sentinel mixture models},
  author  = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal = {arXiv preprint arXiv:1609.07843},
  year    = {2016}
}


@inproceedings{vaswani2017attention,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  editor    = {Isabelle Guyon and
               Ulrike von Luxburg and
               Samy Bengio and
               Hanna M. Wallach and
               Rob Fergus and
               S. V. N. Vishwanathan and
               Roman Garnett},
  title     = {Attention is All you Need},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, December 4-9, 2017,
               Long Beach, CA, {USA}},
  pages     = {5998--6008},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{devlin2019bert,
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of NAACL-HLT 2019},
  pages     = {4171--4186},
  year      = {2019},
  doi       = {10.18653/v1/n19-1423},
  url       = {https://doi.org/10.18653/v1/n19-1423}
}

@article{tay2022ul2,
  title   = {Ul2: Unifying language learning paradigms},
  author  = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Shakeri, Siamak and Bahri, Dara and Schuster, Tal and others},
  journal = {arXiv preprint arXiv:2205.05131},
  year    = {2022},
  url     = {https://arxiv.org/abs/2205.05131}
}


@article{mccandlish2018empirical,
  title   = {An empirical model of large-batch training},
  author  = {McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Team, OpenAI Dota},
  journal = {arXiv preprint arXiv:1812.06162},
  year    = {2018}
}


@inproceedings{rajbhandari2020zero,
  author    = {Samyam Rajbhandari and
               Jeff Rasley and
               Olatunji Ruwase and
               Yuxiong He},
  editor    = {Christine Cuicchi and
               Irene Qualters and
               William T. Kramer},
  title     = {ZeRO: memory optimizations toward training trillion parameter models},
  booktitle = {Proceedings of the International Conference for High Performance Computing,
               Networking, Storage and Analysis, {SC} 2020, Virtual Event / Atlanta,
               Georgia, USA, November 9-19, 2020},
  pages     = {20},
  publisher = {{IEEE/ACM}},
  year      = {2020},
  url       = {https://doi.org/10.1109/SC41405.2020.00024},
  doi       = {10.1109/SC41405.2020.00024},
  timestamp = {Wed, 04 May 2022 13:02:27 +0200},
  biburl    = {https://dblp.org/rec/conf/sc/RajbhandariRRH20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{narayanan2021efficient,
  title     = {Efficient large-scale language model training on gpu clusters using megatron-lm},
  author    = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle = {Proceedings of the international conference for high performance computing, networking, storage and analysis},
  pages     = {1--15},
  year      = {2021}
}


@inproceedings{bengio2009curriculum,
  title     = {Curriculum learning},
  author    = {Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle = {Proceedings of the 26th annual international conference on machine learning},
  pages     = {41--48},
  year      = {2009}
}


@article{wu2022opt,
  title   = {Opt: Open pre-trained transformer language models},
  author  = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal = {arXiv preprint arXiv:2205.01068},
  year    = {2022}
}


@article{xie2023doremi,
  title   = {Doremi: Optimizing data mixtures speeds up language model pretraining},
  author  = {Xie, Sang Michael and Pham, Hieu and Dong, Xuanyi and Du, Nan and Liu, Hanxiao and Lu, Yifeng and Liang, Percy S and Le, Quoc V and Ma, Tengyu and Yu, Adams Wei},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  pages   = {69798--69818},
  year    = {2023}
}


@article{raffel2020exploring,
  author    = {Colin Raffel and
               Noam Shazeer and
               Adam Roberts and
               Katherine Lee and
               Sharan Narang and
               Michael Matena and
               Yanqi Zhou and
               Wei Li and
               Peter J. Liu},
  title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
               Transformer},
  journal   = {J. Mach. Learn. Res.},
  volume    = {21},
  pages     = {140:1--140:67},
  year      = {2020},
  url       = {https://jmlr.org/papers/v21/20-074.html},
  timestamp = {Wed, 11 Sep 2024 14:41:27 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{arivazhagan2019massively,
  title   = {Massively multilingual neural machine translation in the wild: Findings and challenges},
  author  = {Arivazhagan, Naveen and Bapna, Ankur and Firat, Orhan and Lepikhin, Dmitry and Johnson, Melvin and Krikun, Maxim and Chen, Mia Xu and Cao, Yuan and Foster, George and Cherry, Colin and others},
  journal = {arXiv preprint arXiv:1907.05019},
  year    = {2019}
}


@inproceedings{longpre2023pretrainer,
  title     = {A pretrainerâ€™s guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity},
  author    = {Longpre, Shayne and Yauney, Gregory and Reif, Emily and Lee, Katherine and Roberts, Adam and Zoph, Barret and Zhou, Denny and Wei, Jason and Robinson, Kevin and Mimno, David and others},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages     = {3245--3276},
  year      = {2024}
}


@incollection{mccloskey1989catastrophic,
  title     = {Catastrophic interference in connectionist networks: The sequential learning problem},
  author    = {McCloskey, Michael and Cohen, Neal J},
  booktitle = {Psychology of learning and motivation},
  volume    = {24},
  pages     = {109--165},
  year      = {1989},
  publisher = {Elsevier}
}


@article{french1999catastrophic,
  title     = {Catastrophic forgetting in connectionist networks},
  author    = {French, Robert M},
  journal   = {Trends in cognitive sciences},
  volume    = {3},
  number    = {4},
  pages     = {128--135},
  year      = {1999},
  publisher = {Elsevier}
}

@article{kirkpatrick2017overcoming,
  title     = {Overcoming catastrophic forgetting in neural networks},
  author    = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal   = {Proceedings of the national academy of sciences},
  volume    = {114},
  number    = {13},
  pages     = {3521--3526},
  year      = {2017},
  publisher = {National Academy of Sciences}
}


@book{quinonero2009dataset,
  title     = {Dataset shift in machine learning},
  author    = {Qui{\~n}onero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D},
  year      = {2022},
  publisher = {Mit Press}
}

@article{aharoni2020unsupervised,
  title   = {Unsupervised domain clusters in pretrained language models},
  author  = {Aharoni, Roee and Goldberg, Yoav},
  journal = {arXiv preprint arXiv:2004.02105},
  year    = {2020}
}


@article{wu2023bloomberggpt,
  title   = {Bloomberggpt: A large language model for finance},
  author  = {Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  journal = {arXiv preprint arXiv:2303.17564},
  year    = {2023}
}


@article{chen2021finqa,
  title   = {Finqa: A dataset of numerical reasoning over financial data},
  author  = {Chen, Zhiyu and Chen, Wenhu and Smiley, Charese and Shah, Sameena and Borova, Iana and Langdon, Dylan and Moussa, Reema and Beane, Matt and Huang, Ting-Hao and Routledge, Bryan and others},
  journal = {arXiv preprint arXiv:2109.00122},
  year    = {2021}
}


@article{yang2020finbert,
  title   = {Finbert: A pretrained language model for financial communications},
  author  = {Yang, Yi and Uy, Mark Christopher Siy and Huang, Allen},
  journal = {arXiv preprint arXiv:2006.08097},
  year    = {2020}
}

@article{yang2023fingpt,
  title   = {FinGPT: Open-Source Financial Large Language Models},
  author  = {Yang, Hongyang and Liu, Xiao{-}Yang and Wang, Christina Dan},
  journal = {arXiv preprint arXiv:2306.06031},
  year    = {2023},
  url     = {https://arxiv.org/abs/2306.06031}
}


@inproceedings{sanh2022multitask,
  author    = {Victor Sanh and
               Albert Webson and
               Colin Raffel and
               Stephen H. Bach and
               Lintang Sutawika and
               Zaid Alyafeai and
               Antoine Chaffin and
               Arnaud Stiegler and
               Arun Raja and
               Manan Dey and
               M Saiful Bari and
               Canwen Xu and
               Urmish Thakker and
               Shanya Sharma Sharma and
               Eliza Szczechla and
               Taewoon Kim and
               Gunjan Chhablani and
               Nihal V. Nayak and
               Debajyoti Datta and
               Jonathan Chang and
               Mike Tian{-}Jian Jiang and
               Han Wang and
               Matteo Manica and
               Sheng Shen and
               Zheng Xin Yong and
               Harshit Pandey and
               Rachel Bawden and
               Thomas Wang and
               Trishala Neeraj and
               Jos Rozen and
               Abheesht Sharma and
               Andrea Santilli and
               Thibault F{\'{e}}vry and
               Jason Alan Fries and
               Ryan Teehan and
               Teven Le Scao and
               Stella Biderman and
               Leo Gao and
               Thomas Wolf and
               Alexander M. Rush},
  title     = {Multitask Prompted Training Enables Zero-Shot Task Generalization},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=9Vrb9D0WI4},
  timestamp = {Tue, 18 Feb 2025 15:42:06 +0100},
  biburl    = {https://dblp.org/rec/conf/iclr/SanhWRBSACSRDBX22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{mitra2023orca,
  title   = {Orca 2: Teaching small language models how to reason},
  author  = {Mitra, Arindam and Del Corro, Luciano and Mahajan, Shweti and Codas, Andres and Simoes, Clarisse and Agarwal, Sahaj and Chen, Xuxi and Razdaibiedina, Anastasia and Jones, Erik and Aggarwal, Kriti and others},
  journal = {arXiv preprint arXiv:2311.11045},
  year    = {2023}
}


@article{gao2020pile,
  title   = {The pile: An 800gb dataset of diverse text for language modeling},
  author  = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal = {arXiv preprint arXiv:2101.00027},
  year    = {2020}
}


@misc{eu2016gdpr,
  title        = {{Regulation (EU) 2016/679} of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data (General Data Protection Regulation)},
  howpublished = {Official Journal of the European Union},
  year         = {2016},
  url          = {https://eur-lex.europa.eu/eli/reg/2016/679/oj}
}


@article{kingma2014adam,
  title   = {A method for stochastic optimization},
  author  = {Adam, Kingma DP Ba J and others},
  journal = {arXiv preprint arXiv:1412.6980},
  volume  = {1412},
  number  = {6},
  year    = {2014}
}


@article{hu2021lora,
  title   = {Lora: Low-rank adaptation of large language models.},
  author  = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal = {ICLR},
  volume  = {1},
  number  = {2},
  pages   = {3},
  year    = {2022}
}

@article{pan2010transfer,
  title     = {A survey on transfer learning},
  author    = {Pan, Sinno Jialin and Yang, Qiang},
  journal   = {IEEE Transactions on knowledge and data engineering},
  volume    = {22},
  number    = {10},
  pages     = {1345--1359},
  year      = {2009},
  publisher = {IEEE}
}

@article{zhuang2020comprehensive,
  title     = {A comprehensive survey on transfer learning},
  author    = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal   = {Proceedings of the IEEE},
  volume    = {109},
  number    = {1},
  pages     = {43--76},
  year      = {2020},
  publisher = {Ieee}
}


@article{huang2023finbert,
  title     = {FinBERT: A large language model for extracting information from financial text},
  author    = {Huang, Allen H and Wang, Hui and Yang, Yi},
  journal   = {Contemporary Accounting Research},
  volume    = {40},
  number    = {2},
  pages     = {806--841},
  year      = {2023},
  publisher = {Wiley Online Library}
}


@article{team2024gemma,
  title   = {Gemma: Open models based on gemini research and technology},
  author  = {Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal = {arXiv preprint arXiv:2403.08295},
  year    = {2024}
}


@article{javaheripi2023phi,
  title   = {Phi-2: The surprising power of small language models},
  author  = {Javaheripi, Mojan and Bubeck, S{\'e}bastien and Abdin, Marah and Aneja, Jyoti and Bubeck, Sebastien and Mendes, Caio C{\'e}sar Teodoro and Chen, Weizhu and Del Giorno, Allie and Eldan, Ronen and Gopi, Sivakanth and others},
  journal = {Microsoft Research Blog},
  volume  = {1},
  number  = {3},
  pages   = {3},
  year    = {2023}
}


@article{lee2022surgical,
  title   = {Surgical fine-tuning improves adaptation to distribution shifts},
  author  = {Lee, Yoonho and Chen, Annie S and Tajwar, Fahim and Kumar, Ananya and Yao, Huaxiu and Liang, Percy and Finn, Chelsea},
  journal = {arXiv preprint arXiv:2210.11466},
  year    = {2022}
}