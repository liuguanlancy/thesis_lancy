\chapter{Background and Related Work}

This chapter covers four areas we need for the rest of the thesis. Financial NLP. Language‑model pretraining. Data mixture design. Domain adaptation. We aim for focused context, not a complete survey. Put another way, we include what we use later.

\section{Financial NLP}

\subsection{Tasks and Settings in Financial NLP}

Financial NLP has several common tasks. Sentiment analysis over news and social media. Question answering on regulatory text. Numerical reasoning in corporate reports. Information extraction from SEC filings \parencite{araci2019finbert, chen2021finqa}.

Compared with general NLP, this area brings special challenges. Specialized vocabulary shows up everywhere. Terms like ``alpha'', ``beta'', ``EBITDA''. Causal chains in market analysis need domain‑specific reasoning. Reading financial statements needs numerical grounding. And time matters—around earnings releases, for example \parencite{wu2023bloomberggpt, araci2019finbert}. Often these appear in the same document. Sometimes not.

\subsection{Existing Financial Language Models}

We focus on models relevant to our setup. \textbf{BloombergGPT} \parencite{wu2023bloomberggpt} is a 50B model trained on a 51\%/49\% financial/general mix. Strong performance on finance benchmarks. Keeps general ability too.

\textbf{FinBERT} variants \parencite{araci2019finbert, yang2020finbert} use continued pretraining—BERT on financial corpora—which improves news sentiment. More recently, \textbf{FinGPT} \parencite{yang2023fingpt} explored open‑source, instruction‑tuned approaches for finance.

We cite these as context. Not for head‑to‑head comparison.

\subsection{Domain-Specific Challenges}

Three issues show up repeatedly.

First, privacy. Institutions cannot upload sensitive data to external APIs. Portfolios, strategies, client information—all sensitive. So models must run locally \parencite{wu2023bloomberggpt}. No choice there.

Second, data scarcity. Curated financial corpora are much smaller than general web text. So data‑efficient training becomes important.

Third, rapid vocabulary change. Terms like ``DeFi'' and ``ESG'' appear fast and shift with markets. Timely adaptation is needed.

\section{Language Model Pretraining}

\subsection{Pretraining Objectives and Architecture}

Most current work uses \textbf{causal language modeling} (CLM): predict the next token from previous context \parencite{radford2019language, brown2020language}. The objective is self‑supervised. It scales to large unlabeled corpora and provides a clean training signal.

Architecturally, decoder‑only transformers dominate—GPT, LLaMA, Qwen. Multi‑head self‑attention captures long‑range dependencies. Feed‑forward layers add non‑linearity \parencite{vaswani2017attention, touvron2023llama}. Standard setup.

\subsection{Scaling Laws and Model Size Effects}

\textcite{kaplan2020scaling} showed power‑law links: model size, dataset size, compute, and performance are connected. Larger models can be more sample‑efficient. This pushed work toward billion‑parameter scales.

But the story is more complicated. \textcite{hoffmann2022training} argued many models are undertrained for their size (the Chinchilla view). And \textcite{tay2022ul2} emphasized that objectives and data quality also matter. Not just size alone.

\textbf{Hyperparameter sensitivity} gets less attention. \textcite{mccandlish2018empirical} noted that optimal learning rates often decrease with model size. For 0.6B--4B models in specialized domains, systematic studies are limited. Many scaling‑law papers assume ``proper tuning'' but do not say what procedures they used. This hides practical effort.

Tuning matters at this scale. In our experiments, all main runs used LR=2e-5. In a few cases we lowered LR to stabilize training. Simple adjustments, but necessary.

\subsection{Computational and Memory Considerations}

Training large language models requires large compute. A 1B‑parameter model in 32‑bit uses about 4GB for parameters. Optimizer states can double or triple that \parencite{rajbhandari2020zero,kingma2014adam}.

For 0.6B--4B models, we need memory‑efficient approaches. Mixed precision (bfloat16). Gradient accumulation. Activation checkpointing. Parameter‑efficient methods like LoRA. These make training feasible on enterprise GPUs: RTX A6000 (48GB), A100 (40GB), H100 (80GB) \parencite{narayanan2021efficient,hu2021lora}.

In our setup we use bfloat16. And gradient accumulation when needed.

\section{Data Mixture Strategies}

\subsection{Curriculum Learning and Sequential Mixing}

\textbf{Curriculum learning} orders data from easier to harder. Or from general to specialized \parencite{bengio2009curriculum}. \textcite{wu2022opt} used curriculum in OPT pretraining. Increasing difficulty over time. In finance, a natural ordering is Wikipedia → news → SEC filings.

But evidence at large scale is mixed \parencite{longpre2023pretrainer}. Some works report limited gains for masked LM. Others see gains in narrower settings. Many systems sample from mixtures instead of strict curricula \parencite{raffel2020exploring,wu2022opt}. So in practice, simultaneous mixing is more common.

\subsection{Simultaneous Mixture Approaches}

Another option is \textbf{simultaneous mixture}. Sample from multiple datasets throughout training. \textcite{raffel2020exploring} (T5) used a multi‑task mixture. Task prefixes included. Diverse pretraining improved downstream generalization.

\textcite{xie2023doremi} proposed DoReMi. Adjust domain weights during training using validation perplexity. It outperformed static mixtures on The Pile. This approach is widely used.

\textbf{BloombergGPT} \parencite{wu2023bloomberggpt} mixed 51\% financial with 49\% general data. At the token level—The Pile and C4. The balance kept general skills and added domain strength. That study evaluated a single 50B model. How mixture and size interact (0.6B vs 4B) is less explored.

We test this across three sizes. As shown in \Cref{fig:scaling_comparison_all,tab:mixed_financial_results,tab:mixed_wiki_financial_results}, mixed financial datasets reach 21.55 ppl @ 4B. Wiki+Financial mixtures get 26.69 ppl @ 4B. About 24\% worse.

\subsection{Domain Proportions and Sampling Strategies}

Choosing domain proportions is not straightforward. No single rule works for all. We highlight three common strategies.

\textbf{1. Temperature sampling} \parencite{arivazhagan2019massively}. Sample from dataset $d$ with probability $p_d \propto n_d^{1/T}$. Here $n_d$ is dataset size. $T$ is temperature. $T < 1$ upsamples small datasets. $T > 1$ downsamples them. Simple. Works reasonably well.

\textbf{2. Capping strategies} \parencite{longpre2023pretrainer}. Cap the largest dataset(s) at a threshold. Say 50\% of total tokens. This prevents dominance. Then sample others proportionally. Capping stops a single source from taking over. Keeps variety.

\textbf{3. Equal mixing} \parencite{sanh2022multitask}. Assign equal sampling probability to each dataset. Size does not matter. This maximizes task diversity. But it can undersample large datasets.

We use a \textbf{50\% capping strategy} (``50cap'') for financial mixtures. Details in Chapter 3. This balances diversity and data efficiency.

\section{Domain Adaptation and Transfer Learning}

\subsection{Cross-Domain Transfer in Language Models}

\textbf{Transfer learning} has been standard since BERT \parencite{devlin2019bert,pan2010transfer,zhuang2020comprehensive}. Pretrain on broad data. Then fine‑tune for a target domain. We assume general knowledge transfers. This assumption holds in many cases. But not always.

\textcite{gururangan2020don} showed \textbf{domain‑adaptive pretraining} improves performance. Continued pretraining on domain corpora in biomedicine, computer science, news, and reviews. General pretraining alone is not enough for specialized use.

In finance, \textcite{araci2019finbert} got better results via continued pretraining on financial news. \textcite{yang2020finbert} added task‑adaptive pretraining. \textcite{huang2023finbert} found domain‑specific pretraining beats general models on financial IE.

These are mostly BERT‑style. Classification‑focused. Domain adaptation for \textit{causal, generative} LMs in finance is less studied. Parameter‑efficient methods like surgical fine‑tuning \parencite{lee2022surgical} suggest that selective adaptation can help transfer. And reduce forgetting.

\subsection{Catastrophic Forgetting and Stability}

\textbf{Catastrophic forgetting} is a classic issue. Training further on a domain can erase general knowledge \parencite{mccloskey1989catastrophic, french1999catastrophic}. \textcite{kirkpatrick2017overcoming} proposed Elastic Weight Consolidation (EWC). Protect important parameters.

With mixtures, \textit{simultaneous mixing} of general and domain data acts as implicit regularization. The model stays exposed to diverse distributions \parencite{arivazhagan2019massively,raffel2020exploring}.

\subsection{Distribution Shift and Domain Mismatch}

\textbf{Distribution shift} hurts generalization. Differences between training and evaluation \parencite{quinonero2009dataset}. In finance, this shows up in three ways.

Vocabulary shift—financial terms vs general language. Discourse differences—analyst reports vs encyclopedic text. Formatting differences—tables in 10‑K vs narrative news.

\textcite{aharoni2020unsupervised} showed domain mismatch strongly degrades out‑of‑distribution performance. This motivates mixtures that cover sub‑domains.

We test this directly. Does pretraining on high‑quality general text (WikiText) transfer to financial evaluation sets? Does domain mismatch require in‑domain pretraining? And does mixing in‑domain datasets give better generalization than training on a single dataset? Sentiment, Q\&A, news, reports mixed together. Does it help?

\subsection{Related Empirical Studies}

Several studies guide our setup. \textcite{xie2023doremi} showed dynamic mixture optimization can beat static mixes on The Pile. But it needs validation data and multiple runs. Useful. But not always practical.

\textcite{longpre2023pretrainer} surveyed practitioners. Capping and temperature sampling are common in production. \textcite{mitra2023orca2} (Orca‑2) showed that diverse instruction formats help reasoning generalization. This suggests \textit{intra‑domain diversity} can matter. Multiple financial datasets—as much as domain specialization.

What is missing is a systematic look at \textbf{dataset size effects} for mixtures. When is a dataset large enough for standalone pretraining? When does mixing help? When does it hurt? How do these patterns change with model size?

These questions shape our experimental design in Chapter 3.
