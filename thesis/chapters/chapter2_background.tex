\chapter{Background and Related Work}

This chapter reviews research that informs our study of data mixture effects in financial language model pretraining. We start with financial NLP, then pretraining fundamentals, prior mixture strategies, and finally domain adaptation and transfer learning. Put another way: context → mechanisms → practice. Short and to the point. We do not try to survey everything. Just what we need later.

\section{Financial NLP}

\subsection{The Financial NLP Landscape}

Financial natural language processing covers many tasks: sentiment on news and social media, question answering on regulatory documents, numerical reasoning in reports, and information extraction from SEC filings \parencite{araci2019finbert, chen2021finqa}. The domain has specific challenges compared to general NLP: specialized vocabulary (e.g., ``alpha'', ``beta'', ``EBITDA''), domain reasoning patterns (e.g., causal chains in market analysis), numerical grounding (financial statements), and temporal dynamics (market events, earnings releases) \parencite{wu2023bloomberggpt, araci2019finbert}. These points guide our choices later. In our setup, they are constraints, not goals.

\subsection{Existing Financial Language Models}

Several finance‑focused language models appeared in recent years. \textbf{BloombergGPT} \parencite{wu2023bloomberggpt}, a 50‑billion‑parameter model, was pretrained on a mixture of 51\% financial and 49\% general data, showing strong performance on financial benchmarks while keeping general capabilities. \textbf{FinBERT} variants \parencite{araci2019finbert, yang2020finbert} adapted BERT to financial text via continued pretraining, improving sentiment analysis on financial news. More recently, \textbf{FinGPT} \parencite{yang2023fingpt} explored open‑source instruction‑tuning for financial tasks. Together, these lines show both scale‑first and adaptation‑first approaches.

\subsection{Domain-Specific Challenges}

Financial NLP faces three practical challenges. \textbf{First}, privacy concerns: financial institutions cannot upload sensitive data (portfolios, trading strategies, client information) to external APIs, so locally deployable models are needed \parencite{wu2023bloomberggpt}. \textbf{Second}, data scarcity: compared to general web text, curated financial corpora are smaller, so data-efficient training is important. \textbf{Third}, rapid vocabulary change: financial language shifts with market trends (e.g., ``DeFi'', ``ESG''), so models must adapt to new terms. These constraints motivate our focus on 0.6B–4B models.

\section{Language Model Pretraining}

\subsection{Pretraining Objectives and Architecture}

Modern language models mostly use \textbf{causal language modeling}: predict the next token from the context \parencite{radford2019language, brown2020language}. We follow this default. Architecturally, we use the usual decoder‑only transformer (GPT, LLaMA, Qwen): self‑attention for long context; feed‑forward blocks for the non‑linear part \parencite{vaswani2017attention, touvron2023llama}. Nothing fancy.

\subsection{Scaling Laws and Model Size Effects}

The work of \textcite{kaplan2020scaling} linked model size, dataset size, compute, and final performance by power laws. The core point—that larger models can be more sample efficient—pushed the field toward billion‑parameter scales. Later work added nuance: \textcite{hoffmann2022training} showed undertraining is common (Chinchilla); \textcite{tay2022ul2} emphasized objectives and data quality. Put simply: bigger helps, but details matter.

Critically, \textbf{hyperparameter sensitivity} is less studied. While \textcite{mccandlish2018empirical} noted that optimal learning rates can decrease with model size, systematic studies for models in the 0.6B–4B range—especially in specialized domains—remain limited. Many scaling-law papers assume good tuning without showing the details, which can hide training dynamics. In our work, all main runs used LR=2e-5; in a few cases we reduced LR pragmatically to stabilize training. We do not claim a general rule.

\subsection{Computational and Memory Considerations}

Training large language models requires substantial compute. A 1‑billion‑parameter model with 32‑bit precision uses roughly 4GB of memory for parameters alone, with optimizer states (e.g., Adam's momentum terms) doubling or tripling this requirement \parencite{rajbhandari2020zero,kingma2014adam}. For models in the 0.6B–4B range targeted here, memory‑efficient techniques like mixed‑precision (bfloat16), gradient accumulation, activation checkpointing, and parameter‑efficient fine‑tuning such as LoRA allow training on enterprise‑class GPUs (e.g., NVIDIA RTX A6000 48GB, A100 40GB, H100 80GB) \parencite{narayanan2021efficient,hu2021lora}. In practice, these tricks matter more than any single hyperparameter.

\section{Data Mixture Strategies}

\subsection{Curriculum Learning and Sequential Mixing}

\textbf{Curriculum learning} in language model pretraining involves carefully sequencing training data from easier to harder examples, or from general to specialized domains \parencite{bengio2009curriculum}. \textcite{wu2022opt} applied curriculum strategies in pretraining OPT models, progressively increasing data difficulty. In the financial domain, a natural curriculum might proceed from general Wikipedia text to financial news to technical SEC filings. However, empirical evidence for curriculum's effectiveness in large-scale pretraining remains mixed across objectives and domains \parencite{longpre2023pretrainer}. Some works report limited gains for masked language modeling at scale, while others show improvements in specialized settings; in practice, many production systems rely on mixture-based sampling rather than strict curricula \parencite{raffel2020exploring,wu2022opt}.

\subsection{Simultaneous Mixture Approaches}

An alternative to sequential mixing is \textbf{simultaneous mixture}: sample from multiple datasets throughout training. \textcite{raffel2020exploring} (T5) used a multi‑task mixture with task‑specific prefixes and found diverse pretraining helped downstream. \textcite{xie2023doremi} introduced DoReMi, which adjusts domain weights during training by validation perplexity, improving sample efficiency over static mixtures on The Pile.

\textbf{BloombergGPT} \parencite{wu2023bloomberggpt} mixed 51\% financial with 49\% general (The Pile, C4) at token level and showed balanced mixtures can keep general skills while adding domain strength. Their focus was a single 50B model. The interaction with model size (0.6B vs 4B) is less clear. Our runs across three sizes show mixed financial datasets (21.55 ppl @ 4B) outperform Wiki+Financial (26.69 ppl @ 4B, 24\% worse) and pure WikiText (41.96 ppl mean financial @ 4B, 95\% worse), per \Cref{fig:scaling_comparison_all,tab:mixed_financial_results,tab:mixed_wiki_financial_results}. For specialized use, domain purity seems to win over domain balance.

\subsection{Domain Proportions and Sampling Strategies}

Choosing domain proportions is not obvious. Three common strategies:

\textbf{1. Temperature sampling} \parencite{arivazhagan2019massively}: Sample from dataset $d$ with probability $p_d \propto n_d^{1/T}$ where $n_d$ is dataset size and $T$ is temperature. $T < 1$ upsamples small datasets; $T > 1$ downsamples them.

\textbf{2. Capping strategies} \parencite{longpre2023pretrainer}: Cap the largest dataset(s) at a threshold (e.g., 50\% of total tokens) to prevent dominance, then proportionally sample others. This ensures diversity even when one dataset is orders of magnitude larger.

\textbf{3. Equal mixing} \parencite{sanh2022multitask}: Assign equal sampling probability to each dataset regardless of size. This maximizes task diversity but may undersample large datasets.

This thesis uses a \textbf{50\% capping strategy} (``50cap'') for financial mixtures (details in Chapter 3) to balance diversity and efficiency. We chose it for simplicity and stability in our setup.

\section{Domain Adaptation and Transfer Learning}

\subsection{Cross-Domain Transfer in Language Models}

\textbf{Transfer learning}—pretraining on broad data then fine-tuning on specialized tasks—has been the common approach since BERT \parencite{devlin2019bert,pan2010transfer,zhuang2020comprehensive}. The assumption is that general linguistic knowledge transfers to domain applications. However, recent work shows nuance: \textcite{gururangan2020don} found that \textbf{domain-adaptive pretraining} (continued pretraining on domain corpora) improves performance across domains, suggesting general pretraining alone is not enough for specialized use.

In finance, \textcite{araci2019finbert} showed improvements from continued pretraining on financial news; \textcite{yang2020finbert} saw further gains with task-adaptive pretraining. More recently, \textcite{huang2023finbert} found that domain-specific pretraining outperforms general models on financial information extraction. However, these studies focus on BERT-style masked language models and classification tasks—the effectiveness of domain adaptation for \textit{generative causal language models} in financial pretraining is less studied. Advances in parameter-efficient fine-tuning, such as surgical fine-tuning \parencite{lee2022surgical}, suggest selective adaptation may improve transfer while mitigating catastrophic forgetting.

\subsection{Catastrophic Forgetting and Stability}

A key challenge in domain adaptation is \textbf{catastrophic forgetting}: when a pretrained model is further trained on domain-specific data, it may lose general knowledge \parencite{mccloskey1989catastrophic, french1999catastrophic}. \textcite{kirkpatrick2017overcoming} introduced Elastic Weight Consolidation (EWC) to mitigate forgetting by penalizing changes to important parameters. In the context of data mixtures, \textit{simultaneous mixing} of general and domain data can act as a form of implicit regularization, reducing forgetting by continuously exposing the model to diverse distributions \parencite{arivazhagan2019massively,raffel2020exploring}.

\subsection{Distribution Shift and Domain Mismatch}

\textbf{Distribution shift}—the gap between training and evaluation data—directly affects generalization \parencite{quinonero2009dataset}. In finance, this shows up as vocabulary (financial terms vs general), discourse (analytical reports vs encyclopedic text), and formatting (10‑K tables vs narrative news). \textcite{aharoni2020unsupervised} showed domain mismatch can severely degrade out‑of‑distribution performance, which motivates mixtures that cover sub‑domains.

Our thesis investigates this empirically: does pretraining purely on high-quality general corpora (WikiText) transfer to financial evaluation sets? Or does domain mismatch make in-domain pretraining necessary? And when mixing in-domain datasets (sentiment, Q\&A, news, reports), do models generalize better than single-dataset training?

\subsection{Related Empirical Studies}

Several empirical studies inform our methodology. \textcite{xie2023doremi} demonstrated that dynamic mixture optimization can outperform static mixtures on The Pile, but their approach requires validation data and multiple training runs, limiting practicality. \textcite{longpre2023pretrainer} surveyed practitioners' mixture strategies, finding that capping strategies and temperature sampling are most common in production settings. \textcite{mitra2023orca2} (Orca-2) showed that training on diverse instruction formats improves reasoning generalization, suggesting that \textit{intra-domain diversity} (multiple financial datasets) may be as important as domain specialization.

Notably absent from prior work are systematic studies of \textbf{dataset size effects} on mixture strategies: when is a dataset large enough for standalone pretraining? When does mixing help vs hurt? And how do these patterns interact with model size? These questions motivate our experimental design in Chapter 3.
