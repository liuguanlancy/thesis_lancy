\chapter{Background and Related Work}

This chapter reviews four areas used in the rest of the thesis: financial NLP, language‑model pretraining, data mixture design, and domain adaptation. The goal is focused context, not a full survey.

\section{Financial NLP}

\subsection{Tasks and Settings in Financial NLP}

Common tasks include sentiment analysis over news and social media, question answering on regulatory text, numerical reasoning in corporate reports, and information extraction from SEC filings \parencite{araci2019finbert, chen2021finqa}. Compared with general NLP, this area has special issues. Specialized vocabulary appears everywhere: "alpha", "beta", "EBITDA". Domain‑specific reasoning is needed—causal chains in market analysis, for example. Numerical grounding means reading financial statements correctly. Temporal dynamics around events and earnings releases matter \parencite{wu2023bloomberggpt, araci2019finbert}. Often these appear in the same document. Sometimes not.

\subsection{Existing Financial Language Models}

We note only models relevant to our setup. \textbf{BloombergGPT} \parencite{wu2023bloomberggpt} is a 50B model trained on a 51\%/49\% financial/general mix. It reports strong performance on finance benchmarks while keeping general ability. \textbf{FinBERT} variants \parencite{araci2019finbert, yang2020finbert} continue pretraining BERT on financial corpora and improve news sentiment. More recently, \textbf{FinGPT} \parencite{yang2023fingpt} explored open‑source, instruction‑tuned approaches for finance. We cite these to situate our choices rather than make head‑to‑head claims.

\subsection{Domain-Specific Challenges}

Three issues show up repeatedly.

First, privacy. Institutions cannot upload sensitive data (portfolios, strategies, client information) to external APIs. So models often must run locally \parencite{wu2023bloomberggpt}.

Second, data scarcity. Curated financial corpora are much smaller than general web text. Data‑efficient training becomes important.

Third, rapid vocabulary change. Terms like "DeFi" and "ESG" appear and shift with markets. This requires timely adaptation.

\section{Language Model Pretraining}

\subsection{Pretraining Objectives and Architecture}

Most current work uses \textbf{causal language modeling} (CLM): predict the next token from previous context \parencite{radford2019language, brown2020language}. The objective is self‑supervised. It scales to large unlabeled corpora and provides a clean training signal. Architecturally, decoder‑only transformers (GPT, LLaMA, Qwen) dominate. Multi‑head self‑attention captures long‑range dependencies. Feed‑forward layers add non‑linearity \parencite{vaswani2017attention, touvron2023llama}. Standard setup.

\subsection{Scaling Laws and Model Size Effects}

\textcite{kaplan2020scaling} showed power‑law links among model size, dataset size, compute, and performance. Larger models can be more sample‑efficient. This pushed work toward billion‑parameter scales. But the story is more nuanced. \textcite{hoffmann2022training} argued many models are undertrained for their size—the Chinchilla view. And \textcite{tay2022ul2} emphasized that objectives and data quality also shape how models scale.

\textbf{Hyperparameter sensitivity} gets less attention. \textcite{mccandlish2018empirical} noted that optimal learning rates often decrease with model size. For 0.6B--4B models in specialized domains, systematic studies are limited. Many scaling‑law papers assume "proper tuning" but do not specify the procedures they use. This hides practical effort. Tuning is important at this scale. In our experiments, all main runs used LR=2e-5. In a few cases we lowered LR to stabilize training.

\subsection{Computational and Memory Considerations}

Training large language models requires large compute. A 1B‑parameter model in 32‑bit uses about 4GB for parameters. Optimizer states can double or triple that \parencite{rajbhandari2020zero,kingma2014adam}. For 0.6B--4B models, memory‑efficient approaches help: mixed precision (bfloat16), gradient accumulation, activation checkpointing, and parameter‑efficient methods such as LoRA. These make training feasible on enterprise GPUs—RTX A6000 (48GB), A100 (40GB), H100 (80GB) \parencite{narayanan2021efficient,hu2021lora}. In our setup we use bfloat16 and gradient accumulation when needed.

\section{Data Mixture Strategies}

\subsection{Curriculum Learning and Sequential Mixing}

\textbf{Curriculum learning} orders data from easier to harder, or from general to specialized \parencite{bengio2009curriculum}. \textcite{wu2022opt} used curriculum in OPT pretraining by increasing difficulty over time. In finance, a natural ordering is Wikipedia → news → SEC filings. Evidence at large scale is mixed \parencite{longpre2023pretrainer}. Some works report limited gains for masked LM. Others see gains in narrower settings. Many systems sample from mixtures rather than strict curricula \parencite{raffel2020exploring,wu2022opt}. So in practice, simultaneous mixing is more common.

\subsection{Simultaneous Mixture Approaches}

Another option is \textbf{simultaneous mixture}: sample from multiple datasets throughout training. \textcite{raffel2020exploring} (T5) used a multi‑task mixture with task prefixes. Diverse pretraining improved downstream generalization. \textcite{xie2023doremi} proposed DoReMi to adjust domain weights during training using validation perplexity. It outperformed static mixtures on The Pile. This approach is widely used.

\textbf{BloombergGPT} \parencite{wu2023bloomberggpt} mixed 51\% financial with 49\% general data (The Pile, C4) at the token level. The balance kept general skills and added domain strength. That study evaluated a single 50B model. How mixture and size interact (0.6B vs 4B) is less explored. We test this across three sizes. As shown in \Cref{fig:scaling_comparison_all,tab:mixed_financial_results,tab:mixed_wiki_financial_results}, mixed financial datasets (21.55 ppl @ 4B) outperform Wiki+Financial mixtures (26.69 ppl @ 4B). To be specific, about 24\% worse.

\subsection{Domain Proportions and Sampling Strategies}

Choosing domain proportions is not straightforward. No single rule works for all. We highlight three common strategies:

\textbf{1. Temperature sampling} \parencite{arivazhagan2019massively}: sample from dataset $d$ with probability $p_d \propto n_d^{1/T}$ where $n_d$ is dataset size and $T$ is temperature. $T < 1$ upsamples small datasets. $T > 1$ downsamples them. Simple and works reasonably well.

\textbf{2. Capping strategies} \parencite{longpre2023pretrainer}: cap the largest dataset(s) at a threshold (e.g., 50\% of total tokens) to prevent dominance, then sample others proportionally. Capping stops a single source from taking over while keeping variety.

\textbf{3. Equal mixing} \parencite{sanh2022multitask}: assign equal sampling probability to each dataset regardless of size. This maximizes task diversity but can undersample large datasets.

We use a \textbf{50\% capping strategy} (``50cap'') for financial mixtures (details in Chapter 3) to balance diversity and data efficiency.

\section{Domain Adaptation and Transfer Learning}

\subsection{Cross-Domain Transfer in Language Models}

\textbf{Transfer learning}—pretrain on broad data, then fine‑tune for a target domain—has been standard since BERT \parencite{devlin2019bert,pan2010transfer,zhuang2020comprehensive}. We assume general knowledge transfers. This assumption holds in many cases. But not always. \textcite{gururangan2020don} showed \textbf{domain‑adaptive pretraining} (continued pretraining on domain corpora) improves performance in biomedicine, computer science, news, and reviews. General pretraining alone is not enough for specialized use.

In finance, \textcite{araci2019finbert} got better results via continued pretraining on financial news. \textcite{yang2020finbert} added task‑adaptive pretraining. \textcite{huang2023finbert} found domain‑specific pretraining beats general models on financial IE. These are mostly BERT‑style and classification‑focused. Domain adaptation for \textit{causal, generative} LMs in finance is less studied. Parameter‑efficient methods (e.g., surgical fine‑tuning \parencite{lee2022surgical}) suggest that selective adaptation can help transfer and reduce forgetting.

\subsection{Catastrophic Forgetting and Stability}

\textbf{Catastrophic forgetting} is a classic issue: training further on a domain can erase general knowledge \parencite{mccloskey1989catastrophic, french1999catastrophic}. \textcite{kirkpatrick2017overcoming} proposed Elastic Weight Consolidation (EWC) to protect important parameters. With mixtures, \textit{simultaneous mixing} of general and domain data acts as implicit regularization. The model stays exposed to diverse distributions \parencite{arivazhagan2019massively,raffel2020exploring}.

\subsection{Distribution Shift and Domain Mismatch}

\textbf{Distribution shift}—differences between training and evaluation—hurts generalization \parencite{quinonero2009dataset}. In finance, this shows up in three ways. Vocabulary shift: financial terms vs general language. Discourse differences: analyst reports vs encyclopedic text. Formatting differences: tables in 10‑K vs narrative news. \textcite{aharoni2020unsupervised} showed domain mismatch strongly degrades out‑of‑distribution performance. This motivates mixtures that cover sub‑domains.

We test this directly. Does pretraining on high‑quality general text (WikiText) transfer to financial evaluation sets? Does domain mismatch require in‑domain pretraining? And does mixing in‑domain datasets (sentiment, Q\&A, news, reports) give better generalization than training on a single dataset?

\subsection{Related Empirical Studies}

Several studies guide our setup. \textcite{xie2023doremi} showed dynamic mixture optimization can beat static mixes on The Pile. But it needs validation data and multiple runs—useful, but not always practical. \textcite{longpre2023pretrainer} surveyed practitioners and found capping and temperature sampling common in production. \textcite{mitra2023orca2} (Orca‑2) showed that diverse instruction formats help reasoning generalization. This suggests that \textit{intra‑domain diversity} (multiple financial datasets) can matter as much as domain specialization.

What is missing is a systematic look at \textbf{dataset size effects} for mixtures. When is a dataset large enough for standalone pretraining? When does mixing help, and when does it hurt? How do these patterns change with model size? These questions shape our experimental design in Chapter 3.
