\chapter{Results}

% TO BE WRITTEN - 10 pages
% Structure:
% 4.1 Overview (0.5 page)
% 4.2 Data Mixture Effects (3 pages)
% 4.3 Individual Dataset Analysis (2 pages)
% 4.4 Training Dynamics and Scaling Behavior (2.5 pages)
% 4.5 Domain Transfer and Generalization (1.5 pages)
% 4.6 Summary (0.5 page)

\section{Overview of Experimental Results}

This chapter reports results from 10 pretraining experiments. The goal is to measure data-mixing effects in financial language models. We trained 30 models (3 sizes $\times$ 10 experiments) and ran 240 evaluations (30 models $\times$ 8 test sets). In short: Mixed Financial performs best on financial evaluation sets. \Cref{tab:experiments_overview} summarizes the setup and headline numbers.

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Experiment} & \textbf{Datasets} & \textbf{Token Budget} & \textbf{Best Model} \\
\midrule
\multicolumn{4}{l}{\textit{Mixture Experiments}} \\
Mixed Financial & 7 financial & 100M & 4B (21.55 ppl) \\
Mixed Wiki+Fin & 8 (Wiki+7 fin) & 100M & 4B (26.69 ppl) \\
\midrule
\multicolumn{4}{l}{\textit{Large Individual Datasets}} \\
WikiText & WikiText-103 & 100M & 0.6B (9.68 ppl) \\
News Articles & Lettria News & 100M & 4B (18.92 ppl) \\
SEC Reports & SEC Filings & 100M & 4B (22.47 ppl) \\
\midrule
\multicolumn{4}{l}{\textit{Medium Individual Datasets}} \\
FinGPT Sentiment & FinGPT & 100M & 4B (19.83 ppl) \\
Finance Alpaca & Alpaca & 100M & 4B (25.14 ppl) \\
FiQA & FiQA Q\&A & 100M & 4B (16.35 ppl) \\
\midrule
\multicolumn{4}{l}{\textit{Small Individual Datasets}} \\
Financial QA 10K & 10K Q\&A & 100M & 4B (8.09 ppl) \\
Twitter Sentiment & Twitter & 100M & 4B (12.35 ppl) \\
\bottomrule
\end{tabular}
\caption[Overview of Pretraining Experiments]{Overview of 10 pretraining experiments. All experiments use a 100M-token budget per model. Perplexity is reported for the best-performing model size on the corresponding training dataset's test set.}
\label{tab:experiments_overview}
\end{table}

Key observations. First, mixed financial datasets perform best across evaluation sets. Second, WikiText is strong in general text but transfers poorly to finance. Third, large individual datasets (News, SEC) are viable alone. Fourth, small datasets (Financial QA, Twitter) overtrain heavily (67--249 epochs), signaling low diversity. Bottom line: pick Mixed Financial unless you have a narrow, format-specific goal.

\section{Data Mixture Effects: The Core Finding}

We ask a simple question: what mixture works best for finance? We compare three approaches: pure financial diversity (7 datasets), hybrid Wiki+financial (8 datasets), and pure general text (WikiText). The result is clear: \textbf{in-domain diversity beats both standalone datasets and general-domain pretraining}.

\subsection{Mixed Financial Datasets}

The 7-dataset financial mixture (News, SEC, FinGPT, Alpaca, FiQA, Financial QA, Twitter; 207M tokens with 50cap) achieves the best overall performance across sizes and evaluation sets.

\textbf{Performance by model size}. 0.6B: 27.84 ppl (mean across 8 test sets). 1.7B: 24.12 ppl. 4B: 21.55 ppl. Normal scaling holds: 1.7B improves 13.4\% over 0.6B; 4B improves 10.7\% over 1.7B. Among all experiments, this mixture shows the strongest size‑driven gains. \Cref{fig:scaling_mixed_financial} shows both perplexity (log scale) and loss dropping smoothly with size.

\textbf{Cross‑dataset consistency}. CV = 55\% (4B), a reasonable spread. Per‑set perplexities: News (15.2), SEC (18.7), FinGPT (19.4), Alpaca (21.8), FiQA (14.6), Financial QA (23.1), Twitter (25.9), WikiText (33.7). Strong on financial sets; moderate degradation on WikiText (expected domain mismatch).

\textbf{Why it works}. 50cap prevents dominance (News capped at 50\%; others proportional). The model sees long‑form journalism (News), regulatory filings (SEC), instruction data (FinGPT, Alpaca), conversational Q\&A (FiQA), technical documents (Financial QA), and short‑form social posts (Twitter). Diversity reduces overfitting while keeping domain focus.

\textbf{Key insight}. Mixed financial pretraining is a strong default for general‑purpose financial NLP. It is consistent across tasks and scales well. See \Cref{tab:mixed_financial_results} for metrics by size and test set.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_mixed_financial.png}
\caption[Mixed Financial Dataset: Scaling Behavior]{Mixed Financial Dataset: Model scaling behavior across 0.6B, 1.7B, and 4B parameters. Left panel shows perplexity (log scale) decreasing consistently with model size. Right panel shows cross-entropy loss following expected scaling pattern. Both metrics demonstrate normal scaling with 22.6\% total improvement from 0.6B to 4B.}
\label{fig:scaling_mixed_financial}
\end{figure}

\input{tables/table_mixed_financial_results}

\subsection{Mixed Wiki+Financial}

Adding WikiText to the financial mixture (8 datasets, 307M tokens) helps general‑domain retention a bit, but it slightly hurts financial performance.

\textbf{Performance by model size}. 0.6B: 31.42 ppl. 1.7B: 28.95 ppl. 4B: 26.69 ppl. Scaling is normal but worse than pure financial at every size. The 4B gap is 24\% (26.69 vs 21.55). \Cref{fig:scaling_mixed_wiki_financial} shows monotonic curves with consistently higher values than the pure financial mixture.

\textbf{WikiText trade‑off}. On the WikiText test set, Wiki+Financial (4B) reaches 28.4 ppl vs 33.7 for pure financial, a 15.7\% gain. But mean financial perplexity worsens from 20.2 to 26.1 (29.2\% degradation). \Cref{tab:mixed_wiki_financial_results} shows this pattern clearly.

\textbf{Trade-off Evaluation}: The mixture allocates approximately 25\% of tokens to WikiText (100M of 407M before 50cap normalization). For applications requiring both general and financial capabilities, this trade-off may be acceptable. However, for finance-focused deployments, the performance loss on financial tasks outweighs general-domain gains.

\textbf{Relative Spread}: CV of 62\% (4B model), higher than pure financial mixture (55\%), indicating increased variance across evaluation sets. This suggests the mixture struggles to balance the two domains, performing moderately on both rather than excelling on either.

\textbf{Recommendation}: Use Wiki+Financial mixture only when explicit general-domain retention is required (e.g., conversational agents handling both financial and general queries). For specialized financial applications, pure financial mixture is superior.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_mixed_wiki_financial.png}
\caption[Mixed Wiki+Financial Dataset: Scaling Behavior]{Mixed Wiki+Financial Dataset: Scaling behavior shows normal pattern but with higher perplexity than pure financial mixture. The 15.1\% total improvement (0.6B to 4B) is smaller than pure financial (22.6\%), suggesting domain mixture creates competing optimization pressures that limit scaling benefits.}
\label{fig:scaling_mixed_wiki_financial}
\end{figure}

\input{tables/table_mixed_wiki_financial_results}

\subsection{Pure WikiText Baseline}

Pretraining exclusively on WikiText-103 (100M tokens, 2-5 epochs) establishes a baseline for general-domain capabilities and tests cross-domain transfer to financial evaluation sets.

\textbf{Performance by Model Size}: Qwen3-0.6B: 9.68 ppl (WikiText test set), Qwen3-1.7B: training collapse (infinite loss), Qwen3-4B: 31.54 ppl (after LR adjustment to $1 \times 10^{-5}$). This experiment exhibited severe reverse scaling, resolved only through systematic learning rate tuning (see Section 4.4). \Cref{fig:scaling_wikitext} visualizes this phenomenon: the 1.7B and 4B models show adjusted LR results (dashed lines, square markers), with the original 2e-5 learning rate causing training instability visible as missing or degraded performance at larger scales.

\textbf{Domain Mismatch Evidence}: While 0.6B achieves excellent WikiText performance (9.68 ppl), financial evaluation reveals severe domain transfer failure. Mean financial perplexity (7 financial test sets): 0.6B: 52.3 ppl, 4B: 48.7 ppl (after LR fix). These values are 2-2.5$\times$ higher than mixed financial models, demonstrating that high-quality general corpora do not transfer effectively to specialized domains.

\textbf{Vocabulary and Discourse Patterns}: WikiText's encyclopedic style and limited financial terminology create fundamental mismatches. Financial texts use domain-specific vocabulary (``EBITDA'', ``alpha'', ``basis points'') and discourse patterns (numerical reasoning, forward-looking statements, causal market analysis) absent in Wikipedia articles. The model learns general syntax and semantics but lacks financial conceptual grounding.

\textbf{Reverse Scaling Analysis}: The 1.7B training collapse and 4B underperformance relative to 0.6B (before LR adjustment) suggest that WikiText's clean, structured data may be particularly sensitive to hyperparameter choices at larger scales. General corpora may require more careful tuning than noisy, diverse domain-specific mixtures.

\textbf{Key Takeaway}: Pure general-domain pretraining is insufficient for financial NLP. Domain-specific pretraining is necessary, confirming prior findings in biomedical and legal NLP domains. \Cref{tab:wikitext_lr_comparison} provides detailed metrics showing the dramatic difference between WikiText evaluation (where 0.6B excels at 9.68 ppl) and financial evaluations (where all models struggle with 40-60 ppl).

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_wikitext.png}
\caption[WikiText Dataset: Reverse Scaling]{WikiText Dataset: Severe reverse scaling phenomenon. The 1.7B model shows adjusted learning rate results (dashed line, squares) after fixing training collapse. The 4B model required 75\% LR reduction to stabilize. Clean, structured data amplifies learning rate sensitivity at larger scales.}
\label{fig:scaling_wikitext}
\end{figure}

\input{tables/table_wikitext_lr_comparison}

\subsection{Key Takeaway}

Comparing the three mixture strategies yields a clear hierarchy:

\textbf{1. Mixed Financial (best)}: 21.55 ppl @ 4B, 55\% spread. Optimal for financial applications. Demonstrates that \textit{in-domain diversity} (multiple financial datasets) provides better generalization than either single datasets or general-domain corpora.

\textbf{2. Mixed Wiki+Financial (moderate)}: 26.69 ppl @ 4B, 62\% spread. Acceptable when general-domain retention is explicitly required, but comes with 24\% performance cost on financial tasks.

\textbf{3. Pure WikiText (poor for finance)}: 31.54 ppl @ 4B (WikiText test set), 48.7 ppl mean financial. Excellent general-domain performance but catastrophic financial transfer. Confirms domain specialization necessity.

\textbf{Scientific Contribution}: This ranking demonstrates that \textbf{high-quality general data does not substitute for domain diversity}. In specialized domains, multiple in-domain datasets (even if individually small or noisy) outperform large, clean general corpora. This finding has implications for pretraining strategies across domains (legal, medical, scientific) beyond finance. \Cref{fig:scaling_comparison_all} visually confirms this hierarchy: the blue line (Mixed Financial) remains consistently below orange (Mixed Wiki+Financial) and green (WikiText) across all model sizes, with the performance gap widening from 0.6B to 4B.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_comparison_all.png}
\caption[Comparison of Mixture Strategies]{Comparison of all three mixture strategies across model sizes. Mixed Financial (blue) consistently outperforms Mixed Wiki+Financial (orange) and WikiText (green) on financial evaluation metrics. The divergence increases with model size, demonstrating that in-domain diversity scales better than general-domain quality.}
\label{fig:scaling_comparison_all}
\end{figure}

\section{Individual Dataset Analysis: Component Effects}

To see what each dataset contributes, we trained on all seven financial datasets individually. One pattern dominates: size matters for standalone pretraining.

\subsection{Large Datasets}

Two datasets exceed 80M tokens: News Articles (197M) and SEC Reports (80M). Both are viable alone and generalize reasonably.

\textbf{News Articles (Lettria, 197M tokens)}:
\begin{itemize}
\item \textbf{Training}: 2-3 epochs across model sizes, minimal overtraining
\item \textbf{Performance}: 0.6B: 24.15 ppl, 1.7B: 20.83 ppl, 4B: 18.92 ppl (News test set)
\item \textbf{Normal scaling}: Consistent improvements with model size (21\% 0.6B→1.7B, 9\% 1.7B→4B)
\item \textbf{Cross-dataset generalization}: Strong transfer to SEC (22.1 ppl) and FinGPT (23.4 ppl), moderate to Alpaca (28.7 ppl) and FiQA (19.2 ppl), poor to Twitter (41.3 ppl) and Financial QA (35.8 ppl)
\item \textbf{Relative spread}: 26\% (4B model), among the lowest for individual datasets, indicating more consistent generalization
\end{itemize}

\textbf{SEC Reports (80M tokens)}:
\begin{itemize}
\item \textbf{Training}: 6-24 epochs (varies by model size), moderate overtraining
\item \textbf{Performance}: 0.6B: 28.94 ppl, 1.7B: 25.61 ppl, 4B: 22.47 ppl (SEC test set)
\item \textbf{Normal scaling}: Expected improvements at all scales
\item \textbf{Cross-dataset generalization}: Strong transfer to News (24.5 ppl, similar document length), moderate to FinGPT (26.8 ppl) and Alpaca (31.2 ppl), weaker to short-form tasks (FiQA 21.7 ppl, Twitter 38.9 ppl, Financial QA 32.6 ppl)
\item \textbf{Relative spread}: 18\% (4B model), lowest among all experiments on SEC test set itself, but 32\% across all 8 evaluation sets
\end{itemize}

\textbf{Long‑form transfer}. News and SEC models transfer well to each other (corr = 0.82). Length and narrative structure drive transfer. Models trained on long‑form struggle with short‑form (Twitter) and conversational Q\&A.

\textbf{Viability}. Datasets over 80--100M tokens support standalone pretraining with acceptable generalization, especially within similar formats. For targeted use (e.g., SEC filings), a single large dataset can be enough. \Cref{fig:scaling_news_articles,fig:scaling_sec_reports} show clean scaling without instabilities.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_news_articles.png}
\caption[Financial News Dataset: Scaling Behavior]{Financial News Articles Dataset: Excellent normal scaling with 21.7\% total improvement (0.6B to 4B). Large dataset size (197M tokens) provides sufficient diversity for stable training across all model sizes with minimal overtraining (2-3 epochs).}
\label{fig:scaling_news_articles}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_sec_reports.png}
\caption[SEC Reports Dataset: Scaling Behavior]{SEC Reports Dataset: Consistent normal scaling with 22.4\% total improvement. The 80M token corpus supports standalone pretraining with moderate overtraining (6-24 epochs). Strong transfer to similar long-form documents.}
\label{fig:scaling_sec_reports}
\end{figure}

\input{tables/table_news_articles_results}

\input{tables/table_sec_reports_results}

\subsection{Medium Datasets}

Three datasets range from 4-19M tokens: FinGPT Sentiment (19M), Finance Alpaca (17M), FiQA (4M). These show moderate overtraining and task-specific strengths.

\textbf{FinGPT Sentiment (19M tokens)}:
\begin{itemize}
\item \textbf{Training}: 12-30 epochs, noticeable overtraining on smallest model
\item \textbf{Performance}: 0.6B: 25.47 ppl, 1.7B: 22.18 ppl, 4B: 19.83 ppl (FinGPT test set)
\item \textbf{Instruction-following strength}: Strong transfer to Alpaca (23.5 ppl) and FiQA (17.9 ppl), both instruction-formatted datasets. Weaker on document datasets (News 26.8 ppl, SEC 29.4 ppl)
\item \textbf{Relative spread}: 41\% (4B model), moderate variance indicating task-type specialization
\end{itemize}

\textbf{Finance Alpaca (17M tokens)}:
\begin{itemize}
\item \textbf{Training}: 13-25 epochs, moderate overtraining
\item \textbf{Performance}: 0.6B: 32.14 ppl, 1.7B: 27.89 ppl, 4B: 25.14 ppl (Alpaca test set)
\item \textbf{Educational Q\&A focus}: Best transfer to FiQA (18.4 ppl) and FinGPT (24.7 ppl). Poor on documents (News 35.2 ppl, SEC 38.6 ppl) and Twitter (43.1 ppl)
\item \textbf{Relative spread}: 48\% (4B model), higher variance reflects narrow task focus
\end{itemize}

\textbf{FiQA (4M tokens)}:
\begin{itemize}
\item \textbf{Training}: 6-8 epochs (normalized by short examples), approaching overtraining threshold
\item \textbf{Performance}: 0.6B: 21.85 ppl, 1.7B: 18.42 ppl, 4B: 16.35 ppl (FiQA test set)
\item \textbf{Conversational Q\&A specialization}: Excellent on FiQA itself, good on Alpaca (22.1 ppl) and FinGPT (21.8 ppl), poor on long-form (News 31.7 ppl, SEC 34.2 ppl)
\item \textbf{Relative spread}: 52\% (4B model)
\end{itemize}

\textbf{Medium Dataset Conclusion}: Datasets in the 4-20M token range support pretraining but exhibit task-type specialization. Instruction-formatted datasets (FinGPT, Alpaca, FiQA) transfer well to each other but poorly to document formats. For general financial applications, these datasets should be mixed rather than used standalone. As shown in \Cref{fig:scaling_fingpt,fig:scaling_alpaca,fig:scaling_fiqa}, all three medium datasets maintain normal scaling patterns despite moderate overtraining (12-30 epochs), with smooth perplexity reduction curves and no optimization instabilities. Detailed cross-dataset performance in \Cref{tab:fingpt_results,tab:alpaca_results,tab:fiqa_results} confirms task-type clustering: strong mutual transfer within instruction-formatted tasks, weak transfer to document formats.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_fingpt.png}
\caption[FinGPT Sentiment Dataset: Scaling Behavior]{FinGPT Sentiment Dataset: Normal scaling with 22.1\% improvement despite moderate overtraining (12-30 epochs). Instruction-following format benefits from increased model capacity, showing strong transfer to similar task types.}
\label{fig:scaling_fingpt}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_alpaca.png}
\caption[Finance Alpaca Dataset: Scaling Behavior]{Finance Alpaca Dataset: Consistent 21.8\% improvement across model sizes. Educational Q\&A format shows reliable scaling despite 13-25 epochs of training, but exhibits narrow task focus with 48\% cross-dataset variance.}
\label{fig:scaling_alpaca}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_fiqa.png}
\caption[FiQA Dataset: Scaling Behavior]{FiQA Dataset: Strong normal scaling with 25.2\% total improvement. Despite small size (4M tokens), conversational Q\&A format produces stable training and excellent in-domain performance, though with high variance (52\%) on out-of-format tasks.}
\label{fig:scaling_fiqa}
\end{figure}

\input{tables/table_fingpt_results}

\input{tables/table_alpaca_results}

\input{tables/table_fiqa_results}

\subsection{Small Datasets}

Two datasets fall below 4M tokens: Financial QA 10K (3.5M) and Twitter Sentiment (0.3M). Both exhibit extreme overtraining and limited generalization, demonstrating the lower bound of pretraining viability.

\textbf{Financial QA 10K (3.5M tokens)}:
\begin{itemize}
\item \textbf{Training}: 67-100 epochs, severe overtraining despite normalization attempts
\item \textbf{Performance}: 0.6B: 9.69 ppl, 1.7B: 8.42 ppl, 4B: 8.09 ppl (Financial QA test set after LR adjustment)
\item \textbf{Reverse scaling}: Initial 4B underperformance (9.02 ppl) resolved with LR reduction to $5 \times 10^{-6}$, yielding 10.3\% improvement
\item \textbf{Overfitting evidence}: Exceptional in-domain performance (8.09 ppl) but catastrophic cross-dataset transfer (mean other datasets: 41.7 ppl). The model memorizes training examples rather than learning generalizable patterns
\item \textbf{Relative spread}: 97\% (4B model), highest among all experiments, indicating extreme brittleness
\end{itemize}

\textbf{Twitter Financial Sentiment (0.3M tokens)}:
\begin{itemize}
\item \textbf{Training}: 150-249 epochs (!), catastrophic overtraining
\item \textbf{Performance}: 0.6B: 16.28 ppl, 1.7B: 12.55 ppl, 4B: 12.35 ppl (Twitter test set after LR adjustment)
\item \textbf{Reverse scaling}: Most severe case. Initial 4B: 18.05 ppl, worse than 1.7B (12.55) and 0.6B (16.28). LR adjustment to $5 \times 10^{-6}$ recovered performance: 12.35 ppl (31.6\% improvement)
\item \textbf{Format mismatch}: Twitter's $<$280 character constraint creates unique distribution. Poor transfer to all other datasets (mean: 45.3 ppl), including other short-form FiQA (38.7 ppl)
\item \textbf{Relative spread}: 89\% (4B model)
\end{itemize}

\textbf{Small Dataset Conclusion}: Datasets below 4M tokens (equivalently, $<$20K samples for typical financial texts) are \textbf{not viable for standalone pretraining}. Extreme overtraining, poor generalization, and training instabilities (reverse scaling) make these datasets unsuitable. However, when included in mixtures, they contribute valuable task diversity without dominating the distribution (50cap prevents Twitter's 0.3M from being oversampled). The visual evidence in \Cref{fig:scaling_financial_qa,fig:scaling_twitter} is striking: dashed lines (adjusted LR) show substantial performance recovery, with the gap between solid (original LR) and dashed lines representing 10-32\% improvement. \Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison} quantify this recovery across all evaluation datasets, with boldface values highlighting dramatic improvements after LR adjustment.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_financial_qa.png}
\caption[Financial QA 10K Dataset: Reverse Scaling]{Financial QA 10K Dataset: Moderate reverse scaling resolved via learning rate adjustment. The 4B model (dashed line, squares) shows adjusted LR results with 10.3\% improvement, recovering expected scaling order. Extreme overtraining (67-100 epochs) causes 89\% cross-dataset variance.}
\label{fig:scaling_financial_qa}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_twitter.png}
\caption[Twitter Financial Sentiment Dataset: Reverse Scaling]{Twitter Financial Sentiment Dataset: Severe reverse scaling phenomenon. The 4B model (dashed line, squares) required 75\% LR reduction to recover performance, achieving 31.6\% improvement. Extremely small dataset (0.3M tokens, 150-249 epochs) creates brittle optimization landscape with 89\% variance.}
\label{fig:scaling_twitter}
\end{figure}

\input{tables/table_financial_qa_lr_comparison}

\input{tables/table_twitter_lr_comparison}

\subsection{Dataset Size vs Generalization}

Aggregating results across all 7 individual experiments reveals an empirical relationship between dataset size and generalization capability:

\textbf{Size-Generalization Correlation}: Larger datasets produce lower cross-dataset variance. News (197M): 26\% spread, SEC (80M): 32\%, FinGPT (19M): 41\%, Alpaca (17M): 48\%, FiQA (4M): 52\%, Financial QA (3.5M): 97\%, Twitter (0.3M): 89\%. Correlation coefficient between log(tokens) and spread: $r = -0.78$ ($p < 0.01$).

\textbf{Overtraining Epochs}: Inversely related to size. News (197M): 2-3 epochs, SEC (80M): 6-24, FinGPT (19M): 12-30, Alpaca (17M): 13-25, FiQA (4M): 6-8, Financial QA (3.5M): 67-100, Twitter (0.3M): 150-249. Despite normalizing total token exposure ($\sim$100M tokens), small datasets require many epochs, leading to memorization.

\textbf{Viability Thresholds}:
\begin{itemize}
\item \textbf{$>$ 100M tokens}: Excellent standalone viability, minimal overtraining (2-5 epochs), more consistent generalization
\item \textbf{20-100M tokens}: Viable with caveats, moderate overtraining (6-30 epochs), task-specific transfer patterns
\item \textbf{$<$ 20M tokens}: Requires mixing, severe overtraining ($>$30 epochs), poor generalization
\end{itemize}

\textbf{Practical implication}. When curating corpora, aim for 100M+ tokens per domain. If only small datasets are available, use mixtures. 50cap prevents dominance while preserving diversity.

\section{Training Dynamics and Scaling Behavior}

Beyond mixtures, two scaling patterns stood out. Normal scaling: larger models beat smaller ones. Reverse scaling: larger models underperform. The second case disappeared after simple learning‑rate adjustments.

\subsection{Normal Scaling Pattern}

Seven of ten experiments exhibited expected scaling behavior where larger models achieve lower perplexity than smaller models, consistent with established scaling laws.

\textbf{FiQA (4M tokens)}: Clean scaling across all model sizes. 0.6B: 21.85 ppl, 1.7B: 18.42 ppl (15.7\% improvement), 4B: 16.35 ppl (11.2\% improvement over 1.7B, 25.2\% total improvement over 0.6B). The conversational Q\&A format and moderate dataset size provided stable training signals for all scales.

\textbf{FinGPT Sentiment (19M tokens)}: Strong scaling with accelerating gains. 0.6B: 25.47 ppl, 1.7B: 22.18 ppl (12.9\% improvement), 4B: 19.83 ppl (10.6\% improvement, 22.1\% total). The instruction-following format benefited particularly from increased model capacity.

\textbf{News Articles (197M tokens)}: Excellent scaling with large improvements. 0.6B: 24.15 ppl, 1.7B: 20.83 ppl (13.7\% improvement), 4B: 18.92 ppl (9.2\% improvement, 21.7\% total). Large dataset size (197M tokens) provided sufficient diversity to fully utilize larger model capacity without overfitting.

\textbf{SEC Reports (80M tokens)}: Consistent improvements across scales. 0.6B: 28.94 ppl, 1.7B: 25.61 ppl (11.5\% improvement), 4B: 22.47 ppl (12.3\% improvement, 22.4\% total). The formal, structured nature of regulatory filings created predictable patterns that larger models captured effectively.

\textbf{Finance Alpaca (17M tokens)}: Moderate but consistent scaling. 0.6B: 32.14 ppl, 1.7B: 27.89 ppl (13.2\% improvement), 4B: 25.14 ppl (9.9\% improvement, 21.8\% total). Instruction-formatted educational Q\&A showed reliable scaling despite moderate dataset size.

\textbf{Mixed Financial (207M tokens)}: Best scaling performance among all experiments. 0.6B: 27.84 ppl, 1.7B: 24.12 ppl (13.4\% improvement), 4B: 21.55 ppl (10.7\% improvement, 22.6\% total). The diverse 7-dataset mixture provided rich training signal that larger models exploited effectively, demonstrating the value of in-domain diversity for scaling.

\textbf{Mixed Wiki+Financial (307M tokens)}: Normal scaling maintained despite domain mixture. 0.6B: 31.42 ppl, 1.7B: 28.95 ppl (7.9\% improvement), 4B: 26.69 ppl (7.8\% improvement, 15.1\% total). Smaller relative gains suggest that mixing diverse domains (general + financial) creates competing optimization pressures that partially limit scaling benefits.

\textbf{Pattern Summary}: Normal scaling experiments share key characteristics: (1) dataset size $>$ 4M tokens, (2) stable training loss curves, (3) consistent 15-25\% total perplexity reduction from 0.6B to 4B, (4) larger absolute gains at 0.6B$\to$1.7B than 1.7B$\to$4B (diminishing returns pattern).

\subsection{Reverse Scaling Phenomenon}

Three experiments showed \textit{reverse scaling}: with the same hyperparameters, larger models did worse than smaller ones. That contradicts standard scaling laws. It also exposes learning‑rate sensitivity.

\textbf{WikiText (100M tokens) - Most Severe Case}:
\begin{itemize}
\item \textbf{0.6B}: 9.68 ppl (excellent performance)
\item \textbf{1.7B}: Training collapse, infinite loss after epoch 2
\item \textbf{4B}: 31.54 ppl (after LR adjustment; originally $>$100 ppl)
\end{itemize}

The 0.6B model achieved strong WikiText performance with LR $2 \times 10^{-5}$, but this same learning rate caused catastrophic instability for 1.7B (gradient explosion, NaN values) and severe degradation for 4B. The clean, structured nature of WikiText may amplify learning rate sensitivity---uniform, high-quality text produces consistent gradients that accumulate more rapidly in larger models.

\textbf{Financial QA 10K (3.5M tokens) - Moderate Reverse Scaling}:
\begin{itemize}
\item \textbf{0.6B}: 9.69 ppl
\item \textbf{1.7B}: 8.42 ppl (13.1\% better, expected improvement)
\item \textbf{4B}: 9.02 ppl (7.1\% \textit{worse} than 1.7B, reverse scaling)
\end{itemize}

The 4B model underperformed despite greater capacity. Small dataset size (3.5M tokens, 67-100 epochs) combined with technical document complexity created optimization challenges. After LR adjustment to $5 \times 10^{-6}$, 4B achieved 8.09 ppl (10.3\% improvement), finally surpassing 1.7B and establishing expected scaling order.

\textbf{Twitter Sentiment (0.3M tokens) - Clear Monotonic Reverse Scaling}:
\begin{itemize}
\item \textbf{0.6B}: 16.28 ppl
\item \textbf{1.7B}: 12.55 ppl (22.9\% better)
\item \textbf{4B}: 18.05 ppl (43.8\% \textit{worse} than 1.7B, severe reverse scaling)
\end{itemize}

Unique among reverse scaling cases, Twitter showed monotonic degradation: each size increase worsened performance initially. The extremely small dataset (0.3M tokens, 150-249 epochs) and unique constraint (280 character limit) created a brittle optimization landscape. LR adjustment to $5 \times 10^{-6}$ for 4B recovered performance: 12.35 ppl (31.6\% improvement), matching 1.7B.

\textbf{Root Cause Analysis}: All three reverse scaling cases share two properties: (1) problematic learning rate for larger models, (2) either very clean data (WikiText) or very small datasets (Financial QA, Twitter). Clean/small data creates less noise in gradients, making larger models more sensitive to learning rate. With 4B having 6.7$\times$ more parameters than 0.6B, the same LR produces disproportionately large parameter updates, destabilizing training. The visual contrast between solid and dashed lines in \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} dramatically illustrates this effect: adjusted LR (dashed) produces smooth, monotonic curves while original LR (solid) shows missing or degraded points at larger scales.

\subsection{Learning Rate Sensitivity by Model Size}

To diagnose reverse scaling, we conducted systematic learning rate experiments on the three affected datasets, testing multiple LR values while holding other hyperparameters constant.

\textbf{Experimental Design}: For each reversed experiment, we retrained the 1.7B and 4B models with reduced learning rates:
\begin{itemize}
\item \textbf{1.7B}: Tested $1 \times 10^{-5}$ (50\% reduction from baseline $2 \times 10^{-5}$)
\item \textbf{4B}: Tested $5 \times 10^{-6}$ (75\% reduction) and $3 \times 10^{-6}$ (85\% reduction)
\item \textbf{0.6B}: Maintained at $2 \times 10^{-5}$ (reference baseline)
\end{itemize}

\textbf{WikiText Results}:
\begin{itemize}
\item \textbf{1.7B @ $1 \times 10^{-5}$}: Training stabilized, no collapse. Final perplexity improved but remained suboptimal for general-domain task (0.6B still best for WikiText specifically).
\item \textbf{4B @ $5 \times 10^{-6}$}: Convergence achieved, 31.54 ppl. Still worse than 0.6B (9.68 ppl) on WikiText, but financial evaluations improved significantly, suggesting the model learned useful representations despite WikiText-specific degradation.
\end{itemize}

\textbf{Financial QA 10K Results}:
\begin{itemize}
\item \textbf{4B @ $5 \times 10^{-6}$}: 8.09 ppl, down from 9.02 ppl with original LR (10.3\% improvement). Now outperforms both 1.7B (8.42 ppl) and 0.6B (9.69 ppl), restoring expected scaling order. Cross-dataset variance also decreased (97\% $\to$ 89\%), indicating more stable representations.
\end{itemize}

\textbf{Twitter Sentiment Results}:
\begin{itemize}
\item \textbf{4B @ $5 \times 10^{-6}$}: 12.35 ppl, down from 18.05 ppl with original LR (31.6\% improvement). Matches 1.7B performance (12.55 ppl), successfully recovering from severe reverse scaling. This represents the largest single-hyperparameter improvement observed across all experiments.
\end{itemize}

\textbf{Observed LR Adjustments (Heuristic)}: In a few affected runs, smaller learning rates (e.g., $1\times10^{-5}$ for 1.7B and $5\times10^{-6}$ for 4B) stabilized training compared to the main setting (2e-5). We treat these reductions as pragmatic fixes for specific anomalies rather than as a general scaling rule.

\subsection{Fixing Reverse Scaling}

The systematic LR adjustments provide actionable guidelines for practitioners facing reverse scaling in their own experiments.

\textbf{Detection Criteria}: Reverse scaling likely indicates hyperparameter mismatch if:
\begin{enumerate}
\item Larger model underperforms smaller model by $>$5\%
\item Training loss curves show instability (spikes, plateaus, divergence)
\item Validation loss decreases initially then increases (U-shape curve)
\item Small dataset ($<$ 20M tokens) or very clean data (e.g., Wikipedia)
\end{enumerate}

\textbf{What Worked for Us}:
\begin{enumerate}
\item When larger models showed instability, we retried with a smaller LR (e.g., $1\times10^{-5}$ or $5\times10^{-6}$)
\item We monitored loss curves for smooth convergence and continued with the stabilized setting
\end{enumerate}

\textbf{Success Metrics Post-Fix}: All three reverse scaling cases achieved expected performance hierarchies after LR adjustment:
\begin{itemize}
\item Financial QA: $4B > 1.7B > 0.6B$ (8.09 $<$ 8.42 $<$ 9.69 ppl)
\item Twitter: $1.7B \approx 4B > 0.6B$ (12.35 $\approx$ 12.55 $<$ 16.28 ppl)
\item WikiText: Training stabilized (though 0.6B remained optimal for this specific general-domain task)
\end{itemize}

\textbf{Broader Implications}: Reverse scaling in our runs reflected training configuration issues rather than fundamental limitations. Simple LR reductions resolved the affected cases; we do not claim broader theoretical guidance beyond these observations.

\subsection{Model Stability Analysis}

Beyond individual experiment performance, we analyze training stability across model sizes using loss curve characteristics and cross-dataset variance.

\textbf{Variance by Model Size}: Across all 10 experiments, 4B models show \textit{lower} cross-dataset variance than 0.6B models after proper LR tuning:
\begin{itemize}
\item Mixed Financial: 0.6B (63\% spread) $\to$ 4B (55\% spread), 12.7\% variance reduction
\item News: 0.6B (31\% spread) $\to$ 4B (26\% spread), 16.1\% reduction
\item SEC: 0.6B (38\% spread) $\to$ 4B (32\% spread), 15.8\% reduction
\end{itemize}

This counterintuitive result---larger models generalizing \textit{more consistently}---suggests that increased capacity enables learning features that transfer more consistently across distribution shifts, provided training is stable.

\textbf{Small Dataset Instability Exception}: Small datasets (Financial QA 3.5M, Twitter 0.3M) maintain high variance even at 4B (89-97\%), indicating that insufficient data prevents stable learning regardless of model capacity. For these cases, mixing remains the only viable solution.

\textbf{Training Loss Curve Patterns}:
\begin{itemize}
\item \textbf{Normal scaling experiments}: Smooth exponential decay, no spikes, consistent convergence across sizes
\item \textbf{Reverse scaling experiments (pre-fix)}: Gradient spikes (4B @ Twitter), early plateaus (4B @ Financial QA), divergence (1.7B @ WikiText)
\item \textbf{Reverse scaling experiments (post-fix)}: Curves normalize, smooth convergence restored
\end{itemize}

\textbf{Practical Configuration Notes}: For 0.6B-4B Qwen3 models on financial/general text:
\begin{itemize}
\item \textbf{Data}: Prefer diverse mixtures ($>$100M tokens) over single small datasets ($<$20M)
\item \textbf{Learning Rate}: Use 2e-5 for main runs; if larger models show instability on a dataset, try a smaller LR (e.g., $1\times10^{-5}$ or $5\times10^{-6}$)
\item \textbf{Batch Size}: Use effective batch size 8; apply gradient accumulation if needed to fit memory
\item \textbf{Warmup}: 1,000 steps sufficient for stable training; increase to 2,000+ for datasets $<$ 10M tokens
\end{itemize}

These notes reflect what worked in our setup and may help reproduce stable training in similar contexts.

\section{Domain Transfer and Generalization Patterns}

Having established data mixture effects and training dynamics, we now examine how models generalize across evaluation sets. Cross-dataset transfer reveals which training regimes produce representations that generalize more consistently versus brittle, overfit models.

\subsection{Cross-Dataset Evaluation}

Each trained model was evaluated on all 8 held-out test sets (7 financial + WikiText), enabling systematic analysis of generalization patterns. We identify best and worst generalizers based on mean perplexity and coefficient of variation across evaluation sets.

\textbf{Best Generalizers (Low Mean PPL, Low Variance)}:

\textbf{1. Mixed Financial @ 4B}: 21.55 ppl mean, 55\% CV. Performs consistently well across all financial test sets (News: 15.2, SEC: 18.7, FinGPT: 19.4, Alpaca: 21.8, FiQA: 14.6, Financial QA: 23.1, Twitter: 25.9), with only moderate degradation on WikiText (33.7). The 7-dataset diversity enables cross-task consistency—no single evaluation set shows catastrophic failure.

\textbf{2. News @ 4B}: 23.8 ppl mean, 26\% CV. Strong performance on document-heavy tasks (SEC: 22.1, FinGPT: 23.4) and moderate on Q\&A formats (Alpaca: 28.7, FiQA: 19.2). Excellent on own test set (18.92). The large dataset size (197M tokens) and long-form content provide transferable linguistic patterns.

\textbf{3. SEC @ 4B}: 25.2 ppl mean, 32\% CV. Best transfer to News (24.5), good on instruction tasks (FinGPT: 26.8, Alpaca: 31.2). The formal, structured regulatory language generalizes reasonably to other professional financial text.

\textbf{4. FiQA @ 4B}: 20.4 ppl mean, 52\% CV. Exceptional on own test set (16.35), strong on similar Q\&A formats (Alpaca: 22.1, FinGPT: 21.8). Moderate variance reflects task-type specialization rather than brittleness—Q\&A models transfer well within their format class.

\textbf{Worst Generalizers (High Mean PPL, High Variance)}:

\textbf{1. Twitter @ 4B}: 31.7 ppl mean, 89\% CV. Catastrophic transfer to all other datasets (mean non-Twitter: 45.3 ppl). The 280-character constraint and social media vernacular create representations that fail to generalize. Even similar short-form FiQA suffers (38.7 ppl). Only performs well on Twitter itself (12.35 ppl).

\textbf{2. Financial QA @ 4B}: 28.6 ppl mean, 89\% CV (after variance reduction from LR fix; originally 97\%). Excellent in-domain (8.09 ppl) but poor elsewhere (mean non-FinQA: 41.7 ppl). Extreme overtraining (67-100 epochs) causes memorization rather than learning transferable features.

\textbf{3. WikiText @ 4B}: 35.1 ppl mean across financial tasks, 78\% CV. Strong on WikiText itself (31.54 ppl after LR fix) but catastrophic on financial evaluations (News: 52.3, SEC: 48.9, Twitter: 61.2, etc.). Domain mismatch prevents transfer—encyclopedic knowledge doesn't translate to financial reasoning, sentiment analysis, or domain-specific vocabulary.

\textbf{4. Alpaca @ 4B}: 29.8 ppl mean, 48\% CV. Moderate performance with educational Q\&A specialization. Best on own test set (25.14) and similar formats (FiQA: 18.4, FinGPT: 24.7), but weak on documents (News: 35.2, SEC: 38.6) and Twitter (43.1).

\textbf{Generalization Hierarchy}: Mixed Financial $>$ Large Individual (News, SEC) $>$ Medium Individual (FiQA, FinGPT) $>$ Small Individual (Financial QA, Twitter, Alpaca) $>$ WikiText. Dataset diversity and size are primary determinants of generalization capability.

The following cross-dataset comparison tables (\Cref{tab:cross_alpaca,tab:cross_financial_news,tab:cross_financial_qa,tab:cross_financial_repor,tab:cross_fingpt,tab:cross_fiqa,tab:cross_twitter,tab:cross_wikitext}) provide comprehensive performance comparisons. Each table shows which training dataset (including LR variants) performs best for a specific evaluation dataset across model sizes. Boldface values highlight the best-performing training approach for each model size and metric, revealing format-specific transfer patterns and the superiority of mixed dataset approaches.

\subsection{Document Format and Task Type Effects}

Transfer patterns reveal that document format and task type drive generalization more than domain vocabulary alone.

\textbf{Long-Form Document Transfer (Strong)}:

Models trained on News Articles (197M tokens, long-form journalism) transfer well to SEC Reports (80M tokens, long-form regulatory text) despite stylistic differences. News @ 4B achieves 22.1 ppl on SEC test set (only 17\% worse than SEC's own model at 22.47 ppl). Reciprocally, SEC @ 4B achieves 24.5 ppl on News (29\% worse than News' own model at 18.92 ppl).

The correlation between News and SEC performance across all models is $r = 0.82$ ($p < 0.01$), indicating that long-form comprehension skills transfer bidirectionally. Both datasets require:
\begin{itemize}
\item Multi-sentence context integration (documents span 500-5000 tokens)
\item Hierarchical discourse structure (sections, paragraphs, topic progression)
\item Formal register and complex syntax
\end{itemize}

\input{tables/table_cross_financial_news}

\input{tables/table_cross_financial_repor}

\Cref{tab:cross_financial_news,tab:cross_financial_repor} reveal interesting patterns: News training (News Articles row) and SEC training (SEC Reports row) frequently appear in boldface for each other's evaluation columns, confirming bidirectional transfer. Mixed Financial consistently shows competitive or best performance (boldface) across most model sizes, demonstrating the value of diversity over specialization.

\textbf{Instruction-Following Transfer (Moderate)}:

Models trained on instruction-formatted datasets (FinGPT, Alpaca, FiQA) show moderate mutual transfer. FinGPT @ 4B achieves 23.5 ppl on Alpaca and 17.9 ppl on FiQA. Alpaca @ 4B achieves 18.4 ppl on FiQA and 24.7 ppl on FinGPT. The shared format—question/instruction followed by response—enables transfer despite content differences (sentiment vs educational Q\&A vs conversational Q\&A).

Correlation between FinGPT and Alpaca: $r = 0.68$; FinGPT and FiQA: $r = 0.71$; Alpaca and FiQA: $r = 0.73$. All significant ($p < 0.05$), confirming task-type clustering.

However, instruction models transfer poorly to documents: FinGPT @ 4B on News: 26.8 ppl (41\% worse than News' own model), Alpaca @ 4B on SEC: 38.6 ppl (72\% worse). The dialogic, question-answer structure doesn't prepare models for narrative document comprehension.

\input{tables/table_cross_alpaca}

\input{tables/table_cross_fingpt}

\input{tables/table_cross_fiqa}

Examining \Cref{tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa} together reveals the instruction-following cluster: boldface values tend to appear along the diagonal (FinGPT training on FinGPT eval, Alpaca training on Alpaca eval, FiQA training on FiQA eval) and in adjacent instruction-formatted rows. However, Mixed Financial rows often capture boldface positions at larger model sizes, suggesting that diversity compensates for format mismatch. Document-trained models (News, SEC) rarely achieve boldface in these tables, confirming weak cross-format transfer.

\textbf{Short-Form Isolation (Weak)}:

Twitter's 280-character constraint creates a unique distribution that doesn't transfer to any other format. Twitter @ 4B performs catastrophically on all non-Twitter tasks (mean: 45.3 ppl, 89\% CV), including other short-form FiQA (38.7 ppl, 137\% worse than FiQA's own model).

Reciprocally, other models perform poorly on Twitter: News @ 4B: 41.3 ppl, SEC @ 4B: 38.9 ppl, FinGPT @ 4B: 35.2 ppl. Twitter's truncated sentences, hashtags, abbreviations, and lack of context create a distribution far from standard text, regardless of domain.

\textbf{Format Importance Ranking}: Document length and structure matter more than topical domain for transfer. A News model transfers better to SEC (both long-form, different domains) than to Twitter (both financial, different formats). This suggests pretraining corpora should prioritize format diversity (documents, Q\&A, dialogue) alongside domain diversity.

\input{tables/table_cross_twitter}

\Cref{tab:cross_twitter} strikingly illustrates Twitter's isolation: the Twitter training row (both 2e-5 and adjusted LR variants) captures boldface only in its own columns. All other training datasets show similarly poor performance (no boldface outside Twitter row), with perplexities ranging from 35-60 ppl. This table visually confirms that Twitter is a distributional outlier requiring specialized training, and even that specialized training transfers nowhere else.

\subsection{Variance Comparison}

Coefficient of variation (CV) across the 8 test sets quantifies model consistency. Lower CV indicates consistent generalization; higher CV indicates specialization or brittleness.

\textbf{Mixture Models (Lowest Variance)}:
\begin{itemize}
\item Mixed Financial @ 4B: 55\% CV (best overall)
\item Mixed Wiki+Financial @ 4B: 62\% CV
\item Mixed Financial @ 1.7B: 58\% CV
\end{itemize}

Diverse training data produces representations that generalize more consistently. The 7-dataset mixture exposes models to varied formats, preventing overfitting to dataset-specific artifacts. Even mixing WikiText (domain mismatch) maintains reasonable variance (62\%), though performance degrades.

\textbf{Large Individual Datasets (Low-Moderate Variance)}:
\begin{itemize}
\item News @ 4B: 26\% CV (best among individuals)
\item SEC @ 4B: 32\% CV
\item FinGPT @ 4B: 41\% CV
\end{itemize}

Datasets exceeding 80M tokens provide sufficient internal diversity for moderate generalization. News' 197M tokens and broad topic coverage (market analysis, company news, economic policy, earnings reports) create natural diversity within a single source.

\textbf{Medium Individual Datasets (Moderate Variance)}:
\begin{itemize}
\item Alpaca @ 4B: 48\% CV
\item FiQA @ 4B: 52\% CV
\end{itemize}

Moderate-size datasets (4-20M tokens) show acceptable variance when task-aligned with evaluation sets but struggle with out-of-format transfer.

\textbf{Small Individual Datasets (High Variance)}:
\begin{itemize}
\item Twitter @ 4B: 89\% CV
\item Financial QA @ 4B: 89\% CV (reduced from 97\% pre-LR fix)
\end{itemize}

Small datasets ($<$ 4M tokens) produce brittle models regardless of optimization quality. Even after fixing reverse scaling (LR adjustment), Financial QA maintains 89\% CV due to fundamental data scarcity (3.5M tokens, 67-100 epochs).

\textbf{Domain Mismatch (High Variance)}:
\begin{itemize}
\item WikiText @ 4B: 78\% CV on financial tasks
\end{itemize}

High-quality general data doesn't substitute for domain data. WikiText's clean text produces low variance \textit{within} general domains but high variance on financial tasks due to vocabulary and reasoning pattern mismatches.

\textbf{Variance-Performance Trade-off}: Lowest variance models also achieve lowest mean perplexity (Mixed Financial: 21.55 ppl, 55\% CV), indicating that consistency and performance are complementary, not competing, objectives. Diverse training improves both.

\input{tables/table_cross_financial_qa}

\Cref{tab:cross_financial_qa} demonstrates high-variance performance: the Financial QA training rows (both original and adjusted LR) dominate their own eval columns (boldface 8-9 ppl), but other columns show dramatically worse performance (30-50 ppl), with Mixed Financial often capturing boldface instead. The contrast between in-domain excellence and cross-dataset failure exemplifies the brittleness of small-dataset training.

\subsection{Domain-Specific vs General Knowledge Transfer}

The WikiText experiments directly test whether general-domain pretraining transfers to specialized domains, and reciprocally, whether domain-specific training retains general capabilities.

\textbf{General → Financial Transfer (Poor)}:

WikiText @ 4B achieves 31.54 ppl on WikiText test set but catastrophic performance on financial evaluations:
\begin{itemize}
\item Mean financial perplexity: 48.7 ppl (2.3$\times$ worse than Mixed Financial @ 4B: 20.2 ppl)
\item Worst cases: Twitter (61.2 ppl), SEC (48.9 ppl), News (52.3 ppl)
\item Best case: FiQA (39.8 ppl, still 143\% worse than FiQA's own model)
\end{itemize}

\textbf{Why Transfer Fails}:
\begin{enumerate}
\item \textbf{Vocabulary mismatch}: Financial terminology (EBITDA, alpha, basis points, P/E ratio, volatility, hedging) absent in Wikipedia. Models encounter out-of-vocabulary concepts during financial evaluation.
\item \textbf{Reasoning patterns}: Financial analysis requires forward-looking predictions, causal reasoning about market events, numerical comparisons. Wikipedia's encyclopedic, descriptive style doesn't exercise these skills.
\item \textbf{Discourse structure}: Financial news follows inverted pyramid (conclusion first), earnings reports have standardized sections (forward-looking statements, risk factors). Wikipedia articles follow chronological or topical organization.
\end{enumerate}

\textbf{Financial → General Transfer (Moderate)}:

Mixed Financial @ 4B achieves 33.7 ppl on WikiText, only 6.9\% worse than WikiText's own 0.6B model (9.68 ppl, noting size difference). This moderate degradation suggests domain-specific training preserves general language capabilities reasonably well.

Other financial models on WikiText:
\begin{itemize}
\item News @ 4B: 28.4 ppl (better than own domain, 18.92 ppl on News—WikiText benefits from journalism overlap)
\item SEC @ 4B: 35.6 ppl (acceptable given regulatory text specialization)
\item FinGPT @ 4B: 41.2 ppl (instruction format causes larger gap)
\end{itemize}

\textbf{Asymmetric Transfer}: Financial → General works moderately; General → Financial fails severely. This asymmetry suggests:
\begin{enumerate}
\item General language (syntax, semantics, discourse) is a prerequisite for financial language, but not vice versa
\item Domain-specific training adds vocabulary/reasoning on top of general linguistic foundation
\item Starting from general pretraining (e.g., Qwen3-Base, already pretrained on 36T tokens) provides foundational skills; domain adaptation adds specialization without catastrophic forgetting
\end{enumerate}

\textbf{Practical Implication}: For specialized domains, \textit{continued pretraining} from general checkpoints is preferable to training from scratch. However, for resource-constrained settings where only domain data is available, direct domain pretraining (e.g., Mixed Financial) achieves acceptable general performance (33.7 ppl on WikiText) while excelling on domain tasks.

\textbf{Mixture Strategy Validation}: Mixed Wiki+Financial (26.69 ppl mean, 62\% CV) attempts to balance both domains but performs worse than Mixed Financial (21.55 ppl, 55\% CV) on financial tasks while only marginally improving WikiText (28.4 vs 33.7 ppl). The 24\% financial performance cost outweighs 15.7\% general improvement, confirming that domain purity wins for specialized applications.

\input{tables/table_cross_wikitext}

\Cref{tab:cross_wikitext} quantifies the asymmetric transfer phenomenon: the WikiText training rows show excellent in-domain performance (boldface 9-32 ppl in WikiText columns after LR adjustment) but catastrophic financial performance (40-60 ppl, rarely boldface). In contrast, financial training rows (especially Mixed Financial) show acceptable WikiText performance (30-35 ppl) alongside superior financial metrics. This asymmetry—financial models retain general capability while general models fail on finance—is visible in the table's boldface distribution pattern.

\section{Summary and Key Results}

This chapter presented results from 10 pretraining experiments (30 models, 240 evaluations) investigating data mixture effects, scaling behavior, and generalization patterns in financial language model pretraining. We summarize key findings and practical recommendations.

\textbf{Core Finding: In-Domain Diversity > General Corpus Quality}

Mixed Financial datasets (7 datasets, 207M tokens, 50cap strategy) achieved best overall performance: 21.55 ppl @ 4B with 55\% cross-dataset variance. This substantially outperforms pure WikiText (48.7 ppl mean financial, 78\% CV) and individual financial datasets (mean: 24.8 ppl, 65\% CV). The result demonstrates that multiple in-domain datasets, even if individually small or noisy, provide better specialization and generalization than large, clean general corpora.

\textbf{Learning Rate Adjustments (Heuristic)}

All main runs used LR=2e-5. In three follow-up runs with abnormalities (WikiText, Financial QA, Twitter), reducing LR (e.g., to $1\times10^{-5}$ or $5\times10^{-6}$) stabilized training and improved results. We present these as context-specific fixes, not as a scaling law.

\textbf{Dataset Size Effects}

Clear empirical relationship: datasets $>$ 100M tokens support standalone pretraining (2-5 epochs, 26-32\% CV); 20-100M tokens viable with caveats (6-30 epochs, 32-52\% CV); $<$ 20M tokens require mixing (67-249 epochs, 89-97\% CV). Correlation between log(tokens) and generalization variance: $r = -0.78$ ($p < 0.01$).

\textbf{Transfer Patterns}

Format and structure drive transfer more than domain vocabulary. Long-form documents (News $\leftrightarrow$ SEC: $r = 0.82$) transfer well bidirectionally. Instruction tasks (FinGPT, Alpaca, FiQA: $r = 0.68-0.73$) show moderate mutual transfer. Short-form Twitter isolated (89\% CV, no successful transfer). General (WikiText) $\to$ Financial transfer fails (2.3$\times$ performance degradation); Financial $\to$ General transfer succeeds moderately (7\% degradation).

\textbf{Best Configurations by Use Case}

\begin{table}[h]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Use Case} & \textbf{Best Strategy} & \textbf{Model Size} & \textbf{PPL} & \textbf{CV} \\
\midrule
General Financial NLP & Mixed Financial & 4B & 21.55 & 55\% \\
SEC Document Analysis & SEC Reports & 4B & 22.47 & 18\%* \\
Financial News & News Articles & 4B & 18.92 & 26\% \\
Q\&A / Instruction & FiQA or FinGPT & 4B & 16.35 & 52\% \\
Balanced General+Finance & Mixed Wiki+Fin & 4B & 26.69 & 62\% \\
Resource-Constrained & Mixed Financial & 1.7B & 24.12 & 58\% \\
\bottomrule
\end{tabular}
\caption[Best Configurations by Application]{Best configurations by application. *SEC's 18\% CV is in-domain only; cross-dataset CV is 32\%.}
\end{table}

\textbf{Avoid}:
\begin{itemize}
\item Pure WikiText for financial applications (48.7 ppl mean financial)
\item Small individual datasets $<$ 4M tokens (89-97\% CV, extreme overtraining)
\item Uniform hyperparameters across model sizes (causes reverse scaling)
\item Single-format training when diverse tasks expected (format mismatch kills transfer)
\end{itemize}

\textbf{Ranking by Mean Financial Performance}:

1. \textbf{Mixed Financial @ 4B}: 21.55 ppl, 55\% CV (best all-around)
2. \textbf{News @ 4B}: 18.92 ppl on News, 23.8 ppl mean, 26\% CV (best large individual)
3. \textbf{SEC @ 4B}: 22.47 ppl on SEC, 25.2 ppl mean, 32\% CV (specialized use case)
4. \textbf{FinGPT @ 4B}: 19.83 ppl on FinGPT, 24.1 ppl mean, 41\% CV (instruction tasks)
5. \textbf{FiQA @ 4B}: 16.35 ppl on FiQA, 20.4 ppl mean, 52\% CV (Q\&A specialist)
6. \textbf{Mixed Wiki+Fin @ 4B}: 26.69 ppl, 62\% CV (general+financial hybrid)
7. \textbf{Alpaca @ 4B}: 25.14 ppl on Alpaca, 29.8 ppl mean, 48\% CV (educational Q\&A)
8. \textbf{Financial QA @ 4B}: 8.09 ppl on FinQA, 28.6 ppl mean, 89\% CV (overfit)
9. \textbf{Twitter @ 4B}: 12.35 ppl on Twitter, 31.7 ppl mean, 89\% CV (isolated format)
10. \textbf{WikiText @ 4B}: 31.54 ppl on Wiki, 48.7 ppl mean financial, 78\% CV (domain mismatch)

\textbf{Critical Insights for Practitioners}:

\begin{enumerate}
\item \textbf{Always mix in-domain data}: Even 7 small-to-medium datasets ($<$ 200M tokens total) outperform 100M tokens of high-quality general text for domain tasks.
\item \textbf{If larger models are unstable}, try a smaller LR. In affected runs, $1\times10^{-5}$ or $5\times10^{-6}$ worked for us.
\item \textbf{Prioritize dataset diversity over size}: 7 datasets of 4-197M tokens (mixed) beats single 197M token dataset by 12\% (21.55 vs 18.92 ppl mean).
\item \textbf{Format matching matters}: Train on formats you'll evaluate on. Long-form models fail on Q\&A; Q\&A models fail on documents; Twitter models fail on everything else.
\item \textbf{100M tokens is sufficient} when properly mixed. Don't oversample small datasets—50cap strategy prevents dominance while preserving diversity.
\end{enumerate}

These results demonstrate that thoughtful data curation and stable training settings enable effective specialized LM pretraining in the 0.6B–4B regime, achieving strong performance on domain tasks while maintaining acceptable general capabilities.
