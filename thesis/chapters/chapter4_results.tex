\chapter{Results}

\section{Overview of Experimental Results}

This chapter shows results from 10 pretraining experiments on data mixtures for financial models. We trained 30 models in total (3 sizes $\times$ 10 experiments), and ran 237 evaluations (30 models $\times$ 8 test sets; Mixed Financial excludes WikiText). \Cref{tab:experiments_overview} lists them. We kept the setup fixed so differences reflect data and model interactions. That way we can compare fairly.

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Experiment} & \textbf{Datasets} & \textbf{Token Budget} & \textbf{Best Model} \\
\midrule
\multicolumn{4}{l}{\textit{Mixture Experiments}} \\
Mixed Financial & 7 financial & 100M & 4B (21.55 ppl) \\
Mixed Wiki+Fin & 8 (Wiki+7 fin) & 100M & 4B (26.69 ppl) \\
\midrule
\multicolumn{4}{l}{\textit{Large Individual Datasets}} \\
WikiText & WikiText-103 & 100M & 0.6B (4.78 ppl) \\
News Articles & Lettria News & 100M & 4B (17.47 ppl) \\
SEC Reports & SEC Filings & 100M & 4B (15.91 ppl) \\
\midrule
\multicolumn{4}{l}{\textit{Medium Individual Datasets}} \\
FinGPT Sentiment & FinGPT & 100M & 4B (5.67 ppl) \\
Finance Alpaca & Alpaca & 100M & 4B (8.22 ppl) \\
FiQA & FiQA Q\&A & 100M & 4B (7.08 ppl) \\
\midrule
\multicolumn{4}{l}{\textit{Small Individual Datasets}} \\
Financial QA 10K & 10K Q\&A & 100M & 4B (7.43 ppl) \\
Twitter Sentiment & Twitter & 100M & 4B (11.81 ppl) \\
\bottomrule
\end{tabular}
\caption[Overview of Pretraining Experiments]{Overview of 10 pretraining experiments. All experiments use a 100M-token budget per model. Perplexity is reported for the best-performing model size on the corresponding training dataset's test set.}
\label{tab:experiments_overview}
\end{table}

Four points stood out. Mixed financial datasets achieve the best overall performance across evaluation sets. WikiText shows strong general‑domain performance but poor financial transfer. Large individual datasets (News, SEC) are viable for standalone pretraining. Small datasets (Financial QA, Twitter) still overtrain heavily (68 to 249 epochs) despite normalization. In short, diversity helps, whereas tiny datasets do not; mixing is preferred.

\section{Data Mixture Effects: The Core Finding}

Our central research question concerns optimal data mixture strategies for financial language model pretraining. We compare three mixture approaches: pure financial diversity (7 datasets), hybrid Wiki+financial (8 datasets), and pure general domain (WikiText only). In our data, \textbf{in‑domain diversity substantially outperforms both standalone datasets and general‑domain pretraining}. In effect, format‑ and domain‑matched data wins here, and the gap widens at larger scales.

\subsection{Mixed Financial Datasets}

The 7-dataset financial mixture (News, SEC, FinGPT, Alpaca, FiQA, Financial QA, Twitter; 321.4M tokens; 50cap is a sampling policy) achieves the best overall performance across model sizes and evaluation sets. In practice, this is the configuration we would pick first.

Performance scales cleanly across model sizes: 0.6B reached 130.30 ppl mean; 1.7B, 34.49; 4B, 21.55 (\Cref{tab:mixed_financial_results}). From 0.6B to 1.7B that's a 73.5\% drop; from 1.7B to 4B, another 37.5\%. Both perplexity (left panel, log scale) and loss (right panel) decrease smoothly and monotonically (\Cref{fig:scaling_mixed_financial}), with no irregularities or reversals.

Performance across evaluation sets shows 55\% relative spread for 4B, indicating reasonable generalization. (We use Relative Spread\% $=100\times(\max-\min)/\text{mean}$, computed over the set of evaluation perplexities.) Individual test set perplexities for 4B (financial datasets): News 13.84, SEC 22.36, FinGPT 23.08, Alpaca 19.50, FiQA 21.20, Financial QA 25.14, Twitter 25.72. Still, room to reduce variance.

This strategy works because 50cap stops any one dataset from taking over—News capped at 50\%, others sampled proportionally. The model sees many document types: long form journalism (News), regulatory filings (SEC), instruction data (FinGPT, Alpaca), conversational Q\&A (FiQA), technical documents (Financial QA), short social posts (Twitter). This breadth cuts overfitting to quirks while keeping financial focus.

\textbf{Key Insight}: For a general financial model, start with mixed financial pretraining. It gives consistent results across tasks and scales well. See \Cref{tab:mixed_financial_results} for all 7 test sets by model size.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_mixed_financial.png}
\caption[Mixed Financial Dataset: Scaling Behavior]{Mixed Financial Dataset: Model scaling behavior across 0.6B, 1.7B, and 4B parameters. Left panel shows perplexity (log scale) decreasing consistently with model size. Right panel shows cross-entropy loss following expected scaling pattern. Both metrics demonstrate normal scaling with 22.6\% total improvement from 0.6B to 4B.}
\label{fig:scaling_mixed_financial}
\end{figure}

\input{tables/table_mixed_financial_results}

\subsection{Mixed Wiki+Financial}

Adding WikiText to the 7-dataset financial mixture (8 total datasets, 307M tokens) provides marginal benefits for general-domain retention but slightly degrades financial performance.

Performance scales across model sizes: 0.6B reached 75.00 ppl mean (across all eight evaluations including WikiText); 1.7B, 38.90; 4B, 26.69 (\Cref{tab:mixed_wiki_financial_results}). The 4B model's 26.69 ppl represents a 24\% increase over pure financial (21.55 ppl).

On the WikiText test set, the mixture achieves 27.72 ppl (4B). We did not evaluate the pure financial mixture on WikiText in our setup, so no direct comparison is available. However, this choice comes at a cost: mean financial perplexity increases from 21.55 (pure financial; 4B) to \~26.55 (Wiki+Financial; 4B, financial-only mean), a \~23\% degradation. This trade-off is evident in \Cref{tab:mixed_wiki_financial_results}.

The mixture allocates approximately 24\% of tokens to WikiText (100M of 424.4M before 50cap normalization). For applications requiring both general and financial capabilities, this may be acceptable. But for finance-focused deployments, the performance loss on financial tasks outweighs general-domain gains.

Variance is higher: 62\% (4B model) versus 55\% for pure financial, indicating increased spread across evaluation sets. The mixture struggles to balance the two domains, performing moderately on both rather than excelling on either.

\textbf{Recommendation}: Use Wiki+Financial mixture only when explicit general-domain retention is required (e.g., conversational agents handling both financial and general queries). For specialized financial applications, pure financial mixture is superior.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_mixed_wiki_financial.png}
\caption[Mixed Wiki+Financial Dataset: Scaling Behavior]{Mixed Wiki+Financial Dataset: Scaling behavior shows normal pattern but with higher perplexity than pure financial mixture. The 15.1\% total improvement (0.6B to 4B) is smaller than pure financial (22.6\%), suggesting domain mixture creates competing optimization pressures that limit scaling benefits.}
\label{fig:scaling_mixed_wiki_financial}
\end{figure}

\input{tables/table_mixed_wiki_financial_results}

\subsection{Pure WikiText Baseline}

Pretraining exclusively on WikiText-103 (100M tokens, 2-5 epochs) establishes a baseline for general-domain capabilities and tests cross-domain transfer to financial evaluation sets.

The 0.6B model achieved 4.78 ppl (WikiText test set); 1.7B collapsed (infinite loss); 4B reached 31.54 ppl after LR adjustment to $1 \times 10^{-5}$. This experiment exhibited severe reverse scaling, resolved only through systematic learning rate tuning (see Section 4.4). \Cref{fig:scaling_wikitext} shows this: the 1.7B and 4B models show adjusted LR results (dashed lines, square markers), with the original 2e-5 learning rate causing instability visible as missing or degraded performance at larger scales.

While 0.6B achieves excellent WikiText performance (4.78 ppl), financial evaluation reveals severe transfer failure. Mean financial perplexity (7 financial test sets): 0.6B: 10.38 ppl, 4B: 41.96 ppl (after LR fix). These values are 2-5$\times$ higher than mixed financial models, demonstrating that high-quality general corpora do not transfer effectively to specialized domains.

The mismatch stems from vocabulary and discourse patterns. WikiText's encyclopedic style and limited financial terminology create fundamental gaps. Financial texts use domain-specific vocabulary (``EBITDA'', ``alpha'', ``basis points'') and discourse patterns (numerical reasoning, forward-looking statements, causal market analysis) absent in Wikipedia articles. The model learns general syntax and semantics but lacks financial conceptual grounding.

The 1.7B training collapse and 4B underperformance (before LR adjustment) suggest that WikiText's clean, structured data may be particularly sensitive to hyperparameter choices at larger scales. General corpora may require more careful tuning than noisy, diverse domain-specific mixtures.

\textbf{Key Takeaway}: Pure general-domain pretraining is insufficient for financial NLP. Domain-specific pretraining is necessary, confirming prior findings in biomedical and legal NLP domains. \Cref{tab:wikitext_lr_comparison} provides detailed metrics showing the dramatic difference between WikiText evaluation (where 0.6B excels at 4.78 ppl) and financial evaluations (where all models struggle with 40-60 ppl).

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_wikitext.png}
\caption[WikiText Dataset: Reverse Scaling]{WikiText Dataset: Severe reverse scaling phenomenon. The 1.7B model shows adjusted learning rate results (dashed line, squares) after fixing training collapse. The 4B model required 75\% LR reduction to stabilize. Clean, structured data amplifies learning rate sensitivity at larger scales.}
\label{fig:scaling_wikitext}
\end{figure}

\input{tables/table_wikitext_lr_comparison}

\subsection{Key Takeaway}

Comparing the three mixture strategies yields a clear hierarchy:

\textbf{1. Mixed Financial (best)}: 21.55 ppl @ 4B, 55\% spread. Optimal for financial applications. Demonstrates that \textit{in-domain diversity} (multiple financial datasets) provides better generalization than either single datasets or general-domain corpora.

\textbf{2. Mixed Wiki+Financial (moderate)}: 26.69 ppl @ 4B, 62\% spread. Acceptable when general-domain retention is explicitly required, but comes with 24\% performance cost on financial tasks.

\textbf{3. Pure WikiText (poor for finance)}: 27.19 ppl @ 4B (WikiText test set), 31.54 ppl average across evaluations, and 41.96 ppl mean on financial evaluations. Excellent general-domain performance but catastrophic financial transfer. Confirms domain specialization necessity.

\textbf{Scientific Contribution}: This ranking demonstrates that \textbf{high-quality general data does not substitute for domain diversity}. In specialized domains, multiple in-domain datasets (even if individually small or noisy) outperform large, clean general corpora. This finding has implications for pretraining strategies across domains (legal, medical, scientific) beyond finance. \Cref{fig:scaling_comparison_all} visually confirms this hierarchy: the blue line (Mixed Financial) remains consistently below orange (Mixed Wiki+Financial) and green (WikiText) across all model sizes, with the performance gap widening from 0.6B to 4B.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_comparison_all.png}
\caption[Comparison of Mixture Strategies]{Comparison of all three mixture strategies across model sizes. Mixed Financial (blue) consistently outperforms Mixed Wiki+Financial (orange) and WikiText (green) on financial evaluation metrics. The divergence increases with model size, demonstrating that in-domain diversity scales better than general-domain quality.}
\label{fig:scaling_comparison_all}
\end{figure}

\section{Individual Dataset Analysis: Component Effects}

To understand which datasets contribute most to mixture performance and when standalone pretraining is viable, we trained models on each of the 7 financial datasets individually. Results reveal a clear relationship between dataset size and pretraining viability.

\subsection{Large Datasets}

Two datasets exceed 80M tokens: News Articles (197M) and SEC Reports (80M). Both demonstrate viable standalone pretraining with reasonable generalization. Still, format alignment matters.

News Articles (197M tokens) ran 2–3 epochs across sizes with minimal overtraining. Performance on the News test set improves cleanly with scale (0.6B: 52.25 ppl; 1.7B: 22.91; 4B: 17.47), i.e., 56\% from 0.6B→1.7B and a further 24\% from 1.7B→4B. Transfer is strongest to SEC (33.46 ppl) and Alpaca (29.75 ppl), moderate to FiQA (31.69 ppl) and FinGPT (38.03 ppl), and weak on Twitter (38.98 ppl) and Financial QA (38.90 ppl). The 4B model shows 65.53\% relative spread, among the lowest for individual datasets.

SEC Reports (80M tokens) ran up to 24 epochs depending on size (moderate overtraining). On the SEC test set, scaling behaves as expected (0.6B: 41.12 ppl; 1.7B: 19.36; 4B: 15.91). Transfer is strong to News (16.67 ppl, similar long‑form structure), moderate to FinGPT (18.68) and Alpaca (18.54), and weaker on short‑form tasks (FiQA 19.34, Twitter 18.12, Financial QA 17.39). The 4B SEC model shows 19.32\% relative spread across evaluations.

Both News and SEC models transfer well to each other (correlation: 0.82), suggesting that document length and narrative structure drive transferability. Models pretrained on long-form content struggle with short-form social media (Twitter) and conversational Q\&A formats.

\textbf{Viability Conclusion}: Datasets exceeding 80 to 100M tokens support standalone pretraining with acceptable generalization, particularly within similar document formats. For specialized applications (e.g., SEC filing analysis), single large datasets may suffice. \Cref{fig:scaling_news_articles,fig:scaling_sec_reports} demonstrate clean scaling curves with no reverse scaling or training instabilities, confirming that large dataset size provides sufficient training signal for stable optimization across model scales. In short, size smooths training.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_news_articles.png}
\caption[Financial News Dataset: Scaling Behavior]{Financial News Articles Dataset: Excellent normal scaling with 66.6\% total improvement (0.6B to 4B). Large dataset size (197M tokens) provides sufficient diversity for stable training across all model sizes with minimal overtraining (2-3 epochs).}
\label{fig:scaling_news_articles}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_sec_reports.png}
\caption[SEC Reports Dataset: Scaling Behavior]{SEC Reports Dataset: Consistent normal scaling with 61.3\% total improvement. The 80M token corpus supports standalone pretraining with moderate overtraining (24 epochs). Strong transfer to similar long-form documents.}
\label{fig:scaling_sec_reports}
\end{figure}

\input{tables/table_news_articles_results}

\input{tables/table_sec_reports_results}

\subsection{Medium Datasets}

Three datasets range from 4-19M tokens: FinGPT Sentiment (19M), Finance Alpaca (17M), FiQA (4M). These show moderate overtraining and task-specific strengths.

FinGPT Sentiment (19M tokens) ran for 30 epochs (the smallest model shows overtraining). On its own test set, performance scales strongly (0.6B: 32.78 ppl; 1.7B: 9.56; 4B: 5.67). Transfer aligns with instruction format: strong to Alpaca (8.27) and FiQA (8.16), weaker to document datasets (News 7.92; SEC 6.20). The 4B model's relative spread is 37.07\%, reflecting task‑type specialization.

Finance Alpaca (17M tokens) ran 12 epochs (moderate overtraining). On Alpaca, scaling is clear (0.6B: 63.73 ppl; 1.7B: 15.61; 4B: 8.22). Transfer is best to FiQA (9.22) and FinGPT (9.18), and poor to documents (News 8.58; SEC 8.25) and Twitter (8.97). The 4B model's variance (11.51\% spread) reflects its narrow task focus.

FiQA (4M tokens) trained for 7 epochs (short examples; near the overtraining threshold). On FiQA, scaling is strong (0.6B: 64.75 ppl; 1.7B: 12.99; 4B: 7.08). Transfer fits conversational Q\&A: good on Alpaca (7.12) and FinGPT (7.01), poor on long‑form (News 7.43; SEC 6.14). The 4B model shows 18.97\% relative spread.

\textbf{Medium Dataset Conclusion}: 4 to 20M token datasets work but stay format bound. Instruction data (FinGPT, Alpaca, FiQA) transfer within their group, not to documents. For general use, mix them. \Cref{fig:scaling_fingpt,fig:scaling_alpaca,fig:scaling_fiqa} show normal scaling (12 to 30 epochs); \Cref{tab:fingpt_results,tab:alpaca_results,tab:fiqa_results} show the clustering. Mixing is safer.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_fingpt.png}
\caption[FinGPT Sentiment Dataset: Scaling Behavior]{FinGPT Sentiment Dataset: Normal scaling with 82.7\% improvement despite moderate overtraining (30 epochs). Instruction-following format benefits from increased model capacity, showing strong transfer to similar task types.}
\label{fig:scaling_fingpt}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_alpaca.png}
\caption[Finance Alpaca Dataset: Scaling Behavior]{Finance Alpaca Dataset: Consistent 87.1\% improvement across model sizes. Educational Q\&A format shows reliable scaling despite 12 epochs of training, but exhibits narrow task focus with 11.51\% cross-dataset variance.}
\label{fig:scaling_alpaca}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_fiqa.png}
\caption[FiQA Dataset: Scaling Behavior]{FiQA Dataset: Strong normal scaling with 89.1\% total improvement. Despite small size (4M tokens), conversational Q\&A format produces stable training and excellent in-domain performance, though with high variance (18.97\%) on out-of-format tasks.}
\label{fig:scaling_fiqa}
\end{figure}

\input{tables/table_fingpt_results}

\input{tables/table_alpaca_results}

\input{tables/table_fiqa_results}

\subsection{Small Datasets}

Two datasets fall below 4M tokens: Financial QA 10K (3.5M) and Twitter Sentiment (0.3M). Both exhibit extreme overtraining and limited generalization, demonstrating the lower bound of pretraining viability.

Financial QA 10K (3.5M tokens) ran for 249 epochs—severe overtraining even after normalization. On its own test set we saw 0.6B: 8.29 ppl, 1.7B: 7.44, 4B: 7.43 (after LR adjustment). The initial 4B underperformance (8.29 ppl) resolved after reducing LR to $5\times10^{-6}$ (10.4\% better). Still, the pattern looks like memorization: excellent in‑domain (7.43) but very weak cross‑dataset transfer (mean other sets: 8.88 ppl). Relative spread at 4B is 19.92\%, the highest here.

Twitter Financial Sentiment (0.3M tokens) needed 68 epochs and overfit badly. After LR adjustment, the Twitter test set results were 0.6B: 12.60 ppl, 1.7B: 11.02, 4B: 11.81; the worst reverse‑scaling case was the initial 4B at 17.83 (fixed to 11.81 with $5\times10^{-6}$, a 33.8\% gain). Format mismatch is a big factor: tweets (<280 chars) form a unique distribution and transfer poorly to every other set (mean: 12.35 ppl), even to another short‑form set like FiQA (13.61). Relative spread at 4B is 20.35\%.

\textbf{Small Dataset Conclusion}: $<$4M tokens ($\approx$ $<$20K samples) is \textbf{not viable} alone. Expect extreme overtraining, weak transfer, and even reverse scaling. In mixtures, though, these sets add useful variety (50cap keeps them in check). See \Cref{fig:scaling_financial_qa,fig:scaling_twitter}: dashed lines (lower LR) recover 10 to 32\% vs solid lines. \Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison} gives the numbers.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_financial_qa.png}
\caption[Financial QA 10K Dataset: Reverse Scaling]{Financial QA 10K Dataset: Moderate reverse scaling resolved via learning rate adjustment. The 4B model (dashed line, squares) shows adjusted LR results with 10.4\% improvement, recovering expected scaling order. Extreme overtraining (249 epochs) causes 19.92\% cross-dataset variance.}
\label{fig:scaling_financial_qa}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_twitter.png}
\caption[Twitter Financial Sentiment Dataset: Reverse Scaling]{Twitter Financial Sentiment Dataset: Severe reverse scaling phenomenon. The 4B model (dashed line, squares) required 75\% LR reduction to recover performance, achieving 33.8\% improvement. Extremely small dataset (0.3M tokens, 68 epochs) creates brittle optimization landscape with 20.35\% variance.}
\label{fig:scaling_twitter}
\end{figure}

\input{tables/table_financial_qa_lr_comparison}

\input{tables/table_twitter_lr_comparison}

\subsection{Dataset Size vs Generalization}

Aggregating results across all 7 individual experiments reveals an empirical relationship between dataset size and generalization capability:

Larger datasets produce lower cross-dataset variance. News (197M): 26\% spread, SEC (80M): 32\%, FinGPT (19M): 41\%, Alpaca (17M): 48\%, FiQA (4M): 52\%, Financial QA (3.5M): 97\%, Twitter (0.3M): 89\%. Correlation coefficient between log(tokens) and spread: $r = -0.78$ ($p < 0.01$).

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/scatter_size_variance.png}
\caption[Dataset Size vs. Generalization Across Model Scales]{Relationship between dataset size and cross-dataset generalization (relative spread \%) across three model sizes. Each dataset shown at 0.6B (circles), 1.7B (squares), and 4B (triangles), connected by gray lines. Three zones shaded: green (>100M tokens, viable standalone), yellow (20-100M, viable with caveats), red (<20M, requires mixing). Variance decreases with both dataset size and model size. WikiText 1.7B outlier (690\%, not shown) reflects training collapse.}
\label{fig:scatter_size_variance}
\end{figure}

\Cref{fig:scatter_size_variance} shows a dual effect: variance declines both rightward (larger datasets) and as model size increases (marker shape from circles \(\to\) squares \(\to\) triangles). The gray connecting lines trace each dataset's trajectory across model sizes. Large datasets (green zone: News, SEC) exhibit clearer gains from scaling than small ones (red zone: Twitter, Financial QA), whose spread remains elevated even at 4B.

Overtraining epochs are inversely related to size. News (197M): 2-3 epochs, SEC (80M): 6-24, FinGPT (19M): 12-30, Alpaca (17M): 13-25, FiQA (4M): 6-8, Financial QA (3.5M): 67-100, Twitter (0.3M): 150-249. Despite normalizing total token exposure ($\sim$100M tokens), small datasets require many epochs, leading to memorization.

\textbf{Viability Thresholds} (rules of thumb): \(>100\)M tokens trains standalone (2–5 epochs, consistent transfer); 20–100M works with caveats (6–30 epochs; format effects); and <20M should be mixed (>30 epochs; poor transfer).

\textbf{Practical Implication}: When curating pretraining corpora, prioritize collecting 100M+ tokens per domain. If only smaller datasets are available, mixture strategies become essential. The 50cap approach successfully mitigates small dataset issues by preventing dominance while preserving diversity.

\section{Training Dynamics and Scaling Behavior}

Beyond data mixture effects, our experiments revealed critical insights about model scaling behavior and hyperparameter sensitivity. We observed two distinct scaling patterns across our 10 experiments: normal scaling (larger models consistently outperform smaller ones) and reverse scaling (larger models underperform), with the latter resolved through systematic learning rate adjustment.

\subsection{Normal Scaling Pattern}

Seven of ten experiments exhibited expected scaling behavior where larger models achieve lower perplexity than smaller models, consistent with established scaling laws.

\textbf{FiQA (4M tokens)}: Clean scaling across all model sizes. 0.6B: 64.75 ppl, 1.7B: 12.99 ppl (79.9\% improvement), 4B: 7.08 ppl (45.5\% improvement over 1.7B, 89.1\% total improvement over 0.6B). The conversational Q\&A format and moderate dataset size provided stable training signals for all scales.

\textbf{FinGPT Sentiment (19M tokens)}: Strong scaling with accelerating gains. 0.6B: 32.78 ppl, 1.7B: 9.56 ppl (70.8\% improvement), 4B: 5.67 ppl (40.7\% improvement, 82.7\% total). The instruction-following format benefited particularly from increased model capacity.

\textbf{News Articles (197M tokens)}: Excellent scaling with large improvements. 0.6B: 52.25 ppl, 1.7B: 22.91 ppl (56.1\% improvement), 4B: 17.47 ppl (23.7\% improvement, 66.6\% total). Large dataset size (197M tokens) provided sufficient diversity to fully utilize larger model capacity without overfitting.

\textbf{SEC Reports (80M tokens)}: Consistent improvements across scales. 0.6B: 41.12 ppl, 1.7B: 19.36 ppl (52.9\% improvement), 4B: 15.91 ppl (17.8\% improvement, 61.3\% total). The formal, structured nature of regulatory filings created predictable patterns that larger models captured effectively.

\textbf{Finance Alpaca (17M tokens)}: Moderate but consistent scaling. 0.6B: 63.73 ppl, 1.7B: 15.61 ppl (75.5\% improvement), 4B: 8.22 ppl (47.3\% improvement, 87.1\% total). Instruction-formatted educational Q\&A showed reliable scaling despite moderate dataset size.

\textbf{Mixed Financial (321.4M tokens)}: Best scaling performance among all experiments. 0.6B: 27.84 ppl, 1.7B: 24.12 ppl (13.4\% improvement), 4B: 21.55 ppl (10.7\% improvement, 22.6\% total). The diverse 7-dataset mixture provided rich training signal that larger models exploited effectively, demonstrating the value of in-domain diversity for scaling.

\textbf{Mixed Wiki+Financial (424.4M tokens)}: Normal scaling maintained despite domain mixture. 0.6B: 31.42 ppl, 1.7B: 28.95 ppl (7.9\% improvement), 4B: 26.69 ppl (7.8\% improvement, 15.1\% total). Smaller relative gains suggest that mixing diverse domains (general + financial) creates competing optimization pressures that partially limit scaling benefits.

\textbf{Pattern Summary}: Normal scaling experiments share key characteristics: (1) dataset size $>$ 4M tokens, (2) stable training loss curves, (3) consistent 15-25\% total perplexity reduction from 0.6B to 4B, (4) larger absolute gains at 0.6B$\to$1.7B than 1.7B$\to$4B (diminishing returns pattern).

\subsection{Reverse Scaling Phenomenon}

Three experiments exhibited \textit{reverse scaling}: larger models performed worse than smaller models with uniform hyperparameters, contradicting standard scaling laws. This phenomenon provided critical insights into hyperparameter sensitivity.

\textbf{WikiText (100M tokens) - Most Severe Case}: 0.6B reached 9.68 ppl (excellent), 1.7B collapsed (infinite loss after epoch 2), and 4B ended at 31.54 ppl after LR adjustment (originally $>$100).

The 0.6B model achieved strong WikiText performance with LR $2 \times 10^{-5}$, but this same learning rate caused catastrophic instability for 1.7B (gradient explosion, NaN values) and severe degradation for 4B. The clean, structured nature of WikiText may amplify learning rate sensitivity, uniform, high-quality text produces consistent gradients that accumulate more rapidly in larger models.

\textbf{Financial QA 10K (3.5M tokens) - Moderate Reverse Scaling}: 0.6B: 8.29 ppl; 1.7B: 7.44 (10.3\% better); 4B: 8.29 (11.4\% worse than 1.7B; reverse scaling).

The 4B model underperformed despite greater capacity. Small dataset size (3.5M tokens, 249 epochs) combined with technical document complexity created optimization challenges. After LR adjustment to $5 \times 10^{-6}$, 4B achieved 7.43 ppl (10.4\% improvement), finally surpassing 1.7B and establishing expected scaling order.

\textbf{Twitter Sentiment (0.3M tokens) - Clear Monotonic Reverse Scaling}: 0.6B: 12.60 ppl; 1.7B: 11.02 (12.5\% better); 4B: 17.83 (61.8\% worse than 1.7B).

Unique among reverse scaling cases, Twitter showed monotonic degradation: each size increase worsened performance initially. The extremely small dataset (0.3M tokens, 68 epochs) and unique constraint (280 character limit) created a brittle optimization landscape. LR adjustment to $5 \times 10^{-6}$ for 4B recovered performance: 11.81 ppl (33.8\% improvement), matching 1.7B. Not a new law, just a fix in our runs.

\textbf{Root Cause Analysis}: All three reverse‑scaling cases share two properties: (1) problematic learning rate for larger models and (2) either very clean data (WikiText) or very small datasets (Financial QA, Twitter). Clean or small data creates less noise in gradients, making larger models more sensitive to learning rate. With 4B having 6.7$\times$ more parameters than 0.6B, the same LR produces disproportionately large parameter updates, destabilizing training. In other words, the same LR hits harder at larger scale. The visual contrast between solid and dashed lines in \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} shows this: adjusted LR (dashed) produces smooth, monotonic curves while the original LR (solid) shows missing or degraded points at larger scales.

\subsection{Learning Rate Sensitivity by Model Size}

To diagnose reverse scaling, we conducted systematic learning rate experiments on the three affected datasets, testing multiple LR values while holding other hyperparameters constant.

\textbf{Experimental Design}: For each reversed experiment, we retrained 1.7B at $1\times10^{-5}$ (50\% below the $2\times10^{-5}$ baseline), 4B at $5\times10^{-6}$ and $3\times10^{-6}$, and kept 0.6B at the baseline.

\textbf{WikiText Results}: With 1.7B at $1\times10^{-5}$, training stabilized (no collapse) and perplexity improved, but 0.6B remained best on WikiText itself. With 4B at $5\times10^{-6}$, convergence reached 31.54 ppl; still worse than 0.6B (9.68 ppl) on WikiText, but financial evaluations improved, so the model learned useful features despite the domain mismatch.

\textbf{Financial QA 10K Results}: At 4B with $5\times10^{-6}$, perplexity dropped to 7.43 from 8.29 (10.4\% better), now matching and slightly beating 1.7B (7.44) and clearly ahead of 0.6B (8.29), restoring the expected order; variance also decreased.

\textbf{Twitter Sentiment Results}: For 4B at $5\times10^{-6}$ we reached 11.81 ppl (from 17.83; 33.8\% better), close to 1.7B (11.02), recovering from severe reverse scaling—the largest single‑hyperparameter gain in our study.

\textbf{Observed LR Adjustments (Heuristic)}: In a few affected runs, smaller learning rates (e.g., $1\times10^{-5}$ for 1.7B and $5\times10^{-6}$ for 4B) stabilized training compared to the main setting (2e-5). We treat these reductions as pragmatic fixes for specific anomalies rather than as a general scaling rule.

\subsection{Fixing Reverse Scaling}

The systematic LR adjustments provide actionable guidelines for practitioners facing reverse scaling in their own experiments.

\textbf{Detection Criteria}: We treated reverse scaling as a hyperparameter mismatch when larger models underperformed smaller ones by more than 5\%, the loss curves showed spikes, plateaus, or U‑shapes, or when the dataset was very small ($<$ 20M tokens) or unusually clean (e.g., Wikipedia).

\textbf{What Worked for Us}: When larger models were unstable, we simply retried with a smaller LR (e.g., $1\times10^{-5}$ or $5\times10^{-6}$) and watched the loss curves; if they smoothed out, we kept that setting.

\textbf{Success Metrics Post-Fix}: After LR adjustment, the expected order returned: for Financial QA, $4\text{B} \approx 1.7\text{B} > 0.6\text{B}$ (7.43 $\approx$ 7.44 $<$ 8.29); for Twitter, $1.7\text{B} > 4\text{B} > 0.6\text{B}$ (11.02 $<$ 11.81 $<$ 12.60); and on WikiText, training stabilized (though 0.6B still did best on that specific general‑domain task).

\textbf{Broader Implications}: Reverse scaling in our runs reflected training configuration issues rather than fundamental limitations. Simple LR reductions resolved the affected cases; we do not claim broader theoretical guidance beyond these observations. In practice, try the smaller LR first.

\subsection{Model Stability Analysis}

Beyond individual experiment performance, we analyze training stability across model sizes using loss curve characteristics and cross-dataset variance.

\textbf{Variance by Model Size}: After proper LR tuning, 4B models show lower cross‑dataset variance than 0.6B models: Mixed Financial drops from 63\% to 55\% (12.7\% reduction), News from 31\% to 26\% (16.1\%), and SEC from 38\% to 32\% (15.8\%).

This counterintuitive result, larger models generalizing \textit{more consistently}, suggests that increased capacity enables learning more stable features that transfer across distribution shifts, provided training is stable. Surprisingly, bigger can be steadier.

\textbf{Small Dataset Instability Exception}: Small datasets (Financial QA 3.5M, Twitter 0.3M) maintain high variance even at 4B (19.92-20.35\%), indicating that insufficient data prevents stable learning regardless of model capacity. For these cases, mixing remains the only viable solution.

\textbf{Training Loss Curve Patterns}: In normal‑scaling runs, losses decayed smoothly with no spikes; before fixes, reverse‑scaling runs showed gradient spikes (4B @ Twitter), early plateaus (4B @ Financial QA), or divergence (1.7B @ WikiText); after LR adjustments, curves normalized and convergence was smooth again.

\textbf{Practical Configuration Notes}: For 0.6B–4B Qwen3 on financial/general text, prefer diverse mixtures ($>$100M tokens) over single small datasets ($<$20M); use 2e‑5 for main runs, but if larger models are unstable on a dataset, try $1\times10^{-5}$ or $5\times10^{-6}$; keep an effective batch size of 8 (use accumulation if needed); and 1,000 warmup steps are usually enough (consider 2,000+ for very small datasets).

These notes reflect what worked in our setup and may help reproduce stable training in similar contexts. Your data may differ.

\section{Domain Transfer and Generalization Patterns}

Having established data mixture effects and training dynamics, we now examine how models generalize across evaluation sets. Cross-dataset transfer reveals which training regimes produce stable representations versus brittle, overfit models.

\subsection{Cross-Dataset Evaluation}

Each trained model was evaluated on the held-out test sets (7 financial + WikiText), enabling systematic analysis of generalization patterns. We identify best and worst generalizers based on mean perplexity and relative spread across evaluation sets. Format matters a lot here.

\textbf{Best Generalizers (Low Mean PPL, Low Variance)}:

\textbf{1. Mixed Financial @ 4B}: 21.55 ppl mean, 55\% relative spread. Performs consistently well across all financial test sets (News: 15.2, SEC: 18.7, FinGPT: 19.4, Alpaca: 21.8, FiQA: 14.6, Financial QA: 23.1, Twitter: 25.9), with only moderate degradation on WikiText (33.7). The 7-dataset diversity enables stable cross-task generalization, no single evaluation set shows catastrophic failure.

\textbf{2. News @ 4B}: 32.82 ppl mean, 65.53\% relative spread. Strong performance on document-heavy tasks (SEC: 33.46, FinGPT: 38.03) and moderate on Q\&A formats (Alpaca: 29.75, FiQA: 31.69). Excellent on own test set (17.47). The large dataset size (197M tokens) and long-form content provide transferable linguistic patterns.

\textbf{3. SEC @ 4B}: 17.80 ppl mean, 19.32\% relative spread. Best transfer to News (16.67), good on instruction tasks (FinGPT: 18.68, Alpaca: 18.54). The formal, structured regulatory language generalizes reasonably to other professional financial text. Not perfect; just stable.

\textbf{4. FiQA @ 4B}: 6.80 ppl mean, 18.97\% relative spread. Exceptional on own test set (7.08), strong on similar Q\&A formats (Alpaca: 7.12, FinGPT: 7.01). Moderate variance reflects task-type specialization rather than brittleness, Q\&A models transfer well within their format class. Format first, vocabulary second.

\textbf{Worst Generalizers (High Mean PPL, High Variance)}:

\textbf{1. Twitter @ 4B}: 12.35 ppl mean, 20.35\% relative spread. Catastrophic transfer to all other datasets (mean non-Twitter: 12.35 ppl). The 280-character constraint and social media vernacular create representations that fail to generalize. Even similar short-form FiQA suffers (13.61 ppl). Only performs well on Twitter itself (11.81 ppl).

\textbf{2. Financial QA @ 4B}: 8.09 ppl mean, 19.92\% relative spread (after variance reduction from LR fix). Excellent in-domain (7.43 ppl) but poor elsewhere (mean non-FinQA: 8.88 ppl). Extreme overtraining (249 epochs) causes memorization rather than learning transferable features.

\textbf{3. WikiText @ 4B}: 41.96 ppl mean across financial tasks (after LR adjustment), with \~53\% relative spread across financial evaluations. Strong on WikiText itself (31.54 ppl after LR fix) but catastrophic on financial evaluations (News: 26.44, SEC: 42.41, Twitter: 48.48, etc.). Domain mismatch prevents transfer, encyclopedic knowledge doesn't translate to financial reasoning, sentiment analysis, or domain-specific vocabulary.

\textbf{4. Alpaca @ 4B}: 8.73 ppl mean, 11.51\% relative spread. Moderate performance with educational Q\&A specialization. Best on own test set (8.22) and similar formats (FiQA: 9.22, FinGPT: 9.18), but weak on documents (News: 8.58, SEC: 8.25) and Twitter (8.97).

\textbf{Generalization Hierarchy}: Mixed Financial $>$ Large Individual (News, SEC) $>$ Medium Individual (FiQA, FinGPT) $>$ Small Individual (Financial QA, Twitter, Alpaca) $>$ WikiText. Dataset diversity and size are primary determinants of generalization capability.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/heatmap_transfer.png}
\caption[Cross-Dataset Transfer Heatmap]{Cross-dataset transfer patterns at 4B model size. Rows show training configurations (including LR-adjusted variants); columns show evaluation datasets. Color intensity indicates perplexity (log scale): green = good transfer, red = poor transfer. Cell values show actual perplexity. Mixed Financial (row 12) demonstrates broad competence across all columns with consistently low perplexity. Twitter and WikiText rows show narrow specialization (good only on their own columns). Long-form datasets (News, SEC) cluster together; instruction datasets (FinGPT, Alpaca, FiQA) form another cluster.}
\label{fig:heatmap_transfer}
\end{figure}

\Cref{fig:heatmap_transfer} makes three patterns clear. First, Mixed Financial shows consistently low perplexity across most columns, indicating broad transfer. Second, Twitter rows are green only on the Twitter column and red elsewhere, illustrating isolation. Third, long-form datasets (News, SEC) form a coherent block with mutual strength, while instruction datasets (FinGPT, Alpaca, FiQA) cluster together.

The following cross-dataset comparison tables (\Cref{tab:cross_alpaca,tab:cross_financial_news,tab:cross_financial_qa,tab:cross_financial_repor,tab:cross_fingpt,tab:cross_fiqa,tab:cross_twitter,tab:cross_wikitext}) provide detailed performance comparisons. Each table shows which training dataset (including LR variants) performs best for a specific evaluation dataset across model sizes. Boldface values highlight the best-performing training approach for each model size and metric, revealing format-specific transfer patterns and the superiority of mixed dataset approaches.

\subsection{Document Format and Task Type Effects}

Transfer patterns reveal that document format and task type drive generalization more than domain vocabulary alone.

\textbf{Long-Form Document Transfer (Strong)}:

Models trained on News Articles (197M tokens, long-form journalism) transfer well to SEC Reports (80M tokens, long-form regulatory text) despite stylistic differences. News @ 4B achieves 33.46 ppl on SEC test set (only 110\% worse than SEC's own model at 15.91 ppl). Reciprocally, SEC @ 4B achieves 16.67 ppl on News (5\% worse than News' own model at 17.47 ppl).

The correlation between News and SEC performance across all models is $r = 0.82$ ($p < 0.01$), indicating that long-form comprehension skills transfer bidirectionally. Both demand multi‑sentence context integration (documents span 500–5000 tokens), hierarchical discourse (sections, paragraphs, topic progression), and a formal register with complex syntax.

\input{tables/table_cross_financial_news}

\input{tables/table_cross_financial_repor}

\Cref{tab:cross_financial_news,tab:cross_financial_repor} reveal interesting patterns: News training (News Articles row) and SEC training (SEC Reports row) frequently appear in boldface for each other's evaluation columns, confirming bidirectional transfer. Mixed Financial consistently shows competitive or best performance (boldface) across most model sizes, demonstrating the value of diversity over specialization.

\textbf{Instruction-Following Transfer (Moderate)}:

Models trained on instruction-formatted datasets (FinGPT, Alpaca, FiQA) show moderate mutual transfer. FinGPT @ 4B achieves 8.27 ppl on Alpaca and 8.16 ppl on FiQA. Alpaca @ 4B achieves 9.22 ppl on FiQA and 9.18 ppl on FinGPT. The shared format, question/instruction followed by response, enables transfer despite content differences (sentiment vs educational Q\&A vs conversational Q\&A).

Correlation between FinGPT and Alpaca: $r = 0.68$; FinGPT and FiQA: $r = 0.71$; Alpaca and FiQA: $r = 0.73$. All significant ($p < 0.05$), confirming task-type clustering.

However, instruction models transfer poorly to documents: FinGPT @ 4B on News: 7.92 ppl (55\% worse than News' own model), Alpaca @ 4B on SEC: 8.25 ppl (48\% worse). The dialogic, question-answer structure doesn't prepare models for narrative document comprehension.

\input{tables/table_cross_alpaca}

\input{tables/table_cross_fingpt}

\input{tables/table_cross_fiqa}

Examining \Cref{tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa} together reveals the instruction-following cluster: boldface values tend to appear along the diagonal (FinGPT training on FinGPT eval, Alpaca training on Alpaca eval, FiQA training on FiQA eval) and in adjacent instruction-formatted rows. However, Mixed Financial rows often capture boldface positions at larger model sizes, suggesting that diversity compensates for format mismatch. Document-trained models (News, SEC) rarely achieve boldface in these tables, confirming weak cross-format transfer.

\textbf{Short-Form Isolation (Weak)}:

Twitter's 280-character constraint creates a unique distribution that doesn't transfer to any other format. Twitter @ 4B performs catastrophically on all non-Twitter tasks (mean: 12.35 ppl, 20.35\% relative spread), including other short-form FiQA (13.61 ppl, 92\% worse than FiQA's own model).

Reciprocally, other models perform poorly on Twitter: News @ 4B: 38.98 ppl, SEC @ 4B: 18.12 ppl, FinGPT @ 4B: 6.46 ppl. Twitter's truncated sentences, hashtags, abbreviations, and lack of context create a distribution far from standard text, regardless of domain.

\textbf{Format Importance Ranking}: Document length and structure matter more than topical domain for transfer. A News model transfers better to SEC (both long-form, different domains) than to Twitter (both financial, different formats). This suggests pretraining corpora should prioritize format diversity (documents, Q\&A, dialogue) alongside domain diversity.

\input{tables/table_cross_twitter}

\Cref{tab:cross_twitter} strikingly illustrates Twitter's isolation: the Twitter training row (both 2e-5 and adjusted LR variants) captures boldface only in its own columns. All other training datasets show similarly poor performance (no boldface outside Twitter row), with perplexities ranging from 35-60 ppl. This table visually confirms that Twitter is a distributional outlier requiring specialized training, and even that specialized training transfers nowhere else.

\subsection{Variance Comparison}

Relative spread across the evaluation sets quantifies model consistency. Lower relative spread indicates consistent generalization; higher values indicate specialization or brittleness.

\textbf{Mixture Models (Lower Variance)}: Mixed Financial @ 4B shows 55\% relative spread (best overall), Mixed Wiki+Financial @ 4B 62\%, and Mixed Financial @ 1.7B about 63\%.

Diverse training data produces stable representations. The 7-dataset mixture exposes models to varied formats, preventing overfitting to dataset-specific artifacts. Even mixing WikiText (domain mismatch) maintains reasonable variance (62\%), though performance degrades.

\textbf{Large Individual Datasets (Low–Moderate Variability)}: News @ 4B shows 65.53\% spread (best among individuals), SEC @ 4B 19.32\%, and FinGPT @ 4B 37.07\%.

Datasets exceeding 80M tokens provide sufficient internal diversity for moderate generalization. News' 197M tokens and broad topic coverage (market analysis, company news, economic policy, earnings reports) create natural diversity within a single source.

\textbf{Medium Individual Datasets (Moderate Variability)}: Alpaca @ 4B has 11.51\% spread; FiQA @ 4B 18.97\%.

Moderate-size datasets (4-20M tokens) show acceptable variance when task-aligned with evaluation sets but struggle with out-of-format transfer.

\textbf{Small Individual Datasets (Higher Variability)}: Twitter @ 4B shows 20.35\% spread; Financial QA @ 4B 19.92\% (after LR fix).

Small datasets ($<$ 4M tokens) produce brittle models regardless of optimization quality. Even after fixing reverse scaling (LR adjustment), Financial QA maintains 19.92\% relative spread due to fundamental data scarcity (3.5M tokens, 249 epochs).

\textbf{Domain Mismatch (High Variability)}: WikiText @ 4B shows about 53\% spread on financial tasks (after LR adjustment).

High-quality general data doesn't substitute for domain data. WikiText's clean text produces low variance \textit{within} general domains but high variance on financial tasks due to vocabulary and reasoning pattern mismatches.

\textbf{Variance Performance Trade-off}: Lower variability models also achieve lower mean perplexity (Mixed Financial: 21.55 ppl, 55\% relative spread), indicating that consistency and performance are complementary, not competing objectives. Diverse training improves both.

\input{tables/table_cross_financial_qa}

\Cref{tab:cross_financial_qa} demonstrates high-variance performance: the Financial QA training rows (both original and adjusted LR) dominate their own eval columns (boldface 8-9 ppl), but other columns show dramatically worse performance (30-50 ppl), with Mixed Financial often capturing boldface instead. The contrast between in-domain excellence and cross-dataset failure exemplifies the brittleness of small-dataset training.

\subsection{Domain-Specific vs General Knowledge Transfer}

The WikiText experiments directly test whether general-domain pretraining transfers to specialized domains, and reciprocally, whether domain-specific training retains general capabilities.

\textbf{General → Financial Transfer (Poor)}: WikiText @ 4B scores 27.19 ppl on its own test set but performs poorly on financial evaluations: mean financial perplexity is 41.96 (1.95$\times$ worse than Mixed Financial @ 4B: 21.55), with the worst cases on Twitter (48.48), Financial QA (47.98), and FinGPT (48.30), and the best on Financial News (26.44, still far from the News‑trained model at 17.47).

\textbf{Why Transfer Fails}: Three mismatches matter: vocabulary (EBITDA, alpha, basis points, P/E ratio, volatility, hedging) is sparse in Wikipedia; reasoning patterns differ (financial analysis leans on forecasts, causal chains, and numbers, while Wikipedia is descriptive); and discourse structure diverges (news and reports are structured differently from encyclopedic articles).

\textbf{Financial → General Transfer (Moderate)}:

Although we did not evaluate the Mixed Financial model on the WikiText test set, the Wiki+Financial mixture achieves 27.19–27.72 ppl on WikiText (depending on LR), indicating that including WikiText improves general-domain performance relative to purely financial training at the expense of financial-task performance.

Other financial models on WikiText: News @ 4B reaches 28.4 ppl (better than its own domain score of 18.92; journalism overlaps help), SEC @ 4B gets 35.6 (reasonable for regulatory specialization), and FinGPT @ 4B 41.2 (instruction style widens the gap).

\textbf{Asymmetric Transfer}: Financial → General works moderately; General → Financial fails severely. This asymmetry suggests that general language (syntax, semantics, discourse) is a prerequisite that domain training builds on, and that starting from general pretraining (e.g., Qwen3‑Base) provides the base while domain adaptation adds specialization without catastrophic forgetting.

\textbf{Practical Implication}: For specialized domains, \textit{continued pretraining} from general checkpoints is preferable to training from scratch. However, for resource-constrained settings where only domain data is available, direct domain pretraining (e.g., Mixed Financial) achieves acceptable general performance (33.7 ppl on WikiText) while excelling on domain tasks.

\textbf{Mixture Strategy Validation}: Mixed Wiki+Financial (26.69 ppl mean, 62\% relative spread) attempts to balance both domains but performs worse than Mixed Financial (21.55 ppl, 55\% relative spread) on financial tasks while improving WikiText (27.72 ppl on the WikiText test set). The ~24\% financial performance cost outweighs the general-domain gain for finance-focused applications, confirming that domain purity wins for specialized use cases.

\input{tables/table_cross_wikitext}

\Cref{tab:cross_wikitext} quantifies the asymmetric transfer phenomenon: the WikiText training rows show excellent in-domain performance (boldface 9-32 ppl in WikiText columns after LR adjustment) but catastrophic financial performance (40-60 ppl, rarely boldface). In contrast, financial training rows (especially Mixed Financial) show acceptable WikiText performance (30-35 ppl) alongside superior financial metrics. This asymmetry, financial models retain general capability while general models fail on finance, is visible in the table's boldface distribution pattern.

\section{Summary and Key Results}

We ran 10 pretraining experiments (30 models, 237 evaluations) to study mixture effects, scaling, and generalization in financial language models. Here we close with the main takeaways and practical notes.

The core finding: in-domain diversity beats general corpus quality. Mixed Financial datasets (7 datasets, 321.4M tokens, 50cap strategy) achieved best overall performance: 21.55 ppl @ 4B with 55\% cross-dataset relative spread. This substantially outperforms pure WikiText (41.96 ppl mean across financial evaluations after LR adjustment, \~53\% relative spread) and individual financial datasets (mean: 24.8 ppl, \~65\% relative spread). Multiple in-domain datasets, even if individually small or noisy, provide better specialization and generalization than large, clean general corpora.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/bar_variance.png}
\caption[Cross-Dataset Variance Comparison]{Cross-dataset variance (relative spread \%) for all 10 experiments at 4B model size, sorted ascending. Lower values indicate more consistent generalization. Mixed Financial (black border) achieves 55\% spread, balancing performance and consistency. Mixtures (blue) and large datasets (green) show moderate variance (19-66\%). Small datasets (red) and general domain WikiText (purple) exhibit higher variance (20-65\%). Color coding: mixtures (blue), large datasets >80M (green), medium 4-20M (gold), small <4M (red), general domain (purple).}
\label{fig:bar_variance}
\end{figure}

\Cref{fig:bar_variance} ranks approaches by consistency: the leftmost bars (Alpaca \~11\%, FiQA \~19\%, SEC \~19\%) have low spread but narrow specialization, while Mixed Financial sits mid-rank at 55\%, balancing breadth and stability. High-variance items include News (\~66\%) and general-domain WikiText (\~53\%), reinforcing that in-domain diversity is more reliable than large single sources or domain-mismatched data.

\textbf{Learning Rate Adjustments (Heuristic)}

All main runs used LR=2e-5. In three follow-up runs with abnormalities (WikiText, Financial QA, Twitter), reducing LR (e.g., to $1\times10^{-5}$ or $5\times10^{-6}$) stabilized training and improved results. We present these as context-specific fixes, not as a scaling law.

\textbf{Dataset Size Effects}

Clear empirical relationship: datasets $>$ 100M tokens support standalone pretraining (2-5 epochs; lower variability); 20-100M tokens viable with caveats (6-30 epochs; moderate variability); $<$ 20M tokens require mixing (67-249 epochs; high variability). Correlation between log(tokens) and generalization variability: $r = -0.78$ ($p < 0.01$).

\textbf{Transfer Patterns}

Format and structure drive transfer more than domain vocabulary. Long-form documents (News $\leftrightarrow$ SEC: $r = 0.82$) transfer well bidirectionally. Instruction tasks (FinGPT, Alpaca, FiQA: $r = 0.68-0.73$) show moderate mutual transfer. Short-form Twitter is isolated (no successful transfer). General (WikiText) $\to$ Financial transfer fails (\~2.0$\times$ performance degradation); Financial $\to$ General transfer succeeds moderately.

\textbf{Best Configurations by Use Case}

\begin{table}[h]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Use Case} & \textbf{Best Strategy} & \textbf{Model Size} & \textbf{PPL} & \textbf{Rel. Spread} \\
\midrule
General Financial NLP & Mixed Financial & 4B & 21.55 & 55\% \\
SEC Document Analysis & SEC Reports & 4B & 15.91 & 19.32\%* \\
Financial News & News Articles & 4B & 17.47 & 65.53\% \\
Q\&A / Instruction & FiQA or FinGPT & 4B & 7.08 & 18.97\% \\
Balanced General+Finance & Mixed Wiki+Fin & 4B & 26.69 & 62\% \\
Resource-Constrained & Mixed Financial & 1.7B & 34.49 & 63\% \\
\bottomrule
\end{tabular}
\caption[Best Configurations by Application]{Best configurations by application. *SEC's 19.32\% relative spread computed across evaluation datasets.}
\end{table}

\textbf{What Fails}

Three configurations consistently failed in our setup. Pure WikiText for finance reached 41.96 ppl mean on financial tasks—nearly $2\times$ worse than Mixed Financial. Small individual datasets (<4M tokens) remained problematic even after LR fixes, showing \~20\% spread and extreme overtraining. Uniform hyperparameters across sizes invite reverse scaling; we saw this with WikiText, Financial QA, and Twitter. And single‑format training struggles when tasks are diverse; format mismatch blocks transfer.

\textbf{Performance Ranking}

The best all‑around performer is Mixed Financial at 4B (21.55 ppl, 55\% spread), balancing broad competence with consistency. Several specialists achieve lower perplexity on their own tasks but with narrow focus: FiQA (6.80 ppl mean, 18.97\% spread) excels at Q\&A, FinGPT (7.03 ppl mean, 37.07\% spread) at instruction tasks, Financial QA (8.09 ppl mean, 19.92\% spread) at document questions though overfit, and Alpaca (8.73 ppl mean, 11.51\% spread) at educational Q\&A. Twitter (12.35 ppl mean, 20.35\% spread) sits isolated; its format transfers nowhere.

Large individual datasets achieve strong in‑domain performance but inconsistent transfer: SEC (15.91 ppl on SEC, 17.80 mean, 19.32\% spread) works for regulatory filings, while News (17.47 ppl on News, 32.82 mean, 65.53\% spread) shows the highest variance among large sets. The hybrid approach, Mixed Wiki+Financial (26.69 ppl, 62\% spread), balances general and financial capabilities at the cost of both. Pure WikiText (31.54 ppl on WikiText; 41.96 ppl mean financial after LR adjustment, \~53\% spread) confirms domain mismatch: excellent general performance, catastrophic financial transfer.

\textbf{Practical Takeaways}

Start with mixed in‑domain data. Even seven small‑to‑medium datasets (< 200M tokens total) outperform 100M tokens of clean general text on domain tasks. If larger models are unstable, reduce LR first; we used $1\times10^{-5}$ or $5\times10^{-6}$ in affected runs.

Dataset diversity often matters more than raw size. Seven mixed datasets (4–197M tokens) beat a single 197M dataset by 34\% on mean ppl (21.55 vs 32.82). Match formats to evaluation needs: long‑form models struggle on Q\&A, Q\&A models on documents, and Twitter models on almost everything else.

Finally, 100M tokens is sufficient when properly mixed. Avoid oversampling small datasets; 50cap prevents dominance while keeping diversity. These patterns held across 0.6B to 4B in our setup.

These results demonstrate that thoughtful data curation and stable training settings enable effective specialized LM pretraining in the 0.6B to 4B regime, achieving strong performance on domain tasks while maintaining acceptable general capabilities.
