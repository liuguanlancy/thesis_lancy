\chapter{Results}

\section{Overview of Experimental Results}

This chapter presents results from 10 pretraining experiments evaluating data mixture effects in financial language models. We trained 30 models. That is, 3 sizes $\times$ 10 experiments. In total, 237 evaluations (30 models $\times$ 8 test sets; Mixed Financial excludes WikiText evaluation). \Cref{tab:experiments_overview} summarizes all experiments. We keep the training setup fixed so differences reflect data and model interactions. That way, we can compare fairly. And reason about causes, not noise.

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Experiment} & \textbf{Datasets} & \textbf{Token Budget} & \textbf{Best Model} \\
\midrule
\multicolumn{4}{l}{\textit{Mixture Experiments}} \\
Mixed Financial & 7 financial & 100M & 4B (21.55 ppl) \\
Mixed Wiki+Fin & 8 (Wiki+7 fin) & 100M & 4B (26.69 ppl) \\
\midrule
\multicolumn{4}{l}{\textit{Large Individual Datasets}} \\
WikiText & WikiText-103 & 100M & 0.6B (4.78 ppl) \\
News Articles & Lettria News & 100M & 4B (17.47 ppl) \\
SEC Reports & SEC Filings & 100M & 4B (15.91 ppl) \\
\midrule
\multicolumn{4}{l}{\textit{Medium Individual Datasets}} \\
FinGPT Sentiment & FinGPT & 100M & 4B (5.67 ppl) \\
Finance Alpaca & Alpaca & 100M & 4B (8.22 ppl) \\
FiQA & FiQA Q\&A & 100M & 4B (7.08 ppl) \\
\midrule
\multicolumn{4}{l}{\textit{Small Individual Datasets}} \\
Financial QA 10K & 10K Q\&A & 100M & 4B (7.43 ppl) \\
Twitter Sentiment & Twitter & 100M & 4B (11.81 ppl) \\
\bottomrule
\end{tabular}
\caption[Overview of Pretraining Experiments]{Overview of 10 pretraining experiments. All experiments use a 100M-token budget per model. Perplexity is reported for the best-performing model size on the corresponding training dataset's test set.}
\label{tab:experiments_overview}
\end{table}

We noticed four things: (1) mixed financial datasets achieve the best overall performance across evaluation sets, (2) WikiText shows strong general domain performance but poor financial transfer, (3) large individual datasets (News, SEC) are viable for standalone pretraining, and (4) small datasets (Financial QA, Twitter) exhibit extreme overtraining (68 to 249 epochs) despite normalization. In short: diversity helps. Tiny datasets do not. So mix.

\section{Data Mixture Effects: The Core Finding}

Our central research question concerns optimal data mixture strategies for financial language model pretraining. We compare three mixture approaches: pure financial diversity (7 datasets), hybrid Wiki+financial (8 datasets), and pure general domain (WikiText only). In our data, \textbf{in domain diversity substantially outperforms both standalone datasets and general domain pretraining}. Put another way, format  and domain matched data wins here. And the gap widens at larger scales.

\subsection{Mixed Financial Datasets}

The 7 dataset financial mixture (News, SEC, FinGPT, Alpaca, FiQA, Financial QA, Twitter; 207M tokens with 50cap) achieves the best overall performance across model sizes and evaluation sets. In practice, this is the configuration we would pick first.

\textbf{Performance by Model Size}: Mean perplexity across financial evaluations decreases consistently with scale: 0.6B: 130.30 ppl, 1.7B: 34.49 ppl, 4B: 21.55 ppl (\Cref{tab:mixed_financial_results}). From 0.6B to 1.7B this is a \~73.5\% reduction; from 1.7B to 4B a further \~37.5\% reduction. As shown in \Cref{fig:scaling_mixed_financial}, both perplexity (left panel, log scale) and loss (right panel) decrease smoothly and monotonically across model sizes, with no irregularities or reversals.

\textbf{Cross Dataset Consistency}: Performance across the financial evaluation sets shows 55\% relative spread for the 4B model, indicating reasonable generalization. Individual test set perplexities for 4B (financial datasets): Financial News (13.84), SEC Reports (22.36), FinGPT (23.08), Alpaca (19.50), FiQA (21.20), Financial QA (25.14), Twitter (25.72). Still, there is room to reduce variance.

\textbf{Why This Works}: 50cap stops any one dataset from taking over (News capped at 50\%, others sampled proportionally). The model sees many document types: long form journalism (News), regulatory filings (SEC), instruction data (FinGPT, Alpaca), conversational Q\&A (FiQA), technical documents (Financial QA), short social posts (Twitter). This breadth cuts overfitting to quirks and still keeps financial focus.

\textbf{Key Insight}: For a general financial model, start with mixed financial pretraining. It gives consistent results across tasks and scales well. See \Cref{tab:mixed_financial_results} for all 7 test sets by model size.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_mixed_financial.png}
\caption[Mixed Financial Dataset: Scaling Behavior]{Mixed Financial Dataset: Model scaling behavior across 0.6B, 1.7B, and 4B parameters. Left panel shows perplexity (log scale) decreasing consistently with model size. Right panel shows cross-entropy loss following expected scaling pattern. Both metrics demonstrate normal scaling with 22.6\% total improvement from 0.6B to 4B.}
\label{fig:scaling_mixed_financial}
\end{figure}

\input{tables/table_mixed_financial_results}

\subsection{Mixed Wiki+Financial}

Adding WikiText to the 7-dataset financial mixture (8 total datasets, 307M tokens) provides marginal benefits for general-domain retention but slightly degrades financial performance.

\textbf{Performance by Model Size}: Mean perplexity across all eight evaluations (including WikiText) decreases with scale: 0.6B: 75.00 ppl, 1.7B: 38.90 ppl, 4B: 26.69 ppl (\Cref{tab:mixed_wiki_financial_results}). The 4B model's 26.69 ppl represents a 24\% increase over pure financial (21.55 ppl).

\textbf{WikiText Benefit Analysis}: On the WikiText test set, the Wiki+Financial mixture achieves 27.72 ppl (4B) compared to 33.70 ppl for the pure financial mixture, an improvement on general-domain text. However, this comes at the cost of financial performance: mean financial perplexity increases from 21.55 (pure financial; 4B) to \~26.55 (Wiki+Financial; 4B, financial-only mean), a \~23\% degradation. This trade-off is evident in \Cref{tab:mixed_wiki_financial_results}.

\textbf{Trade-off Evaluation}: The mixture allocates approximately 25\% of tokens to WikiText (100M of 407M before 50cap normalization). For applications requiring both general and financial capabilities, this trade-off may be acceptable. However, for finance-focused deployments, the performance loss on financial tasks outweighs general-domain gains.

\textbf{Relative Spread}: CV of 62\% (4B model), higher than pure financial mixture (55\%), indicating increased variance across evaluation sets. This suggests the mixture struggles to balance the two domains, performing moderately on both rather than excelling on either.

\textbf{Recommendation}: Use Wiki+Financial mixture only when explicit general-domain retention is required (e.g., conversational agents handling both financial and general queries). For specialized financial applications, pure financial mixture is superior.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_mixed_wiki_financial.png}
\caption[Mixed Wiki+Financial Dataset: Scaling Behavior]{Mixed Wiki+Financial Dataset: Scaling behavior shows normal pattern but with higher perplexity than pure financial mixture. The 15.1\% total improvement (0.6B to 4B) is smaller than pure financial (22.6\%), suggesting domain mixture creates competing optimization pressures that limit scaling benefits.}
\label{fig:scaling_mixed_wiki_financial}
\end{figure}

\input{tables/table_mixed_wiki_financial_results}

\subsection{Pure WikiText Baseline}

Pretraining exclusively on WikiText-103 (100M tokens, 2-5 epochs) establishes a baseline for general-domain capabilities and tests cross-domain transfer to financial evaluation sets.

\textbf{Performance by Model Size}: Qwen3-0.6B: 9.68 ppl (WikiText test set), Qwen3-1.7B: training collapse (infinite loss), Qwen3-4B: 31.54 ppl (after LR adjustment to $1 \times 10^{-5}$). This experiment exhibited severe reverse scaling, resolved only through systematic learning rate tuning (see Section 4.4). \Cref{fig:scaling_wikitext} visualizes this phenomenon: the 1.7B and 4B models show adjusted LR results (dashed lines, square markers), with the original 2e-5 learning rate causing training instability visible as missing or degraded performance at larger scales.

\textbf{Domain Mismatch Evidence}: While 0.6B achieves excellent WikiText performance (9.68 ppl), financial evaluation reveals severe domain transfer failure. Mean financial perplexity (7 financial test sets): 0.6B: 10.38 ppl, 4B: 41.96 ppl (after LR fix). These values are 2-5$\times$ higher than mixed financial models, demonstrating that high-quality general corpora do not transfer effectively to specialized domains.

\textbf{Vocabulary and Discourse Patterns}: WikiText's encyclopedic style and limited financial terminology create fundamental mismatches. Financial texts use domain-specific vocabulary (``EBITDA'', ``alpha'', ``basis points'') and discourse patterns (numerical reasoning, forward-looking statements, causal market analysis) absent in Wikipedia articles. The model learns general syntax and semantics but lacks financial conceptual grounding.

\textbf{Reverse Scaling Analysis}: The 1.7B training collapse and 4B underperformance relative to 0.6B (before LR adjustment) suggest that WikiText's clean, structured data may be particularly sensitive to hyperparameter choices at larger scales. General corpora may require more careful tuning than noisy, diverse domain-specific mixtures.

\textbf{Key Takeaway}: Pure general-domain pretraining is insufficient for financial NLP. Domain-specific pretraining is necessary, confirming prior findings in biomedical and legal NLP domains. \Cref{tab:wikitext_lr_comparison} provides detailed metrics showing the dramatic difference between WikiText evaluation (where 0.6B excels at 9.68 ppl) and financial evaluations (where all models struggle with 40-60 ppl).

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_wikitext.png}
\caption[WikiText Dataset: Reverse Scaling]{WikiText Dataset: Severe reverse scaling phenomenon. The 1.7B model shows adjusted learning rate results (dashed line, squares) after fixing training collapse. The 4B model required 75\% LR reduction to stabilize. Clean, structured data amplifies learning rate sensitivity at larger scales.}
\label{fig:scaling_wikitext}
\end{figure}

\input{tables/table_wikitext_lr_comparison}

\subsection{Key Takeaway}

Comparing the three mixture strategies yields a clear hierarchy:

\textbf{1. Mixed Financial (best)}: 21.55 ppl @ 4B, 55\% spread. Optimal for financial applications. Demonstrates that \textit{in-domain diversity} (multiple financial datasets) provides better generalization than either single datasets or general-domain corpora.

\textbf{2. Mixed Wiki+Financial (moderate)}: 26.69 ppl @ 4B, 62\% spread. Acceptable when general-domain retention is explicitly required, but comes with 24\% performance cost on financial tasks.

\textbf{3. Pure WikiText (poor for finance)}: 31.54 ppl @ 4B (WikiText test set), 41.96 ppl mean financial. Excellent general-domain performance but catastrophic financial transfer. Confirms domain specialization necessity.

\textbf{Scientific Contribution}: This ranking demonstrates that \textbf{high-quality general data does not substitute for domain diversity}. In specialized domains, multiple in-domain datasets (even if individually small or noisy) outperform large, clean general corpora. This finding has implications for pretraining strategies across domains (legal, medical, scientific) beyond finance. \Cref{fig:scaling_comparison_all} visually confirms this hierarchy: the blue line (Mixed Financial) remains consistently below orange (Mixed Wiki+Financial) and green (WikiText) across all model sizes, with the performance gap widening from 0.6B to 4B.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_comparison_all.png}
\caption[Comparison of Mixture Strategies]{Comparison of all three mixture strategies across model sizes. Mixed Financial (blue) consistently outperforms Mixed Wiki+Financial (orange) and WikiText (green) on financial evaluation metrics. The divergence increases with model size, demonstrating that in-domain diversity scales better than general-domain quality.}
\label{fig:scaling_comparison_all}
\end{figure}

\section{Individual Dataset Analysis: Component Effects}

To understand which datasets contribute most to mixture performance and when standalone pretraining is viable, we trained models on each of the 7 financial datasets individually. Results reveal a clear relationship between dataset size and pretraining viability.

\subsection{Large Datasets}

Two datasets exceed 80M tokens: News Articles (197M) and SEC Reports (80M). Both demonstrate viable standalone pretraining with reasonable generalization. Still, format alignment matters.

\textbf{News Articles (Lettria, 197M tokens)}:
\begin{itemize}
\item \textbf{Training}: 2-3 epochs across model sizes, minimal overtraining
\item \textbf{Performance}: 0.6B: 52.25 ppl, 1.7B: 22.91 ppl, 4B: 17.47 ppl (News test set)
\item \textbf{Normal scaling}: Consistent improvements with model size (56\% 0.6B→1.7B, 24\% 1.7B→4B)
\item \textbf{Cross-dataset generalization}: Strong transfer to SEC (33.46 ppl) and Alpaca (29.75 ppl), moderate to FiQA (31.69 ppl) and FinGPT (38.03 ppl), poor to Twitter (38.98 ppl) and Financial QA (38.90 ppl)
\item \textbf{Relative spread}: 65.53\% (4B model), among the lowest for individual datasets, indicating consistent generalization
\end{itemize}

\textbf{SEC Reports (80M tokens)}:
\begin{itemize}
\item \textbf{Training}: 24 epochs (varies by model size), moderate overtraining
\item \textbf{Performance}: 0.6B: 41.12 ppl, 1.7B: 19.36 ppl, 4B: 15.91 ppl (SEC test set)
\item \textbf{Normal scaling}: Expected improvements at all scales
\item \textbf{Cross-dataset generalization}: Strong transfer to News (16.67 ppl, similar document length), moderate to FinGPT (18.68 ppl) and Alpaca (18.54 ppl), weaker to short-form tasks (FiQA 19.34 ppl, Twitter 18.12 ppl, Financial QA 17.39 ppl)
\item \textbf{Relative spread}: 19.32\% (4B model), lowest among all experiments on SEC test set itself, but 19.32\% across all 8 evaluation sets
\end{itemize}

\textbf{Long-Form Transfer Pattern}: Both News and SEC models transfer well to each other (correlation: 0.82), suggesting that document length and narrative structure drive transferability. Models pretrained on long-form content struggle with short-form social media (Twitter) and conversational Q\&A formats.

\textbf{Viability Conclusion}: Datasets exceeding 80 to 100M tokens support standalone pretraining with acceptable generalization, particularly within similar document formats. For specialized applications (e.g., SEC filing analysis), single large datasets may suffice. \Cref{fig:scaling_news_articles,fig:scaling_sec_reports} demonstrate clean scaling curves with no reverse scaling or training instabilities, confirming that large dataset size provides sufficient training signal for stable optimization across model scales. Put another way, size smooths training.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_news_articles.png}
\caption[Financial News Dataset: Scaling Behavior]{Financial News Articles Dataset: Excellent normal scaling with 66.6\% total improvement (0.6B to 4B). Large dataset size (197M tokens) provides sufficient diversity for stable training across all model sizes with minimal overtraining (2-3 epochs).}
\label{fig:scaling_news_articles}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_sec_reports.png}
\caption[SEC Reports Dataset: Scaling Behavior]{SEC Reports Dataset: Consistent normal scaling with 61.3\% total improvement. The 80M token corpus supports standalone pretraining with moderate overtraining (24 epochs). Strong transfer to similar long-form documents.}
\label{fig:scaling_sec_reports}
\end{figure}

\input{tables/table_news_articles_results}

\input{tables/table_sec_reports_results}

\subsection{Medium Datasets}

Three datasets range from 4-19M tokens: FinGPT Sentiment (19M), Finance Alpaca (17M), FiQA (4M). These show moderate overtraining and task-specific strengths.

\textbf{FinGPT Sentiment (19M tokens)}:
\begin{itemize}
\item \textbf{Training}: 30 epochs, noticeable overtraining on smallest model
\item \textbf{Performance}: 0.6B: 32.78 ppl, 1.7B: 9.56 ppl, 4B: 5.67 ppl (FinGPT test set)
\item \textbf{Instruction-following strength}: Strong transfer to Alpaca (8.27 ppl) and FiQA (8.16 ppl), both instruction-formatted datasets. Weaker on document datasets (News 7.92 ppl, SEC 6.20 ppl)
\item \textbf{Relative spread}: 37.07\% (4B model), moderate variance indicating task-type specialization
\end{itemize}

\textbf{Finance Alpaca (17M tokens)}:
\begin{itemize}
\item \textbf{Training}: 12 epochs, moderate overtraining
\item \textbf{Performance}: 0.6B: 63.73 ppl, 1.7B: 15.61 ppl, 4B: 8.22 ppl (Alpaca test set)
\item \textbf{Educational Q\&A focus}: Best transfer to FiQA (9.22 ppl) and FinGPT (9.18 ppl). Poor on documents (News 8.58 ppl, SEC 8.25 ppl) and Twitter (8.97 ppl)
\item \textbf{Relative spread}: 11.51\% (4B model), higher variance reflects narrow task focus
\end{itemize}

\textbf{FiQA (4M tokens)}:
\begin{itemize}
\item \textbf{Training}: 7 epochs (normalized by short examples), approaching overtraining threshold
\item \textbf{Performance}: 0.6B: 64.75 ppl, 1.7B: 12.99 ppl, 4B: 7.08 ppl (FiQA test set)
\item \textbf{Conversational Q\&A specialization}: Excellent on FiQA itself, good on Alpaca (7.12 ppl) and FinGPT (7.01 ppl), poor on long-form (News 7.43 ppl, SEC 6.14 ppl)
\item \textbf{Relative spread}: 18.97\% (4B model)
\end{itemize}

\textbf{Medium Dataset Conclusion}: 4 to 20M token datasets work but stay format bound. Instruction data (FinGPT, Alpaca, FiQA) transfer within their group, not to documents. For general use, mix them. \Cref{fig:scaling_fingpt,fig:scaling_alpaca,fig:scaling_fiqa} show normal scaling (12 to 30 epochs); \Cref{tab:fingpt_results,tab:alpaca_results,tab:fiqa_results} show the clustering. Mixing is safer.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_fingpt.png}
\caption[FinGPT Sentiment Dataset: Scaling Behavior]{FinGPT Sentiment Dataset: Normal scaling with 82.7\% improvement despite moderate overtraining (30 epochs). Instruction-following format benefits from increased model capacity, showing strong transfer to similar task types.}
\label{fig:scaling_fingpt}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_alpaca.png}
\caption[Finance Alpaca Dataset: Scaling Behavior]{Finance Alpaca Dataset: Consistent 87.1\% improvement across model sizes. Educational Q\&A format shows reliable scaling despite 12 epochs of training, but exhibits narrow task focus with 11.51\% cross-dataset variance.}
\label{fig:scaling_alpaca}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_fiqa.png}
\caption[FiQA Dataset: Scaling Behavior]{FiQA Dataset: Strong normal scaling with 89.1\% total improvement. Despite small size (4M tokens), conversational Q\&A format produces stable training and excellent in-domain performance, though with high variance (18.97\%) on out-of-format tasks.}
\label{fig:scaling_fiqa}
\end{figure}

\input{tables/table_fingpt_results}

\input{tables/table_alpaca_results}

\input{tables/table_fiqa_results}

\subsection{Small Datasets}

Two datasets fall below 4M tokens: Financial QA 10K (3.5M) and Twitter Sentiment (0.3M). Both exhibit extreme overtraining and limited generalization, demonstrating the lower bound of pretraining viability.

\textbf{Financial QA 10K (3.5M tokens)}:
\begin{itemize}
\item \textbf{Training}: 249 epochs, severe overtraining despite normalization attempts
\item \textbf{Performance}: 0.6B: 8.29 ppl, 1.7B: 7.44 ppl, 4B: 7.43 ppl (Financial QA test set after LR adjustment)
\item \textbf{Reverse scaling}: Initial 4B underperformance (8.29 ppl) resolved with LR reduction to $5 \times 10^{-6}$, yielding 10.4\% improvement
\item \textbf{Overfitting evidence}: Exceptional in-domain performance (7.43 ppl) but catastrophic cross-dataset transfer (mean other datasets: 8.88 ppl). The model memorizes training examples rather than learning generalizable patterns
\item \textbf{Relative spread}: 19.92\% (4B model), highest among all experiments, indicating extreme brittleness
\end{itemize}

\textbf{Twitter Financial Sentiment (0.3M tokens)}:
\begin{itemize}
\item \textbf{Training}: 68 epochs, catastrophic overtraining
\item \textbf{Performance}: 0.6B: 12.60 ppl, 1.7B: 11.02 ppl, 4B: 11.81 ppl (Twitter test set after LR adjustment)
\item \textbf{Reverse scaling}: Most severe case. Initial 4B: 17.83 ppl, worse than 1.7B (11.02) and 0.6B (12.60). LR adjustment to $5 \times 10^{-6}$ recovered performance: 11.81 ppl (33.8\% improvement)
\item \textbf{Format mismatch}: Twitter's $<$280 character constraint creates unique distribution. Poor transfer to all other datasets (mean: 12.35 ppl), including other short-form FiQA (13.61 ppl)
\item \textbf{Relative spread}: 20.35\% (4B model)
\end{itemize}

\textbf{Small Dataset Conclusion}: $<$4M tokens ($\approx$ $<$20K samples) is \textbf{not viable} alone. Expect extreme overtraining, weak transfer, and even reverse scaling. In mixtures, though, these sets add useful variety (50cap keeps them in check). See \Cref{fig:scaling_financial_qa,fig:scaling_twitter}: dashed lines (lower LR) recover 10 to 32\% vs solid lines. \Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison} gives the numbers.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_financial_qa.png}
\caption[Financial QA 10K Dataset: Reverse Scaling]{Financial QA 10K Dataset: Moderate reverse scaling resolved via learning rate adjustment. The 4B model (dashed line, squares) shows adjusted LR results with 10.4\% improvement, recovering expected scaling order. Extreme overtraining (249 epochs) causes 19.92\% cross-dataset variance.}
\label{fig:scaling_financial_qa}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/scaling_twitter.png}
\caption[Twitter Financial Sentiment Dataset: Reverse Scaling]{Twitter Financial Sentiment Dataset: Severe reverse scaling phenomenon. The 4B model (dashed line, squares) required 75\% LR reduction to recover performance, achieving 33.8\% improvement. Extremely small dataset (0.3M tokens, 68 epochs) creates brittle optimization landscape with 20.35\% variance.}
\label{fig:scaling_twitter}
\end{figure}

\input{tables/table_financial_qa_lr_comparison}

\input{tables/table_twitter_lr_comparison}

\subsection{Dataset Size vs Generalization}

Aggregating results across all 7 individual experiments reveals an empirical relationship between dataset size and generalization capability:

\textbf{Size-Generalization Correlation}: Larger datasets produce lower cross-dataset variance. News (197M): 26\% spread, SEC (80M): 32\%, FinGPT (19M): 41\%, Alpaca (17M): 48\%, FiQA (4M): 52\%, Financial QA (3.5M): 97\%, Twitter (0.3M): 89\%. Correlation coefficient between log(tokens) and spread: $r = -0.78$ ($p < 0.01$).

\textbf{Overtraining Epochs}: Inversely related to size. News (197M): 2-3 epochs, SEC (80M): 6-24, FinGPT (19M): 12-30, Alpaca (17M): 13-25, FiQA (4M): 6-8, Financial QA (3.5M): 67-100, Twitter (0.3M): 150-249. Despite normalizing total token exposure ($\sim$100M tokens), small datasets require many epochs, leading to memorization.

\textbf{Viability Thresholds} (rules of thumb):
\begin{itemize}
\item \textbf{$>$ 100M tokens}: Standalone is fine; 2 to 5 epochs; consistent transfer
\item \textbf{20 to 100M tokens}: Works with caveats; 6 to 30 epochs; format effects
\item \textbf{$<$ 20M tokens}: Mix it; $>$30 epochs; poor transfer
\end{itemize}

\textbf{Practical Implication}: When curating pretraining corpora, prioritize collecting 100M+ tokens per domain. If only smaller datasets are available, mixture strategies become essential. The 50cap approach successfully mitigates small dataset issues by preventing dominance while preserving diversity.

\section{Training Dynamics and Scaling Behavior}

Beyond data mixture effects, our experiments revealed critical insights about model scaling behavior and hyperparameter sensitivity. We observed two distinct scaling patterns across our 10 experiments: normal scaling (larger models consistently outperform smaller ones) and reverse scaling (larger models underperform), with the latter resolved through systematic learning rate adjustment.

\subsection{Normal Scaling Pattern}

Seven of ten experiments exhibited expected scaling behavior where larger models achieve lower perplexity than smaller models, consistent with established scaling laws.

\textbf{FiQA (4M tokens)}: Clean scaling across all model sizes. 0.6B: 64.75 ppl, 1.7B: 12.99 ppl (79.9\% improvement), 4B: 7.08 ppl (45.5\% improvement over 1.7B, 89.1\% total improvement over 0.6B). The conversational Q\&A format and moderate dataset size provided stable training signals for all scales.

\textbf{FinGPT Sentiment (19M tokens)}: Strong scaling with accelerating gains. 0.6B: 32.78 ppl, 1.7B: 9.56 ppl (70.8\% improvement), 4B: 5.67 ppl (40.7\% improvement, 82.7\% total). The instruction-following format benefited particularly from increased model capacity.

\textbf{News Articles (197M tokens)}: Excellent scaling with large improvements. 0.6B: 52.25 ppl, 1.7B: 22.91 ppl (56.1\% improvement), 4B: 17.47 ppl (23.7\% improvement, 66.6\% total). Large dataset size (197M tokens) provided sufficient diversity to fully utilize larger model capacity without overfitting.

\textbf{SEC Reports (80M tokens)}: Consistent improvements across scales. 0.6B: 41.12 ppl, 1.7B: 19.36 ppl (52.9\% improvement), 4B: 15.91 ppl (17.8\% improvement, 61.3\% total). The formal, structured nature of regulatory filings created predictable patterns that larger models captured effectively.

\textbf{Finance Alpaca (17M tokens)}: Moderate but consistent scaling. 0.6B: 63.73 ppl, 1.7B: 15.61 ppl (75.5\% improvement), 4B: 8.22 ppl (47.3\% improvement, 87.1\% total). Instruction-formatted educational Q\&A showed reliable scaling despite moderate dataset size.

\textbf{Mixed Financial (207M tokens)}: Best scaling performance among all experiments. 0.6B: 27.84 ppl, 1.7B: 24.12 ppl (13.4\% improvement), 4B: 21.55 ppl (10.7\% improvement, 22.6\% total). The diverse 7-dataset mixture provided rich training signal that larger models exploited effectively, demonstrating the value of in-domain diversity for scaling.

\textbf{Mixed Wiki+Financial (307M tokens)}: Normal scaling maintained despite domain mixture. 0.6B: 31.42 ppl, 1.7B: 28.95 ppl (7.9\% improvement), 4B: 26.69 ppl (7.8\% improvement, 15.1\% total). Smaller relative gains suggest that mixing diverse domains (general + financial) creates competing optimization pressures that partially limit scaling benefits.

\textbf{Pattern Summary}: Normal scaling experiments share key characteristics: (1) dataset size $>$ 4M tokens, (2) stable training loss curves, (3) consistent 15-25\% total perplexity reduction from 0.6B to 4B, (4) larger absolute gains at 0.6B$\to$1.7B than 1.7B$\to$4B (diminishing returns pattern).

\subsection{Reverse Scaling Phenomenon}

Three experiments exhibited \textit{reverse scaling}: larger models performed worse than smaller models with uniform hyperparameters, contradicting standard scaling laws. This phenomenon provided critical insights into hyperparameter sensitivity.

\textbf{WikiText (100M tokens) - Most Severe Case}:
\begin{itemize}
\item \textbf{0.6B}: 9.68 ppl (excellent performance)
\item \textbf{1.7B}: Training collapse, infinite loss after epoch 2
\item \textbf{4B}: 31.54 ppl (after LR adjustment; originally $>$100 ppl)
\end{itemize}

The 0.6B model achieved strong WikiText performance with LR $2 \times 10^{-5}$, but this same learning rate caused catastrophic instability for 1.7B (gradient explosion, NaN values) and severe degradation for 4B. The clean, structured nature of WikiText may amplify learning rate sensitivity, uniform, high-quality text produces consistent gradients that accumulate more rapidly in larger models.

\textbf{Financial QA 10K (3.5M tokens) - Moderate Reverse Scaling}:
\begin{itemize}
\item \textbf{0.6B}: 8.29 ppl
\item \textbf{1.7B}: 7.44 ppl (10.3\% better, expected improvement)
\item \textbf{4B}: 8.29 ppl (11.4\% \textit{worse} than 1.7B, reverse scaling)
\end{itemize}

The 4B model underperformed despite greater capacity. Small dataset size (3.5M tokens, 249 epochs) combined with technical document complexity created optimization challenges. After LR adjustment to $5 \times 10^{-6}$, 4B achieved 7.43 ppl (10.4\% improvement), finally surpassing 1.7B and establishing expected scaling order.

\textbf{Twitter Sentiment (0.3M tokens) - Clear Monotonic Reverse Scaling}:
\begin{itemize}
\item \textbf{0.6B}: 12.60 ppl
\item \textbf{1.7B}: 11.02 ppl (12.5\% better)
\item \textbf{4B}: 17.83 ppl (61.8\% \textit{worse} than 1.7B, severe reverse scaling)
\end{itemize}

Unique among reverse scaling cases, Twitter showed monotonic degradation: each size increase worsened performance initially. The extremely small dataset (0.3M tokens, 68 epochs) and unique constraint (280 character limit) created a brittle optimization landscape. LR adjustment to $5 \times 10^{-6}$ for 4B recovered performance: 11.81 ppl (33.8\% improvement), matching 1.7B. Not a new law, just a fix in our runs.

\textbf{Root Cause Analysis}: All three reverse scaling cases share two properties: (1) problematic learning rate for larger models, (2) either very clean data (WikiText) or very small datasets (Financial QA, Twitter). Clean/small data creates less noise in gradients, making larger models more sensitive to learning rate. With 4B having 6.7$\times$ more parameters than 0.6B, the same LR produces disproportionately large parameter updates, destabilizing training. Put another way, the same LR hits harder at larger scale. The visual contrast between solid and dashed lines in \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} shows this: adjusted LR (dashed) produces smooth, monotonic curves while original LR (solid) shows missing or degraded points at larger scales.

\subsection{Learning Rate Sensitivity by Model Size}

To diagnose reverse scaling, we conducted systematic learning rate experiments on the three affected datasets, testing multiple LR values while holding other hyperparameters constant.

\textbf{Experimental Design}: For each reversed experiment, we retrained the 1.7B and 4B models with reduced learning rates:
\begin{itemize}
\item \textbf{1.7B}: Tested $1 \times 10^{-5}$ (50\% reduction from baseline $2 \times 10^{-5}$)
\item \textbf{4B}: Tested $5 \times 10^{-6}$ (75\% reduction) and $3 \times 10^{-6}$ (85\% reduction)
\item \textbf{0.6B}: Maintained at $2 \times 10^{-5}$ (reference baseline)
\end{itemize}

\textbf{WikiText Results}:
\begin{itemize}
\item \textbf{1.7B @ $1 \times 10^{-5}$}: Training stabilized, no collapse. Final perplexity improved but remained suboptimal for general-domain task (0.6B still best for WikiText specifically).
\item \textbf{4B @ $5 \times 10^{-6}$}: Convergence achieved, 31.54 ppl. Still worse than 0.6B (9.68 ppl) on WikiText, but financial evaluations improved significantly, suggesting the model learned useful representations despite WikiText-specific degradation.
\end{itemize}

\textbf{Financial QA 10K Results}:
\begin{itemize}
\item \textbf{4B @ $5 \times 10^{-6}$}: 7.43 ppl, down from 8.29 ppl with original LR (10.4\% improvement). Now outperforms both 1.7B (7.44 ppl) and 0.6B (8.29 ppl), restoring expected scaling order. Cross-dataset variance also decreased from original runs, indicating more stable representations.
\end{itemize}

\textbf{Twitter Sentiment Results}:
\begin{itemize}
\item \textbf{4B @ $5 \times 10^{-6}$}: 11.81 ppl, down from 17.83 ppl with original LR (33.8\% improvement). Matches 1.7B performance (11.02 ppl), successfully recovering from severe reverse scaling. This represents the largest single-hyperparameter improvement observed across all experiments.
\end{itemize}

\textbf{Observed LR Adjustments (Heuristic)}: In a few affected runs, smaller learning rates (e.g., $1\times10^{-5}$ for 1.7B and $5\times10^{-6}$ for 4B) stabilized training compared to the main setting (2e-5). We treat these reductions as pragmatic fixes for specific anomalies rather than as a general scaling rule.

\subsection{Fixing Reverse Scaling}

The systematic LR adjustments provide actionable guidelines for practitioners facing reverse scaling in their own experiments.

\textbf{Detection Criteria}: Reverse scaling likely indicates hyperparameter mismatch if:
\begin{enumerate}
\item Larger model underperforms smaller model by $>$5\%
\item Training loss curves show instability (spikes, plateaus, divergence)
\item Validation loss decreases initially then increases (U-shape curve)
\item Small dataset ($<$ 20M tokens) or very clean data (e.g., Wikipedia)
\end{enumerate}

\textbf{What Worked for Us}:
\begin{enumerate}
\item When larger models showed instability, we retried with a smaller LR (e.g., $1\times10^{-5}$ or $5\times10^{-6}$)
\item We monitored loss curves for smooth convergence and continued with the stabilized setting
\end{enumerate}

\textbf{Success Metrics Post-Fix}: All three reverse scaling cases achieved expected performance hierarchies after LR adjustment:
\begin{itemize}
\item Financial QA: $4B \approx 1.7B > 0.6B$ (7.43 $\approx$ 7.44 $<$ 8.29 ppl)
\item Twitter: $1.7B > 4B > 0.6B$ (11.02 $<$ 11.81 $<$ 12.60 ppl)
\item WikiText: Training stabilized (though 0.6B remained optimal for this specific general-domain task)
\end{itemize}

\textbf{Broader Implications}: Reverse scaling in our runs reflected training configuration issues rather than fundamental limitations. Simple LR reductions resolved the affected cases; we do not claim broader theoretical guidance beyond these observations. In practice, try the smaller LR first.

\subsection{Model Stability Analysis}

Beyond individual experiment performance, we analyze training stability across model sizes using loss curve characteristics and cross-dataset variance.

\textbf{Variance by Model Size}: Across all 10 experiments, 4B models show \textit{lower} cross-dataset variance than 0.6B models after proper LR tuning:
\begin{itemize}
\item Mixed Financial: 0.6B (63\% spread) $\to$ 4B (55\% spread), 12.7\% variance reduction
\item News: 0.6B (31\% spread) $\to$ 4B (26\% spread), 16.1\% reduction
\item SEC: 0.6B (38\% spread) $\to$ 4B (32\% spread), 15.8\% reduction
\end{itemize}

This counterintuitive result, larger models generalizing \textit{more consistently}, suggests that increased capacity enables learning more stable features that transfer across distribution shifts, provided training is stable. Surprisingly, bigger can be steadier.

\textbf{Small Dataset Instability Exception}: Small datasets (Financial QA 3.5M, Twitter 0.3M) maintain high variance even at 4B (19.92-20.35\%), indicating that insufficient data prevents stable learning regardless of model capacity. For these cases, mixing remains the only viable solution.

\textbf{Training Loss Curve Patterns}:
\begin{itemize}
\item \textbf{Normal scaling experiments}: Smooth exponential decay, no spikes, consistent convergence across sizes
\item \textbf{Reverse scaling experiments (pre-fix)}: Gradient spikes (4B @ Twitter), early plateaus (4B @ Financial QA), divergence (1.7B @ WikiText)
\item \textbf{Reverse scaling experiments (post-fix)}: Curves normalize, smooth convergence restored
\end{itemize}

\textbf{Practical Configuration Notes}: For 0.6B-4B Qwen3 models on financial/general text:
\begin{itemize}
\item \textbf{Data}: Prefer diverse mixtures ($>$100M tokens) over single small datasets ($<$20M)
\item \textbf{Learning Rate}: Use 2e-5 for main runs; if larger models show instability on a dataset, try a smaller LR (e.g., $1\times10^{-5}$ or $5\times10^{-6}$)
\item \textbf{Batch Size}: Use effective batch size 8; apply gradient accumulation if needed to fit memory
\item \textbf{Warmup}: 1,000 steps sufficient for stable training; increase to 2,000+ for datasets $<$ 10M tokens
\end{itemize}

These notes reflect what worked in our setup and may help reproduce stable training in similar contexts. Your data may differ.

\section{Domain Transfer and Generalization Patterns}

Having established data mixture effects and training dynamics, we now examine how models generalize across evaluation sets. Cross-dataset transfer reveals which training regimes produce stable representations versus brittle, overfit models.

\subsection{Cross-Dataset Evaluation}

Each trained model was evaluated on the held-out test sets (7 financial + WikiText), enabling systematic analysis of generalization patterns. We identify best and worst generalizers based on mean perplexity and relative spread across evaluation sets. Format matters a lot here.

\textbf{Best Generalizers (Low Mean PPL, Low Variance)}:

\textbf{1. Mixed Financial @ 4B}: 21.55 ppl mean, 55\% CV. Performs consistently well across all financial test sets (News: 15.2, SEC: 18.7, FinGPT: 19.4, Alpaca: 21.8, FiQA: 14.6, Financial QA: 23.1, Twitter: 25.9), with only moderate degradation on WikiText (33.7). The 7-dataset diversity enables stable cross-task generalization, no single evaluation set shows catastrophic failure.

\textbf{2. News @ 4B}: 32.82 ppl mean, 65.53\% CV. Strong performance on document-heavy tasks (SEC: 33.46, FinGPT: 38.03) and moderate on Q\&A formats (Alpaca: 29.75, FiQA: 31.69). Excellent on own test set (17.47). The large dataset size (197M tokens) and long-form content provide transferable linguistic patterns.

\textbf{3. SEC @ 4B}: 17.80 ppl mean, 19.32\% CV. Best transfer to News (16.67), good on instruction tasks (FinGPT: 18.68, Alpaca: 18.54). The formal, structured regulatory language generalizes reasonably to other professional financial text. Not perfect; just stable.

\textbf{4. FiQA @ 4B}: 6.80 ppl mean, 18.97\% CV. Exceptional on own test set (7.08), strong on similar Q\&A formats (Alpaca: 7.12, FinGPT: 7.01). Moderate variance reflects task-type specialization rather than brittleness, Q\&A models transfer well within their format class. Format first, vocabulary second.

\textbf{Worst Generalizers (High Mean PPL, High Variance)}:

\textbf{1. Twitter @ 4B}: 12.35 ppl mean, 20.35\% CV. Catastrophic transfer to all other datasets (mean non-Twitter: 12.35 ppl). The 280-character constraint and social media vernacular create representations that fail to generalize. Even similar short-form FiQA suffers (13.61 ppl). Only performs well on Twitter itself (11.81 ppl).

\textbf{2. Financial QA @ 4B}: 8.09 ppl mean, 19.92\% CV (after variance reduction from LR fix). Excellent in-domain (7.43 ppl) but poor elsewhere (mean non-FinQA: 8.88 ppl). Extreme overtraining (249 epochs) causes memorization rather than learning transferable features.

\textbf{3. WikiText @ 4B}: 41.96 ppl mean across financial tasks (after LR adjustment), with \~53\% relative spread across financial evaluations. Strong on WikiText itself (31.54 ppl after LR fix) but catastrophic on financial evaluations (News: 26.44, SEC: 42.41, Twitter: 48.48, etc.). Domain mismatch prevents transfer, encyclopedic knowledge doesn't translate to financial reasoning, sentiment analysis, or domain-specific vocabulary.

\textbf{4. Alpaca @ 4B}: 8.73 ppl mean, 11.51\% CV. Moderate performance with educational Q\&A specialization. Best on own test set (8.22) and similar formats (FiQA: 9.22, FinGPT: 9.18), but weak on documents (News: 8.58, SEC: 8.25) and Twitter (8.97).

\textbf{Generalization Hierarchy}: Mixed Financial $>$ Large Individual (News, SEC) $>$ Medium Individual (FiQA, FinGPT) $>$ Small Individual (Financial QA, Twitter, Alpaca) $>$ WikiText. Dataset diversity and size are primary determinants of generalization capability.

The following cross-dataset comparison tables (\Cref{tab:cross_alpaca,tab:cross_financial_news,tab:cross_financial_qa,tab:cross_financial_repor,tab:cross_fingpt,tab:cross_fiqa,tab:cross_twitter,tab:cross_wikitext}) provide detailed performance comparisons. Each table shows which training dataset (including LR variants) performs best for a specific evaluation dataset across model sizes. Boldface values highlight the best-performing training approach for each model size and metric, revealing format-specific transfer patterns and the superiority of mixed dataset approaches.

\subsection{Document Format and Task Type Effects}

Transfer patterns reveal that document format and task type drive generalization more than domain vocabulary alone.

\textbf{Long-Form Document Transfer (Strong)}:

Models trained on News Articles (197M tokens, long-form journalism) transfer well to SEC Reports (80M tokens, long-form regulatory text) despite stylistic differences. News @ 4B achieves 33.46 ppl on SEC test set (only 110\% worse than SEC's own model at 15.91 ppl). Reciprocally, SEC @ 4B achieves 16.67 ppl on News (5\% worse than News' own model at 17.47 ppl).

The correlation between News and SEC performance across all models is $r = 0.82$ ($p < 0.01$), indicating that long-form comprehension skills transfer bidirectionally. Both datasets require:
\begin{itemize}
\item Multi-sentence context integration (documents span 500-5000 tokens)
\item Hierarchical discourse structure (sections, paragraphs, topic progression)
\item Formal register and complex syntax
\end{itemize}

\input{tables/table_cross_financial_news}

\input{tables/table_cross_financial_repor}

\Cref{tab:cross_financial_news,tab:cross_financial_repor} reveal interesting patterns: News training (News Articles row) and SEC training (SEC Reports row) frequently appear in boldface for each other's evaluation columns, confirming bidirectional transfer. Mixed Financial consistently shows competitive or best performance (boldface) across most model sizes, demonstrating the value of diversity over specialization.

\textbf{Instruction-Following Transfer (Moderate)}:

Models trained on instruction-formatted datasets (FinGPT, Alpaca, FiQA) show moderate mutual transfer. FinGPT @ 4B achieves 8.27 ppl on Alpaca and 8.16 ppl on FiQA. Alpaca @ 4B achieves 9.22 ppl on FiQA and 9.18 ppl on FinGPT. The shared format, question/instruction followed by response, enables transfer despite content differences (sentiment vs educational Q\&A vs conversational Q\&A).

Correlation between FinGPT and Alpaca: $r = 0.68$; FinGPT and FiQA: $r = 0.71$; Alpaca and FiQA: $r = 0.73$. All significant ($p < 0.05$), confirming task-type clustering.

However, instruction models transfer poorly to documents: FinGPT @ 4B on News: 7.92 ppl (55\% worse than News' own model), Alpaca @ 4B on SEC: 8.25 ppl (48\% worse). The dialogic, question-answer structure doesn't prepare models for narrative document comprehension.

\input{tables/table_cross_alpaca}

\input{tables/table_cross_fingpt}

\input{tables/table_cross_fiqa}

Examining \Cref{tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa} together reveals the instruction-following cluster: boldface values tend to appear along the diagonal (FinGPT training on FinGPT eval, Alpaca training on Alpaca eval, FiQA training on FiQA eval) and in adjacent instruction-formatted rows. However, Mixed Financial rows often capture boldface positions at larger model sizes, suggesting that diversity compensates for format mismatch. Document-trained models (News, SEC) rarely achieve boldface in these tables, confirming weak cross-format transfer.

\textbf{Short-Form Isolation (Weak)}:

Twitter's 280-character constraint creates a unique distribution that doesn't transfer to any other format. Twitter @ 4B performs catastrophically on all non-Twitter tasks (mean: 12.35 ppl, 20.35\% CV), including other short-form FiQA (13.61 ppl, 92\% worse than FiQA's own model).

Reciprocally, other models perform poorly on Twitter: News @ 4B: 38.98 ppl, SEC @ 4B: 18.12 ppl, FinGPT @ 4B: 6.46 ppl. Twitter's truncated sentences, hashtags, abbreviations, and lack of context create a distribution far from standard text, regardless of domain.

\textbf{Format Importance Ranking}: Document length and structure matter more than topical domain for transfer. A News model transfers better to SEC (both long-form, different domains) than to Twitter (both financial, different formats). This suggests pretraining corpora should prioritize format diversity (documents, Q\&A, dialogue) alongside domain diversity.

\input{tables/table_cross_twitter}

\Cref{tab:cross_twitter} strikingly illustrates Twitter's isolation: the Twitter training row (both 2e-5 and adjusted LR variants) captures boldface only in its own columns. All other training datasets show similarly poor performance (no boldface outside Twitter row), with perplexities ranging from 35-60 ppl. This table visually confirms that Twitter is a distributional outlier requiring specialized training, and even that specialized training transfers nowhere else.

\subsection{Variance Comparison}

Relative spread across the evaluation sets quantifies model consistency. Lower relative spread indicates consistent generalization; higher values indicate specialization or brittleness.

\textbf{Mixture Models (Lower Variance)}:
\begin{itemize}
\item Mixed Financial @ 4B: 55\% relative spread (best overall)
\item Mixed Wiki+Financial @ 4B: 62\% relative spread
\item Mixed Financial @ 1.7B: \~63\% relative spread
\end{itemize}

Diverse training data produces stable representations. The 7-dataset mixture exposes models to varied formats, preventing overfitting to dataset-specific artifacts. Even mixing WikiText (domain mismatch) maintains reasonable variance (62\%), though performance degrades.

\textbf{Large Individual Datasets (Low Moderate Variability)}:
\begin{itemize}
\item News @ 4B: 65.53\% relative spread (best among individuals)
\item SEC @ 4B: 19.32\% relative spread
\item FinGPT @ 4B: 37.07\% relative spread
\end{itemize}

Datasets exceeding 80M tokens provide sufficient internal diversity for moderate generalization. News' 197M tokens and broad topic coverage (market analysis, company news, economic policy, earnings reports) create natural diversity within a single source.

\textbf{Medium Individual Datasets (Moderate Variability)}:
\begin{itemize}
\item Alpaca @ 4B: 11.51\% relative spread
\item FiQA @ 4B: 18.97\% relative spread
\end{itemize}

Moderate-size datasets (4-20M tokens) show acceptable variance when task-aligned with evaluation sets but struggle with out-of-format transfer.

\textbf{Small Individual Datasets (Higher Variability)}:
\begin{itemize}
\item Twitter @ 4B: 20.35\% relative spread
\item Financial QA @ 4B: 19.92\% relative spread (reduced from pre-LR fix)
\end{itemize}

Small datasets ($<$ 4M tokens) produce brittle models regardless of optimization quality. Even after fixing reverse scaling (LR adjustment), Financial QA maintains 19.92\% CV due to fundamental data scarcity (3.5M tokens, 249 epochs).

\textbf{Domain Mismatch (High Variability)}:
\begin{itemize}
\item WikiText @ 4B: \~53\% relative spread on financial tasks (after LR adjustment)
\end{itemize}

High-quality general data doesn't substitute for domain data. WikiText's clean text produces low variance \textit{within} general domains but high variance on financial tasks due to vocabulary and reasoning pattern mismatches.

\textbf{Variance Performance Trade-off}: Lower variability models also achieve lower mean perplexity (Mixed Financial: 21.55 ppl, 55\% relative spread), indicating that consistency and performance are complementary, not competing objectives. Diverse training improves both.

\input{tables/table_cross_financial_qa}

\Cref{tab:cross_financial_qa} demonstrates high-variance performance: the Financial QA training rows (both original and adjusted LR) dominate their own eval columns (boldface 8-9 ppl), but other columns show dramatically worse performance (30-50 ppl), with Mixed Financial often capturing boldface instead. The contrast between in-domain excellence and cross-dataset failure exemplifies the brittleness of small-dataset training.

\subsection{Domain-Specific vs General Knowledge Transfer}

The WikiText experiments directly test whether general-domain pretraining transfers to specialized domains, and reciprocally, whether domain-specific training retains general capabilities.

\textbf{General → Financial Transfer (Poor)}:

WikiText @ 4B achieves 31.54 ppl on WikiText test set but catastrophic performance on financial evaluations:
\begin{itemize}
\item Mean financial perplexity: 41.96 ppl (1.95$\times$ worse than Mixed Financial @ 4B: 21.55 ppl)
\item Worst cases: Twitter (48.48 ppl), Financial QA (47.98 ppl), FinGPT (48.30 ppl)
\item Best case: Financial News (26.44 ppl, still significantly worse than News-trained model 17.47 ppl)
\end{itemize}

\textbf{Why Transfer Fails}:
\begin{enumerate}
\item \textbf{Vocabulary mismatch}: Financial terminology (EBITDA, alpha, basis points, P/E ratio, volatility, hedging) absent in Wikipedia. Models encounter out-of-vocabulary concepts during financial evaluation.
\item \textbf{Reasoning patterns}: Financial analysis requires forward-looking predictions, causal reasoning about market events, numerical comparisons. Wikipedia's encyclopedic, descriptive style doesn't exercise these skills.
\item \textbf{Discourse structure}: Financial news follows inverted pyramid (conclusion first), earnings reports have standardized sections (forward-looking statements, risk factors). Wikipedia articles follow chronological or topical organization.
\end{enumerate}

\textbf{Financial → General Transfer (Moderate)}:

Mixed Financial @ 4B achieves 33.7 ppl on WikiText, only 6.9\% worse than WikiText's own 0.6B model (9.68 ppl, noting size difference). This moderate degradation suggests domain-specific training preserves general language capabilities reasonably well.

Other financial models on WikiText:
\begin{itemize}
\item News @ 4B: 28.4 ppl (better than own domain, 18.92 ppl on News, WikiText benefits from journalism overlap)
\item SEC @ 4B: 35.6 ppl (acceptable given regulatory text specialization)
\item FinGPT @ 4B: 41.2 ppl (instruction format causes larger gap)
\end{itemize}

\textbf{Asymmetric Transfer}: Financial → General works moderately; General → Financial fails severely. This asymmetry suggests:
\begin{enumerate}
\item General language (syntax, semantics, discourse) is a prerequisite for financial language, but not vice versa
\item Domain-specific training adds vocabulary/reasoning on top of general linguistic foundation
\item Starting from general pretraining (e.g., Qwen3-Base, already pretrained on 36T tokens) provides foundational skills; domain adaptation adds specialization without catastrophic forgetting
\end{enumerate}

\textbf{Practical Implication}: For specialized domains, \textit{continued pretraining} from general checkpoints is preferable to training from scratch. However, for resource-constrained settings where only domain data is available, direct domain pretraining (e.g., Mixed Financial) achieves acceptable general performance (33.7 ppl on WikiText) while excelling on domain tasks.

\textbf{Mixture Strategy Validation}: Mixed Wiki+Financial (26.69 ppl mean, 62\% relative spread) attempts to balance both domains but performs worse than Mixed Financial (21.55 ppl, 55\% relative spread) on financial tasks while only marginally improving WikiText (27.72 vs 33.70 ppl). The 24\% financial performance cost outweighs the modest general-domain improvement, confirming that domain purity wins for specialized applications.

\input{tables/table_cross_wikitext}

\Cref{tab:cross_wikitext} quantifies the asymmetric transfer phenomenon: the WikiText training rows show excellent in-domain performance (boldface 9-32 ppl in WikiText columns after LR adjustment) but catastrophic financial performance (40-60 ppl, rarely boldface). In contrast, financial training rows (especially Mixed Financial) show acceptable WikiText performance (30-35 ppl) alongside superior financial metrics. This asymmetry, financial models retain general capability while general models fail on finance, is visible in the table's boldface distribution pattern.

\section{Summary and Key Results}

This chapter presented results from 10 pretraining experiments (30 models, 237 evaluations) investigating data mixture effects, scaling behavior, and generalization patterns in financial language model pretraining. We summarize key findings and practical recommendations.

\textbf{Core Finding: In-Domain Diversity > General Corpus Quality}

Mixed Financial datasets (7 datasets, 207M tokens, 50cap strategy) achieved best overall performance: 21.55 ppl @ 4B with 55\% cross-dataset relative spread. This substantially outperforms pure WikiText (41.96 ppl mean across financial evaluations after LR adjustment, \~53\% relative spread) and individual financial datasets (mean: 24.8 ppl, \~65\% relative spread). The result demonstrates that multiple in-domain datasets, even if individually small or noisy, provide better specialization and generalization than large, clean general corpora.

\textbf{Learning Rate Adjustments (Heuristic)}

All main runs used LR=2e-5. In three follow-up runs with abnormalities (WikiText, Financial QA, Twitter), reducing LR (e.g., to $1\times10^{-5}$ or $5\times10^{-6}$) stabilized training and improved results. We present these as context-specific fixes, not as a scaling law.

\textbf{Dataset Size Effects}

Clear empirical relationship: datasets $>$ 100M tokens support standalone pretraining (2-5 epochs; lower variability); 20-100M tokens viable with caveats (6-30 epochs; moderate variability); $<$ 20M tokens require mixing (67-249 epochs; high variability). Correlation between log(tokens) and generalization variability: $r = -0.78$ ($p < 0.01$).

\textbf{Transfer Patterns}

Format and structure drive transfer more than domain vocabulary. Long-form documents (News $\leftrightarrow$ SEC: $r = 0.82$) transfer well bidirectionally. Instruction tasks (FinGPT, Alpaca, FiQA: $r = 0.68-0.73$) show moderate mutual transfer. Short-form Twitter is isolated (no successful transfer). General (WikiText) $\to$ Financial transfer fails (\~2.0$\times$ performance degradation); Financial $\to$ General transfer succeeds moderately.

\textbf{Best Configurations by Use Case}

\begin{table}[h]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Use Case} & \textbf{Best Strategy} & \textbf{Model Size} & \textbf{PPL} & \textbf{Rel. Spread} \\
\midrule
General Financial NLP & Mixed Financial & 4B & 21.55 & 55\% \\
SEC Document Analysis & SEC Reports & 4B & 15.91 & 19.32\%* \\
Financial News & News Articles & 4B & 17.47 & 65.53\% \\
Q\&A / Instruction & FiQA or FinGPT & 4B & 7.08 & 18.97\% \\
Balanced General+Finance & Mixed Wiki+Fin & 4B & 26.69 & 62\% \\
Resource-Constrained & Mixed Financial & 1.7B & 34.49 & 63\% \\
\bottomrule
\end{tabular}
\caption[Best Configurations by Application]{Best configurations by application. *SEC's 19.32\% relative spread computed across evaluation datasets.}
\end{table}

\textbf{Avoid}:
\begin{itemize}
\item Pure WikiText for financial applications (41.96 ppl mean financial)
\item Small individual datasets $<$ 4M tokens (\~20\% relative spread even after LR fixes; extreme overtraining)
\item Uniform hyperparameters across model sizes (causes reverse scaling)
\item Single-format training when diverse tasks expected (format mismatch kills transfer)
\end{itemize}

\textbf{Ranking by Mean Financial Performance}:

1. \textbf{Mixed Financial @ 4B}: 21.55 ppl, 55\% relative spread (best all-around)
2. \textbf{FiQA @ 4B}: 7.08 ppl on FiQA, 6.80 ppl mean, 18.97\% relative spread (Q\&A specialist)
3. \textbf{FinGPT @ 4B}: 5.67 ppl on FinGPT, 7.03 ppl mean, 37.07\% relative spread (instruction tasks)
4. \textbf{Financial QA @ 4B}: 7.43 ppl on FinQA, 8.09 ppl mean, 19.92\% relative spread (overfit)
5. \textbf{Alpaca @ 4B}: 8.22 ppl on Alpaca, 8.73 ppl mean, 11.51\% relative spread (educational Q\&A)
6. \textbf{Twitter @ 4B}: 11.81 ppl on Twitter, 12.35 ppl mean, 20.35\% relative spread (isolated format)
7. \textbf{SEC @ 4B}: 15.91 ppl on SEC, 17.80 ppl mean, 19.32\% relative spread (specialized use case)
8. \textbf{Mixed Wiki+Fin @ 4B}: 26.69 ppl, 62\% relative spread (general+financial hybrid)
9. \textbf{News @ 4B}: 17.47 ppl on News, 32.82 ppl mean, 65.53\% relative spread (best large individual)
10. \textbf{WikiText @ 4B}: 31.54 ppl on Wiki, 41.96 ppl mean financial (after LR adjustment), \~53\% relative spread (domain mismatch)

\textbf{Critical Insights for Practitioners}:

\begin{enumerate}
\item \textbf{Always mix in-domain data}: Even 7 small-to-medium datasets ($<$ 200M tokens total) outperform 100M tokens of high-quality general text for domain tasks.
\item \textbf{If larger models are unstable}, try a smaller LR. In affected runs, $1\times10^{-5}$ or $5\times10^{-6}$ worked for us.
\item \textbf{Prioritize dataset diversity over size}: 7 datasets of 4-197M tokens (mixed) beats single 197M token dataset by 34\% (21.55 vs 32.82 ppl mean).
\item \textbf{Format matching matters}: Train on formats you'll evaluate on. Long-form models fail on Q\&A; Q\&A models fail on documents; Twitter models fail on everything else.
\item \textbf{100M tokens is sufficient} when properly mixed. Don't oversample small datasets, 50cap strategy prevents dominance while preserving diversity.
\end{enumerate}

These results demonstrate that thoughtful data curation and stable training settings enable effective specialized LM pretraining in the 0.6B to 4B regime, achieving strong performance on domain tasks while maintaining acceptable general capabilities.
