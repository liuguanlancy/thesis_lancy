\chapter{Conclusion}

This thesis investigated efficient pretraining strategies for financial language models, addressing the critical challenge of developing lightweight, privacy-preserving models suitable for on-device deployment. Through systematic experimentation with 10 pretraining configurations across three model scales (0.6B, 1.7B, 4B parameters), we established empirical guidelines for data mixture composition, hyperparameter scaling, and resource allocation that enable practitioners to train effective specialized models without access to massive computational resources.

\section{Summary of Contributions}

This work contributes to the intersection of domain adaptation and language model scaling through five key empirical findings and one practical deliverable:

\subsection{Data Mixture Guidelines for Financial NLP}

We demonstrated that \textbf{diverse in-domain data mixtures significantly outperform general-domain pretraining} for financial applications. Mixed Financial (7 datasets, 322M tokens total capped at 50\% per dataset) achieved 21.55 perplexity at 4B scale with 55\% coefficient of variation across financial tasks—substantially better than WikiText (48.7 ppl mean, 78\% CV) despite WikiText's larger individual size (100M tokens). Including WikiText in mixtures (Mixed Wiki+Financial) degraded financial performance by 24\% (26.69 ppl) while improving general-domain performance by only 16\%—an unfavorable trade-off for finance-focused applications. This finding is supported by comprehensive visual evidence: \Cref{fig:scaling_comparison_all} shows the widening performance gap between mixed financial and WikiText approaches across model sizes, while cross-dataset comparison tables reveal that mixed financial training rows consistently capture best-performance (boldface) positions across financial evaluation columns.

The 50cap mixture strategy proved effective in balancing large dominant datasets (Financial News 197M, SEC Reports 80M tokens) with smaller specialized sources (Twitter Financial 0.3M, Financial QA 3.5M tokens). This approach prevents dominance-driven overfitting while preserving diversity benefits—small datasets contributed meaningful format variety despite non-viability as standalone training sources.

\subsection{Learning Rate Scaling Laws for Decoder-Only Transformers}

We identified a critical empirical relationship: \textbf{learning rate must scale inversely with the square root of parameter count} to maintain training stability across model sizes. Optimal learning rates followed $\text{LR}(N) = 2 \times 10^{-5} \times \sqrt{0.6 \times 10^9 / N}$, yielding: 0.6B ($2 \times 10^{-5}$), 1.7B ($1 \times 10^{-5}$, 50\% reduction), 4B ($5 \times 10^{-6}$, 75\% reduction).

Failure to scale learning rates caused \textbf{reverse scaling}—larger models underperforming smaller ones—in 3 of 10 initial experiments (WikiText, Financial QA, Twitter Financial). After systematic LR adjustment, all experiments exhibited normal scaling, with 4B models consistently outperforming 0.6B by 18-38\% in perplexity. This finding addresses a critical gap in scaling laws literature \parencite{kaplan2020scaling, hoffmann2022training}, which assume proper hyperparameter tuning but provide limited practical guidance. The visual evidence is dramatic: \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} show dashed lines (adjusted LR) recovering 10-32\% performance over solid lines (original LR), and \Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison} document how boldface positions shift from smaller to larger models after adjustment, restoring expected scaling order.

The scaling law derives from gradient magnitude scaling in deeper networks: larger models produce proportionally larger gradient norms, requiring LR reduction to prevent optimizer instability. This relationship should generalize beyond financial domain to other decoder-only transformer architectures (LLaMA, Gemma, Mistral), though architecture-specific validation remains future work.

\subsection{Dataset Size Effects and Generalization}

We established quantitative thresholds for dataset viability: \textbf{datasets exceeding 100M tokens enable stable standalone training, while datasets below 20M tokens require mixture strategies}. Large datasets (Financial News 197M, SEC Reports 80M tokens) exhibited 26-32\% coefficient of variation, indicating robust cross-format generalization. Medium datasets (FinGPT 19M, Alpaca 17M, FiQA 4M tokens) showed 41-52\% CV—acceptable variance requiring careful hyperparameter tuning. Small datasets (Financial QA 3.5M, Twitter 0.3M tokens) exhibited extreme variance (89-97\% CV), performing well on in-distribution data but catastrophically failing on out-of-distribution formats.

The correlation between dataset size (log-transformed token count) and generalization (inverse CV) was strong ($r = -0.78$), validating intuitions about data scale but providing specific actionable thresholds. Critically, small datasets remain valuable in mixtures—their contribution to format diversity and vocabulary coverage improves overall mixture quality despite standalone non-viability. Scaling figures illustrate this distinction: \Cref{fig:scaling_news_articles,fig:scaling_sec_reports} (large datasets) show smooth curves, while \Cref{fig:scaling_financial_qa,fig:scaling_twitter} (small datasets) require LR interventions and exhibit erratic patterns. Cross-dataset tables (\Cref{tab:cross_financial_qa,tab:cross_twitter}) reveal the brittleness: these training rows achieve boldface only in their own columns (extreme specialization) while showing 30-50 ppl elsewhere (catastrophic transfer).

\subsection{Domain Transfer and Format Effects}

Contrary to common assumptions that domain vocabulary drives transfer, we found \textbf{format consistency determines generalization more than semantic domain}. Long-form financial documents (News $\leftrightarrow$ SEC) exhibited strongest transfer ($r = 0.82$ cross-perplexity correlation), while instruction-format transfers (FiQA $\leftrightarrow$ FinGPT $\leftrightarrow$ Alpaca) achieved moderate correlation ($r = 0.68-0.73$). Cross-format transfer failed: document-pretrained models achieved 2-3$\times$ worse perplexity on instruction tasks (and vice versa) despite shared financial vocabulary.

Domain transfer proved asymmetric: financial pretraining enabled reasonable general-domain performance (WikiText perplexity competitive with specialized general-domain models), but general pretraining failed catastrophically for financial tasks (WikiText pretraining: 48.7 mean financial ppl vs 21.55 for Mixed Financial). This asymmetry reflects vocabulary coverage—financial text includes substantial general vocabulary, but general corpora lack domain-specific terminology (EBITDA, prospectus, liquidity ratios).

These findings suggest \textbf{practitioners should prioritize format diversity over domain purity} when curating pretraining mixtures. A mixture spanning documents, dialogues, and Q\&A formats within the financial domain generalizes better than narrow focus on a single format, even with larger data volume. Cross-dataset comparison tables provide striking visual evidence: \Cref{tab:cross_financial_news,tab:cross_financial_repor} show boldface clustering along the News-SEC diagonal (long-form transfer), \Cref{tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa} exhibit diagonal boldface patterns plus adjacency (instruction-format clustering), while \Cref{tab:cross_twitter} shows complete isolation (boldface only in Twitter's own column).

\subsection{Model Size Selection for Resource-Constrained Settings}

We demonstrated that \textbf{1.7B models offer optimal performance-efficiency balance}, achieving 92\% of 4B's performance (24.12 vs 21.55 ppl on Mixed Financial) while requiring 50\% memory (10GB vs 20GB) and 50\% training time (12 vs 24 hours for 100M tokens on RTX 4090). This finding directly addresses the thesis motivation of developing lightweight, privacy-preserving models for on-device deployment.

0.6B models remain viable for rapid prototyping and extremely resource-constrained scenarios (mobile devices), accepting 22\% performance degradation (27.84 ppl) for 3$\times$ faster training. 4B models justify their cost only when maximizing absolute performance is critical and hyperparameter tuning expertise is available—improper learning rate selection at this scale causes training collapse, making 4B models less robust than smaller alternatives.

For the privacy-preserving financial chatbot application motivating this thesis, \textbf{1.7B represents the recommended deployment target}: small enough for laptop inference (MacBook Pro M1 with 16GB RAM), large enough for acceptable performance, and robust enough for reliable training without extensive hyperparameter search.

\subsection{Open-Source Reproducible Pipeline}

Beyond empirical findings, we delivered a production-ready training pipeline supporting 26 datasets (10 financial classification, 11 generative Q\&A, 5 pretraining corpora), multiple model architectures (Qwen, LLaMA, Gemma, Phi), and advanced techniques (LoRA, FlashAttention, MOE support). The pipeline includes automatic experiment naming, TensorBoard logging, checkpoint management, and comprehensive documentation, lowering barriers to entry for researchers and practitioners.

All experiments in this thesis are fully reproducible using documented commands and publicly available datasets. The codebase has been structured for extensibility—adding new datasets requires minimal modifications (label mappings, prompt templates). This contribution addresses reproducibility challenges in NLP research and provides a foundation for future work in specialized domain adaptation.

\section{Implications for Practice and Research}

\subsection{For Practitioners: Actionable Deployment Guidelines}

Financial institutions and fintech companies developing on-premise NLP systems can directly apply our findings:

\textbf{Data Strategy}: Curate diverse in-domain mixtures (aim for 100M+ tokens across multiple formats) rather than attempting to acquire massive single-format datasets. Prioritize format diversity (documents + Q\&A + dialogue) over volume. Use 50cap or similar strategies to prevent dominance. Avoid general-domain corpora (WikiText, C4) unless explicitly required for hybrid applications.

\textbf{Model Selection}: Deploy 1.7B models for production applications balancing quality and resources. Use 0.6B for prototyping and testing. Reserve 4B for scenarios where performance justifies cost and expertise for careful LR tuning is available.

\textbf{Training Configuration}: Scale learning rates inversely with model size: 0.6B ($2 \times 10^{-5}$) $\rightarrow$ 1.7B ($1 \times 10^{-5}$) $\rightarrow$ 4B ($5 \times 10^{-6}$). Monitor gradient norms and validation loss curves to detect instability. Allocate 100M token budgets—diminishing returns beyond this threshold for sub-5B models. Use standard configurations (AdamW, cosine schedules, 1000 warmup steps) which proved robust across experiments.

\textbf{Privacy and Compliance}: On-device deployment with 1.7B models enables GDPR-compliant financial NLP without data transmission to external services—critical for banks, investment firms, and financial advisors handling sensitive client information.

\subsection{For Researchers: Open Questions and Methodological Lessons}

Our work highlights underexplored areas in LM scaling research:

\textbf{Hyperparameter Scaling}: Scaling laws literature \parencite{kaplan2020scaling, hoffmann2022training} focuses on compute-optimal training but provides limited guidance for hyperparameter transfer across scales. Our empirical $\text{LR} \propto 1/\sqrt{N}$ relationship requires theoretical grounding—connecting to gradient flow analysis, optimizer dynamics (momentum accumulation in Adam), and effective learning rate theory. Future work should derive principled scaling relationships for batch size, warmup steps, weight decay, and optimizer hyperparameters.

\textbf{Domain Adaptation Theory}: Why does format dominate vocabulary for transfer? Our findings challenge intuitions about domain similarity. Theoretical frameworks explaining when syntactic structure (format) versus semantic content (vocabulary) drives generalization would inform curriculum design and transfer learning strategies. Neuroscience-inspired probing of intermediate representations may reveal whether format information resides in different layers/attention heads than vocabulary.

\textbf{Data Mixing Algorithms}: We used static 50cap throughout. Dynamic strategies (DoReMi, temperature sampling, curriculum learning) that adjust mixture weights based on validation loss or task difficulty may outperform static mixing. Ablation studies varying cap thresholds (30\%, 40\%, 60\%) would clarify sensitivity. Meta-learning approaches that optimize mixture ratios as hyperparameters represent promising future directions.

\textbf{Evaluation Methodology}: We assessed pretraining quality via perplexity on held-out test sets. Downstream task evaluation (sentiment classification accuracy, Q\&A F1, summarization ROUGE) would validate that pretraining improvements transfer to practical applications. Establishing correlations between pretraining perplexity and downstream performance across diverse tasks would enable efficient model selection without exhaustive downstream evaluation.

\subsection{For Industry: Privacy-Preserving Financial AI}

This work directly addresses emerging regulatory and business needs:

\textbf{Regulatory Compliance}: GDPR, CCPA, and emerging AI regulations increasingly prohibit or restrict transmission of financial data to external APIs (OpenAI, Anthropic). On-device models trained using our methods enable compliant NLP for document analysis, risk assessment, and customer service without data exfiltration.

\textbf{Competitive Differentiation}: Financial institutions accumulating proprietary datasets (transaction records, analyst reports, client communications) can leverage specialized pretraining to develop competitive advantages—custom models trained on confidential data without exposing information to vendors.

\textbf{Cost Efficiency}: Cloud API costs for LLM inference ($\$0.002-0.03$ per 1K tokens for GPT-4 class models) accumulate rapidly at scale. On-premise 1.7B models reduce marginal costs to negligible levels (electricity, amortized hardware), enabling aggressive deployment for high-volume applications (transaction categorization, automated report generation).

\textbf{Latency and Reliability}: Local inference eliminates network latency and dependency on external service availability—critical for real-time trading applications and customer-facing systems requiring <100ms response times.

\section{Future Research Directions}

\subsection{Scaling to Larger Models and Architectures}

Our experiments covered 0.6B-4B parameters (6.7$\times$ range) on Qwen3 architecture exclusively. Critical open questions:

\textbf{Larger Scales}: Do 7B, 13B, 70B models exhibit the same mixture effects and LR scaling? Larger models may benefit more from diverse pretraining (improved few-shot generalization) or less (stronger preexisting representations). The $\text{LR} \propto 1/\sqrt{N}$ relationship may require adjustment at scales exceeding 10B parameters—theoretical analysis or large-scale empirical validation needed.

\textbf{Architectural Diversity}: LLaMA, Gemma, Mistral, Phi use different architectural choices (grouped-query attention variants, different activation functions, rotary embeddings). Validating our findings across architectures would establish generality. Encoder-decoder models (T5, BART) and encoder-only models (BERT, RoBERTa) may show different mixture effects due to bidirectional attention or different pretraining objectives (masked language modeling vs causal).

\textbf{MOE Architectures}: Mixture-of-Experts models (Mixtral, DeepSeek-MOE) offer computational efficiency through sparse activation. Do MOE models benefit more from data mixtures (experts specializing on subdomains) or less (already mixture-like internally)? MOE-specific mixture strategies matching expert count to dataset count represent unexplored territory.

\subsection{Advanced Mixture Optimization}

Our 50cap strategy worked well but wasn't rigorously optimized. Future work should explore:

\textbf{Dynamic Mixture Schedules}: Curriculum learning approaches that shift mixture composition during training—start with general data for basic capabilities, transition to specialized data for domain expertise. Or reverse: start specialized to establish domain vocabulary, add general data to improve robustness.

\textbf{Adaptive Weighting}: Use validation loss gradients or uncertainty estimates to identify which datasets currently contribute most to learning. Upweight informative datasets, downweight exhausted sources. DoReMi \parencite{doremi2023} provides reference implementation; adaptation to specialized domains requires experimentation.

\textbf{Mixture Ablations}: Systematically vary cap threshold (30\%, 40\%, 50\%, 60\%, 70\%) and measure sensitivity. Optimal threshold may depend on dataset size distribution—highly imbalanced mixtures (one dataset 90\% of total) may require aggressive capping, while balanced mixtures may prefer minimal intervention.

\textbf{Multi-Stage Mixing}: Train separate models on individual datasets, then merge via model averaging, task arithmetic, or TIES merging. Compare to simultaneous mixture training. Sequential training (pretrain on Dataset A, continue on Dataset B) versus concurrent mixing represents underexplored design space.

\subsection{Comprehensive Downstream Evaluation}

This thesis assessed pretraining quality via perplexity. Validation on downstream applications would strengthen practical relevance:

\textbf{Financial Sentiment Analysis}: FPB, FiQA-SA, Twitter Financial sentiment datasets. Compare finetuning performance from different pretrained checkpoints (Mixed Financial vs WikiText vs single-dataset). Measure few-shot and zero-shot transfer.

\textbf{Financial Q\&A}: FinQA, ConvFinQA, Alpaca-Finance benchmarks. Assess both extractive (span selection) and generative (free-form answer) settings. Evaluate factual accuracy and hallucination rates—critical for financial applications.

\textbf{Document Summarization}: SEC filing summarization, earnings call summarization. Metrics: ROUGE, BERTScore, human evaluation for factuality and conciseness. Privacy-preserving summarization represents high-value application for on-device models.

\textbf{Long-Context Understanding}: Financial documents often exceed 10K tokens (10-K filings, prospectuses). Evaluate long-context capabilities using retrieval-augmented generation or extended-context versions of Qwen3 (32K+ tokens). Does mixture pretraining improve long-document coherence?

\subsection{Multi-Stage Pretraining Strategies}

Our single-stage approach (pretrain directly on financial mixtures) represents one point in a broader design space:

\textbf{General $\rightarrow$ Domain Adaptation}: Pretrain on WikiText or C4 for general capabilities, then continue pretraining on financial data. Compare to direct financial pretraining. Theory suggests general stage builds robust syntax/reasoning, domain stage adds specialized vocabulary—empirical validation needed.

\textbf{Domain $\rightarrow$ Task Specialization}: Pretrain on broad financial mixture, then continue on task-specific data (e.g., only sentiment data for sentiment model). Balances general financial knowledge with task-specific optimization.

\textbf{Mixture Schedules}: Gradually shift mixture composition across training—start balanced, progressively upweight high-priority datasets. Or inverse: start specialized, progressively diversify to improve robustness.

Optimal strategies likely depend on target application and available data. Practitioners need decision frameworks: "If you have 10M domain tokens and 100M general tokens, use Strategy X; if 100M domain and 10M general, use Strategy Y."

\subsection{Theoretical Understanding of Learning Rate Scaling}

Our empirical $\text{LR} \propto 1/\sqrt{N}$ relationship requires rigorous theoretical foundation:

\textbf{Gradient Flow Analysis}: Formally derive LR scaling from depth-dependent gradient magnitudes in transformer architectures. Connect to neural tangent kernel (NTK) theory and mean-field analysis at initialization and during training.

\textbf{Optimizer Dynamics}: Analyze how Adam's momentum and adaptive learning rate interact with model scale. Derive stability conditions for momentum accumulation in large models. Explain why $1/\sqrt{N}$ scaling provides optimal balance.

\textbf{Loss Landscape Geometry}: Investigate whether larger models have sharper minima requiring smaller learning rates (conflicting with flat minima hypothesis). Use Hessian eigenvalue analysis to characterize landscape curvature as function of scale.

\textbf{Batch Size Interactions}: We held batch size constant (32). Scaling both LR and batch size requires joint optimization—effective LR scales as $\alpha \times \sqrt{B}$. Derive principled relationships: "If you increase model size by $k$, decrease LR by $f(k)$ and increase batch size by $g(k)$."

Theoretical understanding would enable principled hyperparameter setting without costly empirical search—critical for democratizing LLM development beyond organizations with massive compute budgets.

\section{Closing Remarks}

This thesis demonstrates that effective specialized language models can be developed without massive computational resources or proprietary datasets. By carefully curating diverse in-domain data mixtures, properly scaling hyperparameters across model sizes, and targeting lightweight 1-2B parameter models, practitioners can train privacy-preserving financial NLP systems suitable for on-device deployment.

The core insight—that diverse in-domain mixing dramatically outperforms general-domain pretraining for specialized applications—challenges prevalent assumptions favoring general-purpose foundation models. For domains with sufficient available data (finance, legal, medical, scientific), specialized pretraining offers superior performance at lower cost compared to adapting general-purpose models via finetuning or prompting.

As privacy regulations tighten and organizations recognize competitive value in proprietary data, on-device specialized models will become increasingly important. This work provides empirical foundations and practical guidelines for developing such systems, contributing to a future where powerful NLP capabilities are accessible without sacrificing privacy, incurring ongoing API costs, or depending on external service providers.

The open-source pipeline and reproducible experimental methodology lower barriers to entry, enabling researchers and practitioners to build on these findings and extend them to new domains, architectures, and applications. By sharing not just results but complete implementations, we hope to accelerate progress toward privacy-preserving, efficient, and democratically accessible language understanding for specialized domains.