\chapter{Conclusion}


This thesis shows that effective specialized language models can be developed without massive computational resources or diverse data mixtures. By selecting focused medium datasets (3.6–8.5M tokens), using stable training settings, and targeting lightweight 0.6–4B parameter models, one can train privacy-preserving financial NLP systems suitable for on-device deployment. Contrary to expectations, individual datasets (FiQA, FinGPT, Alpaca) consistently outperform mixtures on both performance (2.5–3.2$\times$ better) and consistency (1.5–4.8$\times$ better).

The core insight that {individual medium datasets (3.6–8.5M tokens) consistently outperform mixtures on both performance and consistency} challenges conventional belief favoring data diversity. At fixed token budgets (100M), FiQA/FinGPT/Alpaca achieve 2.5–3.2$\times$ better perplexity and 1.5–4.8$\times$ better consistency than 7-dataset mixtures. Format inconsistency, differences in vocabulary distribution, and multi-task interference degrade mixture performance despite anticipated diversity benefits. From our experiments and findings, we argue that specialized pretraining should prioritize focused, high-quality medium datasets over diverse mixtures, especially when token budgets are limited. For domains with curated data (finance, legal, medical), individual dataset optimization offers superior performance at lower cost than either mixtures or general-purpose model adaptation.

As privacy regulations tighten and organizations recognize competitive value in proprietary data, on-device specialized models will become increasingly important. This work provides empirical foundations and practical guidelines for developing such systems where powerful NLP capabilities are accessible while at the same time also ensuring privacy and low cost requirements.
