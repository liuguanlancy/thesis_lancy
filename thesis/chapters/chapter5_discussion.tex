\chapter{Discussion}

This chapter interprets the findings from Chapter 4. We explain what likely drives the mixture effects, the training dynamics, and the generalization patterns. The goal is simple: turn results into guidance without overreaching. Where the data are thin, we say so. We keep claims tied to evidence. Some choices were pragmatic.

\section{Key Empirical Findings}

Our 10 experiments (30 models, 237 evaluations) lead to four main findings about data‑mixture effects in specialized‑domain language model pretraining. First, in‑domain diversity outperforms general‑corpus quality. Mixed Financial datasets achieve 21.55 ppl (4B) with 55\% relative spread, substantially better than WikiText’s 41.96 ppl mean financial performance (~53\% spread after LR adjustment). This $1.95\times$ gap suggests that several in‑domain datasets, even if small or noisy, provide stronger specialization than one large, clean general corpus. \Cref{fig:scaling_comparison_all} shows the gap widening from 0.6B to 4B; the cross‑dataset tables (\Cref{tab:cross_financial_news,tab:cross_financial_qa,tab:cross_fingpt,tab:cross_fiqa}) tell the same story.

Second, simple learning‑rate reductions stabilized a few runs. We used LR=2e‑5 for all main runs; in three configurations (WikiText, Financial QA, Twitter), smaller LRs (e.g., $1\times10^{-5}$ or $5\times10^{-6}$) improved stability and performance. We treat these as pragmatic fixes, not a scaling rule (see \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter}; \Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison}).

Third, dataset size critically affects pretraining viability. Datasets >100M tokens support standalone pretraining (2–5 epochs; consistent generalization); 20–100M tokens are viable with caveats (6–30 epochs; moderate generalization); <20M tokens are not viable alone (67–249 epochs; severe overtraining and high cross‑dataset variability). Correlation between log(tokens) and variability is $r=-0.78$ ($p<0.01$). Small datasets require mixing regardless of optimization quality. The figures make this visible: \Cref{fig:scaling_news_articles,fig:scaling_sec_reports} are smooth; \Cref{fig:scaling_financial_qa,fig:scaling_twitter} are erratic and need LR interventions. \Cref{tab:cross_financial_qa,tab:cross_twitter} show specialization without transfer.

Fourth, format drives transfer more than domain vocabulary. Long‑form documents transfer well (News $\leftrightarrow$ SEC: $r=0.82$); instruction tasks cluster (FinGPT/Alpaca/FiQA: $r=0.68$–$0.73$); Twitter is isolated. A News model transfers better to SEC filings (long‑form $\leftrightarrow$ long‑form) than to Twitter (same domain label, different format). We therefore prioritize format diversity alongside domain coverage. \Cref{tab:cross_financial_news,tab:cross_financial_repor,tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa,tab:cross_twitter} show the diagonals and clusters.

These findings generalize beyond finance to any specialized‑domain pretraining scenario where practitioners face similar trade‑offs: domain vs general data, mixture composition, model scaling, and format diversity.

\section{Interpretation of Data Interaction Effects}

\subsection{Why WikiText Underperforms on Financial Tasks}

WikiText's catastrophic financial transfer (41.96 ppl mean vs 21.55 ppl for Mixed Financial) stems from three fundamental mismatches.

First, vocabulary. Financial language contains specialized terminology absent in encyclopedic text. Terms like ``EBITDA'' (earnings before interest, taxes, depreciation, amortization), ``alpha'' (excess returns), ``basis points'' (0.01\%), ``volatility'' (price fluctuation measure), ``hedging'' (risk mitigation strategy), and ``P/E ratio'' (price-to-earnings valuation) rarely appear in Wikipedia. When WikiText models encounter financial evaluation texts, they face effective out-of-vocabulary scenarios despite shared syntactic structure. The model's vocabulary distribution mismatches the evaluation domain's lexical requirements.

Second, reasoning patterns. Financial analysis requires forward-looking causal reasoning: ``Company X's earnings miss will pressure the stock downward'' (cause-effect prediction), ``Rising interest rates typically compress equity valuations'' (conditional reasoning), ``The Fed's hawkish stance suggests tightening ahead'' (implicit reasoning from policy to outcomes). Wikipedia's encyclopedic, descriptive style, focused on established facts, historical narratives, and definitional content, doesn't exercise these prospective reasoning patterns. Models pretrained on WikiText learn to predict continuations based on factual descriptions, not anticipatory financial logic.

Third, discourse structures diverge. Financial news follows inverted pyramid structure (conclusion first, then supporting details); earnings reports have standardized sections (forward-looking statements, risk factors, MD\&A); analyst reports use comparison tables and numerical evidence. Wikipedia articles employ chronological narratives (biographical entries), topical organization (scientific articles), or definitional structures (concept entries). These discourse patterns create different coherence signals. WikiText models learn topic progression and factual elaboration, while financial texts require comparative analysis and evidential reasoning structures.

The asymmetry reveals hierarchical structure. General language (syntax, semantics, discourse coherence) forms a foundation; financial language adds specialized vocabulary and reasoning on top. Starting from general pretraining provides linguistic prerequisites; domain-specific training adds specialization without catastrophic forgetting of fundamentals. Conversely, general pretraining lacks domain prerequisites. Vocabulary and reasoning gaps cannot be bridged by linguistic competence alone. This asymmetry is strikingly visible in \Cref{tab:cross_wikitext}: WikiText training rows show boldface in WikiText columns (4.78-38.60 ppl across model sizes) but poor financial performance (26-58 ppl depending on dataset and LR). Financial training rows show acceptable WikiText performance (27-42 ppl) alongside superior financial metrics. The table's boldface distribution pattern, concentrated in financial rows for most columns, scattered in WikiText rows, quantitatively demonstrates that financial pretraining retains general capability while general pretraining fails to acquire domain specialization.

\subsection{Benefits of In-Domain Diversity}

Mixed Financial's advantage (21.55 ppl, 55\% relative spread) over individual datasets (mean: 24.8 ppl, \~65\% relative spread) and WikiText (41.96 ppl financial, \~53\% relative spread after LR adjustment) stems from diversity-driven stability:

The 7-dataset mixture spans long-form documents (News 197M, SEC 80M), instruction formats (FinGPT 19M, Alpaca 17M, FiQA 4M), and short‑form text (Twitter 0.3M, Financial QA 3.5M). This format diversity prevents overfitting to structural artifacts. Models trained on pure News learn long-form coherence but fail on dialogic Q\&A (41\% worse on FiQA); mixed models handle both, averaging only 30\% degradation across all formats.

Different financial datasets emphasize different lexical subdomains: News covers market events and company names; SEC covers regulatory terminology (``10-K'', ``forward-looking statements''); FinGPT covers sentiment vocabulary (``bullish'', ``bearish''); Alpaca covers financial concepts (``compound interest'', ``diversification''). The mixture creates broad vocabulary coverage. No single dataset provides this breadth. Mixed models encounter 3.2$\times$ more unique financial terms during training than the largest individual dataset (News), improving lexical stability.

Mixing datasets with different objectives (sentiment classification, Q\&A, document completion) acts as implicit multi-task learning. The model cannot overfit to any single task's superficial cues (e.g., specific sentiment indicators in FinGPT, formulaic question structures in Alpaca) because the loss function averages across diverse distributions. This produces representations that capture underlying financial semantics rather than task-specific shortcuts.

Small datasets suffer from memorization. Financial QA (3.5M tokens, 67-100 epochs) achieves 8.09 ppl in-domain but 41.7 ppl cross‑dataset. The model memorizes training examples rather than learning generalizable patterns. Mixing prevents memorization by capping each dataset's contribution (50cap strategy limits News to 50\%, ensuring others get exposure) and diversifying the training distribution. Mixed models see fewer repeated examples from any single source, forcing extraction of transferable features.

Variability reduction correlates with mixture diversity: the 7-dataset mixture (\~55\% relative spread) compares favorably to individual datasets (often \~65\% or higher). The mixture improves both performance (21.55 vs 24.8 ppl mean) and consistency simultaneously. The cross‑dataset tables illustrate this: Mixed Financial rows appear most frequently in boldface across evaluation columns. Individual dataset rows (News, SEC, FinGPT, etc.) capture boldface mainly in their own or nearby columns, while Mixed Financial remains competitive across the board. This broad vs narrow boldface distribution visualizes how diversity enables more stable generalization across heterogeneous evaluation scenarios.

\subsection{Domain Interference Patterns}

While in-domain diversity helps, cross-domain mixing (Mixed Wiki+Financial) shows interference:

Mixed Wiki+Financial achieves 26.69 ppl (4B), 24\% worse than pure Mixed Financial (21.55 ppl), despite including WikiText. On WikiText specifically, the mixed approach improves performance modestly compared to pure Financial, but mean financial performance degrades notably. The trade-off is unfavorable for finance-focused applications: sacrificing financial performance for a small general-domain gain.

Financial and general domains create conflicting gradients. Financial texts reward predicting domain terminology (``EBITDA'' following ``reported''); general texts reward different continuations (``findings'' following ``reported''). The model's parameters cannot simultaneously optimize for both distributions without compromise. Mixed Wiki+Financial models average these signals, achieving moderate performance on both rather than excellence on either. The 62\% variance (vs 55\% pure financial) reflects this optimization conflict.

\textbf{When Mixing Hurts vs Helps}: Intra-domain mixing helps because datasets share core semantics (financial vocabulary, reasoning patterns) while differing in format and task type, diversity reinforces fundamentals. Cross-domain mixing hurts when domains diverge in vocabulary and reasoning (encyclopedic vs analytical), creating zero-sum trade‑offs. The 50cap strategy mitigates but doesn't eliminate interference: capping WikiText at 50\% limits damage but still dilutes financial specialization. This distinction is evident comparing \Cref{tab:mixed_financial_results} (pure financial mixture) and \Cref{tab:mixed_wiki_financial_results} (cross-domain mixture): the former shows consistently lower perplexity across all financial evaluation datasets, with the performance advantage increasing at larger model sizes. \Cref{fig:scaling_mixed_financial,fig:scaling_mixed_wiki_financial} visually confirm this, the pure financial mixture (first figure) shows steeper slope (22.6\% total improvement) compared to Wiki+Financial (second figure, 15.1\% improvement), indicating that domain conflict reduces scaling efficiency.

\textbf{Practical Implication}: For specialized applications, domain purity wins. Only mix cross-domain when explicit general-domain retention is required (e.g., conversational agents handling both financial and general queries). For finance-focused deployments, pure in-domain mixtures maximize performance.

\subsection{Scale-Dependent Training Notes}

Our experience suggests that larger models can be more sensitive to optimization settings on some datasets. While we kept LR=2e-5 for main runs, reducing LR in a handful of follow-ups helped stabilize training. We do not claim a general rule beyond this observation.

\section{Practical Guidelines for Financial LM Pretraining}

Synthesizing experimental findings into actionable recommendations:

\subsection{Data Mixture Strategies by Use Case}

\textbf{General-Purpose Financial NLP}: Use Mixed Financial (7 datasets, 50cap). Achieves best all-around performance (21.55 ppl, 55\% relative spread) with stable cross-task generalization. Suitable for applications requiring diverse financial capabilities: sentiment analysis, document summarization, Q\&A, information extraction. As shown in \Cref{fig:scaling_mixed_financial,fig:scaling_comparison_all}, this approach scales reliably across model sizes and consistently outperforms alternatives. The cross‑dataset tables also support this choice: Mixed Financial rows capture boldface positions more often than any individual dataset across the eight evaluation scenarios.

\textbf{Specialized Document Analysis}: Use single large dataset if available ($>$ 100M tokens). SEC @ 4B (15.91 ppl on SEC; \~19\% relative spread across evaluations) excels for regulatory filing analysis; News @ 4B (17.47 ppl on News; \~66\% relative spread) excels for journalism. Specialization improves in-domain performance but sacrifices cross-format transfer. \Cref{fig:scaling_news_articles,fig:scaling_sec_reports} show these datasets maintain stable scaling without requiring LR adjustments. However, \Cref{tab:cross_financial_news,tab:cross_financial_repor} reveal that News and SEC training rows achieve boldface primarily within document-format columns, confirming limited format diversity.

For instruction-following and Q\&A applications, use FiQA (4M tokens, 16.35 ppl) or FinGPT (19M tokens, 19.83 ppl) for specialized Q\&A, or include in mixture for general applications. Instruction formats transfer moderately within task type ($r = 0.68-0.73$) but poorly to documents. The instruction-following tables (\Cref{tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa}) show boldface clustering along the diagonal and adjacent instruction rows, visualizing the format-based transfer limitation.

\textbf{Balanced General + Financial Capabilities}: Use Mixed Wiki+Financial only if general-domain retention is explicitly required (e.g., chatbots handling both financial and general queries). Accepts 24\% financial performance cost for 16\% general improvement, unfavorable for finance-focused deployments. \Cref{fig:scaling_mixed_wiki_financial} shows reduced slope compared to pure financial mixture, and \Cref{tab:mixed_wiki_financial_results} documents the performance cost across all financial evaluation datasets.

\textbf{Avoid}: Pure WikiText for financial applications (2.0$\times$ performance degradation vs Mixed Financial on average across financial tasks), small individual datasets $<$ 20M tokens (non-viable standalone due to severe overtraining and high variability), single-format training when diverse tasks are expected (format mismatch prevents transfer). \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} provide visual evidence: WikiText requires LR adjustment and still shows poor financial transfer, while small datasets exhibit brittleness visible in both scaling curves and cross‑dataset table patterns.

\subsection{Model Size Selection}

\textbf{0.6B Models}: Fast training ($\sim$6 hours for 100M tokens on Lambda Labs GPUs), low memory (4GB), suitable for rapid prototyping. Performance is acceptable for exploratory work, but variability is high (Mixed Financial: \~98\% relative spread). Use for development, experimentation, or extremely resource-constrained deployment (mobile devices).

\textbf{1.7B Models}: Best performance-efficiency balance. Training moderate ($\sim$12 hours), memory reasonable (10GB), performance strong with improved consistency vs 0.6B (Mixed Financial: \~63\% relative spread). Recommended for most applications, strong performance at substantially lower resource cost than 4B. Optimal for production deployment balancing quality and resource constraints.

\textbf{4B Models}: Best absolute performance (21.55 ppl, 55\% relative spread) but requires careful hyperparameter tuning (LR $5 \times 10^{-6}$ in affected cases) and substantial resources (20GB memory, $\sim$24 hours training). Use when maximizing performance justifies cost, and when expertise for hyperparameter tuning is available. Critical: failure to tune learning rate can cause reverse scaling, practitioners may need to reduce LR substantially at larger scales.

\textbf{Scaling Decision Tree}: In short, choose 0.6B under tight resource constraints (mobile, edge) and accept about a 22\% loss versus 4B; pick 1.7B for the best performance–efficiency balance (about 92\% of 4B at roughly half the resources); go to 4B only when maximum performance justifies the tuning effort.

\subsection{Learning Rate Notes}

We used LR=2e-5 for all main runs. In three follow-ups with anomalies (WikiText, Financial QA, Twitter), smaller LRs (e.g., $1\times10^{-5}$ or $5\times10^{-6}$) stabilized training. These are practical notes from our setup, not prescriptive guidelines for other contexts.

\subsection{Token Budget Allocation}

A 100M token budget proved sufficient when properly mixed, showing diminishing returns beyond that threshold for our 0.6B-4B models. Larger models ($>$ 7B) may benefit from extended training (200-500M tokens), but we did not test this.

We used 50cap to prevent dominance: if the largest dataset exceeds 50\% of the total, cap it there and sample others proportionally. This ensures diversity while respecting relative dataset informativeness. During training, sample each batch from the mixture distribution at the token level, not by example or epoch. Sequential exposure can cause catastrophic forgetting; token-level interleaving avoids that.

When curating datasets, format diversity comes first, followed by size (aim for $\geq$ 100M total), then quality. In-domain noisy data beats out-of-domain clean data in our runs. Don't exclude small datasets ($<$ 20M tokens) from mixtures; they contribute valuable diversity despite non-viability as standalone sources.

\section{Limitations and Threats to Validity}

All experiments used Qwen3 (0.6B/1.7B/4B). Observations about LR behavior may be architecture- and dataset-specific. Other decoder-only transformers (LLaMA, Gemma, Phi) could behave differently; validation required. Encoder-only (BERT) or encoder-decoder (T5) models may show different mixture effects due to bidirectional attention or different pretraining objectives.

We used 50cap exclusively. Other algorithms (temperature sampling, equal mixing, DoReMi dynamic weighting) remain unexplored. The 50cap heuristic worked well but may not be optimal. Ablation studies varying cap thresholds (30\%, 40\%, 60\%) could reveal improvements. Dynamic mixture strategies that adjust dataset weights during training based on validation loss may outperform static 50cap.

We evaluated using perplexity on held-out test sets from the same distributions as training data. This measures pretraining quality but doesn't directly assess downstream task performance. Fine-tuned performance on financial NLP tasks (sentiment classification accuracy, Q\&A F1, summarization ROUGE) may differ from pretraining perplexity rankings. Future work should validate that Mixed Financial's pretraining advantage transfers to downstream applications.

Experiments were limited to 0.6B-4B models due to available hardware (RTX A6000 48GB, A100 40GB, H100 80GB rented from Lambda Labs). Larger models (7B, 13B, 70B) may show different patterns; mixture benefits may increase or decrease with scale. We did not investigate LR behavior beyond the few follow-ups reported here.

We systematically explored learning rates but kept other hyperparameters fixed (effective batch size 8, warmup 1000 steps, cosine schedule). Larger hyperparameter sweeps over batch size (4, 8, 16, 32), warmup ratios (1\%, 3\%, 5\%), and schedules (linear, cosine, polynomial) may reveal better configurations. Computational budget constraints prevented exhaustive search.

Results may not generalize to other specialized domains with different characteristics. Legal text (extremely long documents, formal citations) or medical text (heavy abbreviations, multimodal integration) may show different mixture effects. The core principles (in-domain diversity, and our LR heuristics) may generalize, but specific mixture ratios and optimal configurations require domain-specific validation.

Despite these limitations, our findings provide solid empirical evidence for data mixture effects, training dynamics, and practical practices applicable to financial LM pretraining and likely informative for other specialized domains.
