\chapter{Discussion}

This chapter interprets the results in Chapter 4. What drives the mixture effects, the scaling behavior, and the transfer patterns. Then we turn them into practical guidelines. Finally, we note limits.

\section{Key Empirical Findings}

Across 10 experiments (30 models, 240 evaluations), four findings stand out:

\textbf{Finding 1: In-domain diversity beats general quality.}

Mixed Financial reaches 21.55 ppl (4B) with 55\% variance. WikiText averages 48.7 ppl on financial sets with 78\% variance. A 2.3$\times$ gap. Multiple in-domain datasets -- even if small (Twitter 0.3M) or noisy -- produce stronger domain specialization than a large, curated general corpus. This challenges the idea that "high-quality general text is enough." \Cref{fig:scaling_comparison_all} shows the gap widening from 0.6B to 4B. The cross-dataset tables (\Cref{tab:cross_financial_news,tab:cross_financial_qa,tab:cross_fingpt,tab:cross_fiqa}) echo this: Mixed Financial rows frequently take boldface; WikiText rows rarely do outside their own domain.

\textbf{Finding 2: Small LR cuts fixed a few runs.}

All main runs used LR=2e-5. Three cases misbehaved (WikiText, Financial QA, Twitter). Dropping LR (to $1\times10^{-5}$ or $5\times10^{-6}$) stabilized training and improved results. These are practical fixes, not a general rule. Solid vs dashed lines in \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} show the effect; \Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison} list the numbers.

\textbf{Finding 3: Size decides standalone viability.}

Thresholds are clear. $>$100M tokens: standalone works (2–5 epochs), robust generalization. 20–100M: viable with caveats (6–30 epochs). $<$20M: not viable alone (67–249 epochs), 89–97\% variance. Correlation log(tokens) vs variance: $r = -0.78$ ($p < 0.01$). The limit is data, not just hyperparameters. Large sets show smooth scaling (\Cref{fig:scaling_news_articles,fig:scaling_sec_reports}); small sets show erratic curves and need LR changes (\Cref{fig:scaling_financial_qa,fig:scaling_twitter}). Cross‑dataset tables (\Cref{tab:cross_financial_qa,tab:cross_twitter}) show brittleness: boldface appears only in the dataset’s own column.

\textbf{Finding 4: Format beats vocabulary for transfer.}

Document format and task structure predict transfer better than topic domain. Long‑form (News $\leftrightarrow$ SEC: $r = 0.82$) transfers well. Instruction tasks cluster (FinGPT/Alpaca/FiQA: $r = 0.68$–$0.73$). Twitter is isolated (89\% variance). A News model transfers to SEC better than to Twitter—even though Twitter is “finance.” Pretraining corpora should include format diversity, not just domain coverage. The cross‑dataset tables show diagonal boldface in long‑form (\Cref{tab:cross_financial_news,tab:cross_financial_repor}) and in instruction clusters (\Cref{tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa}). \Cref{tab:cross_twitter} shows isolation: boldface only in Twitter’s own column.

These points likely generalize beyond finance when facing the same trade‑offs: domain vs general data, mixture composition, model scaling, and format diversity. With local details changing, of course.

\section{Interpretation of Data Interaction Effects}

\subsection{Why WikiText Underperforms on Financial Tasks}

WikiText's catastrophic financial transfer (48.7 ppl mean vs 21.55 ppl for Mixed Financial) stems from three fundamental mismatches:

\textbf{1. Vocabulary Gap}: Financial language contains specialized terminology absent in encyclopedic text. Terms like ``EBITDA'' (earnings before interest, taxes, depreciation, amortization), ``alpha'' (excess returns), ``basis points'' (0.01\%), ``volatility'' (price fluctuation measure), ``hedging'' (risk mitigation strategy), and ``P/E ratio'' (price-to-earnings valuation) rarely appear in Wikipedia. When WikiText models encounter financial evaluation texts, they face effective out-of-vocabulary scenarios despite shared syntactic structure. The model's vocabulary distribution mismatches the evaluation domain's lexical requirements.

\textbf{2. Reasoning Pattern Mismatch}: Financial analysis requires forward-looking causal reasoning: ``Company X's earnings miss will pressure the stock downward'' (cause-effect prediction), ``Rising interest rates typically compress equity valuations'' (conditional reasoning), ``The Fed's hawkish stance suggests tightening ahead'' (implicit reasoning from policy to outcomes). Wikipedia's encyclopedic, descriptive style—focused on established facts, historical narratives, and definitional content—doesn't exercise these prospective reasoning patterns. Models pretrained on WikiText learn to predict continuations based on factual descriptions, not anticipatory financial logic.

\textbf{3. Discourse Structure Divergence}: Financial news follows inverted pyramid structure (conclusion first, then supporting details); earnings reports have standardized sections (forward-looking statements, risk factors, MD\&A); analyst reports use comparison tables and numerical evidence. Wikipedia articles employ chronological narratives (biographical entries), topical organization (scientific articles), or definitional structures (concept entries). These discourse patterns create different coherence signals—WikiText models learn topic progression and factual elaboration, while financial texts require comparative analysis and evidential reasoning structures.

\textbf{Why General → Financial Transfer Fails But Financial → General Succeeds}: The asymmetry (WikiText @ 4B: 48.7 ppl financial vs Mixed Financial @ 4B: 33.7 ppl WikiText) reveals hierarchical structure. General language (syntax, semantics, discourse coherence) forms a foundation; financial language adds specialized vocabulary and reasoning on top. Starting from general pretraining provides linguistic prerequisites; domain-specific training adds specialization without catastrophic forgetting of fundamentals. Conversely, starting from general pretraining lacks domain prerequisites—vocabulary and reasoning gaps cannot be bridged by linguistic competence alone. This asymmetry is strikingly visible in \Cref{tab:cross_wikitext}: WikiText training rows show boldface in WikiText columns (9-32 ppl after LR adjustment) but catastrophic financial performance (40-60 ppl, rarely boldface). Financial training rows show acceptable WikiText performance (30-35 ppl) alongside superior financial metrics. The table's boldface distribution pattern—concentrated in financial rows for most columns, scattered in WikiText rows—quantitatively demonstrates that financial pretraining retains general capability while general pretraining fails to acquire domain specialization.

\subsection{Benefits of In-Domain Diversity}

Mixed Financial's superiority (21.55 ppl, 55\% CV) over individual datasets (mean: 24.8 ppl, 65\% CV) and WikiText (48.7 ppl financial, 78\% CV) stems from diversity-driven robustness:

\textbf{Cross-Format Exposure}: The 7-dataset mixture spans long-form documents (News 197M, SEC 80M), instruction formats (FinGPT 19M, Alpaca 17M, FiQA 4M), and short-form text (Twitter 0.3M, Financial QA 3.5M). This format diversity prevents overfitting to structural artifacts. Models trained on pure News learn long-form coherence but fail on dialogic Q\&A (41\% worse on FiQA); mixed models handle both, averaging only 30\% degradation across all formats.

\textbf{Vocabulary Coverage}: Different financial datasets emphasize different lexical subdomains: News covers market events and company names; SEC covers regulatory terminology (``10-K'', ``forward-looking statements''); FinGPT covers sentiment vocabulary (``bullish'', ``bearish''); Alpaca covers financial concepts (``compound interest'', ``diversification''). The mixture creates comprehensive vocabulary coverage—no single dataset provides this breadth. Mixed models encounter 3.2$\times$ more unique financial terms during training than largest individual dataset (News), improving lexical robustness.

\textbf{Task Diversity Regularization}: Mixing datasets with different objectives (sentiment classification, Q\&A, document completion) acts as implicit multi-task learning. The model cannot overfit to any single task's superficial cues (e.g., specific sentiment indicators in FinGPT, formulaic question structures in Alpaca) because the loss function averages across diverse distributions. This produces representations that capture underlying financial semantics rather than task-specific shortcuts.

\textbf{Preventing Data Memorization}: Small datasets suffer from memorization—Financial QA (3.5M tokens, 67-100 epochs) achieves 8.09 ppl in-domain but 41.7 ppl cross-dataset. The model memorizes training examples rather than learning generalizable patterns. Mixing prevents memorization by capping each dataset's contribution (50cap strategy limits News to 50\%, ensuring others get exposure) and diversifying the training distribution. Mixed models see fewer repeated examples from any single source, forcing extraction of transferable features.

\textbf{Quantitative Evidence}: Variance reduction correlates with mixture diversity: 7-dataset mixture (55\% CV) $<$ largest individual (News 26\% CV in-domain, 65\% cross-dataset) $<$ small individuals (89-97\% CV). The mixture achieves 12.7\% lower variance than same-scale individual training, demonstrating that diversity improves both performance (21.55 vs 24.8 ppl) and robustness simultaneously. The cross-dataset tables provide visual proof: examining all eight tables together, Mixed Financial rows dominate boldface positions—appearing most frequently across different evaluation columns. Individual dataset rows (News, SEC, FinGPT, etc.) capture boldface primarily in their own or closely related columns, while Mixed Financial maintains competitive boldface presence everywhere. This boldface distribution pattern—broad for mixed, narrow for individuals—visualizes how diversity enables robust generalization across heterogeneous evaluation scenarios.

\subsection{Domain Interference Patterns}

While in-domain diversity helps, cross-domain mixing (Mixed Wiki+Financial) shows interference:

\textbf{Performance-Diversity Trade-off}: Mixed Wiki+Financial achieves 26.69 ppl (4B), 24\% worse than pure Mixed Financial (21.55 ppl), despite including WikiText. On WikiText specifically, Wiki+Financial achieves 28.4 ppl vs pure Financial's 33.7 ppl (15.7\% improvement), but mean financial performance degrades from 20.2 to 26.1 ppl (29.2\% degradation). The trade-off is unfavorable: sacrificing 29\% financial performance for 16\% general improvement.

\textbf{Competing Optimization Signals}: Financial and general domains create conflicting gradients. Financial texts reward predicting domain terminology (``EBITDA'' following ``reported''); general texts reward different continuations (``findings'' following ``reported''). The model's parameters cannot simultaneously optimize for both distributions without compromise. Mixed Wiki+Financial models average these signals, achieving moderate performance on both rather than excellence on either. The 62\% variance (vs 55\% pure financial) reflects this optimization conflict.

\textbf{When Mixing Hurts vs Helps}: Intra-domain mixing helps because datasets share core semantics (financial vocabulary, reasoning patterns) while differing in format and task type—diversity reinforces fundamentals. Cross-domain mixing hurts when domains diverge in vocabulary and reasoning (encyclopedic vs analytical), creating zero-sum trade-offs. The 50cap strategy mitigates but doesn't eliminate interference: capping WikiText at 50\% limits damage but still dilutes financial specialization. This distinction is evident comparing \Cref{tab:mixed_financial_results} (pure financial mixture) and \Cref{tab:mixed_wiki_financial_results} (cross-domain mixture): the former shows consistently lower perplexity across all financial evaluation datasets, with the performance advantage increasing at larger model sizes. \Cref{fig:scaling_mixed_financial,fig:scaling_mixed_wiki_financial} visually confirm this—the pure financial mixture (first figure) shows steeper slope (22.6\% total improvement) compared to Wiki+Financial (second figure, 15.1\% improvement), indicating that domain conflict reduces scaling efficiency.

\textbf{Practical Implication}: For specialized applications, domain purity wins. Only mix cross-domain when explicit general-domain retention is required (e.g., conversational agents handling both financial and general queries). For finance-focused deployments, pure in-domain mixtures maximize performance.

\subsection{Scale-Dependent Training Notes}

Our experience suggests that larger models can be more sensitive to optimization settings on some datasets. While we kept LR=2e-5 for main runs, reducing LR in a handful of follow-ups helped stabilize training. We do not claim a general rule beyond this observation.

\section{Practical Guidelines for Financial LM Pretraining}

Synthesizing experimental findings into actionable recommendations:

\subsection{Data Mixture Strategies by Use Case}

\textbf{General-Purpose Financial NLP}: Use Mixed Financial (7 datasets, 50cap). Achieves best all-around performance (21.55 ppl, 55\% CV) with robust cross-task generalization. Suitable for applications requiring diverse financial capabilities: sentiment analysis, document summarization, Q\&A, information extraction. As demonstrated in \Cref{fig:scaling_mixed_financial,fig:scaling_comparison_all}, this approach scales reliably across model sizes and consistently outperforms alternatives. The cross-dataset tables further validate this choice: Mixed Financial rows capture boldface positions more frequently than any individual dataset across the eight evaluation scenarios, providing empirical evidence of broad generalization capability.

\textbf{Specialized Document Analysis}: Use single large dataset if available ($>$ 100M tokens). SEC @ 4B (22.47 ppl on SEC, 18\% in-domain CV) excels for regulatory filing analysis; News @ 4B (18.92 ppl on News, 26\% CV) excels for journalism. Specialization improves in-domain performance slightly but sacrifices cross-format transfer. \Cref{fig:scaling_news_articles,fig:scaling_sec_reports} show these datasets maintain stable scaling without requiring LR adjustments. However, \Cref{tab:cross_financial_news,tab:cross_financial_repor} reveal that News and SEC training rows achieve boldface primarily within document-format columns, confirming limited format diversity.

\textbf{Instruction-Following / Q\&A Applications}: Use FiQA (4M tokens, 16.35 ppl) or FinGPT (19M tokens, 19.83 ppl) for specialized Q\&A, or include in mixture for general applications. Instruction formats transfer moderately within task type ($r = 0.68-0.73$) but poorly to documents. The instruction-following tables (\Cref{tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa}) show boldface clustering along the diagonal and adjacent instruction rows, visualizing the format-based transfer limitation.

\textbf{Balanced General + Financial Capabilities}: Use Mixed Wiki+Financial only if general-domain retention is explicitly required (e.g., chatbots handling both financial and general queries). Accepts 24\% financial performance cost for 16\% general improvement—unfavorable for finance-focused deployments. \Cref{fig:scaling_mixed_wiki_financial} shows reduced slope compared to pure financial mixture, and \Cref{tab:mixed_wiki_financial_results} documents the performance cost across all financial evaluation datasets.

\textbf{Avoid}: Pure WikiText for financial applications (2.3$\times$ performance degradation), small individual datasets $<$ 20M tokens (89-97\% variance, non-viable standalone), single-format training when diverse tasks expected (format mismatch prevents transfer). \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} provide visual evidence: WikiText requires heavy LR adjustment and still shows poor financial transfer, while small datasets exhibit extreme brittleness visible in both scaling curves and cross-dataset table patterns.

\subsection{Model Size Selection}

\textbf{0.6B Models}: Fast training ($\sim$6 hours for 100M tokens on Lambda Labs GPUs), low memory (4GB), suitable for rapid prototyping. Performance acceptable (27.84 ppl Mixed Financial) but high variance (63\% CV). Use for development, experimentation, or extremely resource-constrained deployment (mobile devices).

\textbf{1.7B Models}: Best performance-efficiency balance. Training moderate ($\sim$12 hours), memory reasonable (10GB), performance strong (24.12 ppl, 58\% CV). Recommended for most applications—92\% of 4B's performance at 2.4$\times$ lower memory and 2$\times$ faster training. Optimal for production deployment balancing quality and resource constraints.

\textbf{4B Models}: Best absolute performance (21.55 ppl, 55\% CV) but requires careful hyperparameter tuning (LR $5 \times 10^{-6}$) and substantial resources (20GB memory, $\sim$24 hours training). Use when maximizing performance justifies cost, and when expertise for hyperparameter tuning is available. Critical: failure to tune learning rate causes reverse scaling—practitioners must reduce LR by 75\% from 0.6B baseline.

\textbf{Scaling Decision Tree}:
\begin{enumerate}
\item \textbf{Resource-constrained} (mobile, edge devices): 0.6B, accept 22\% performance loss vs 4B
\item \textbf{Balanced production deployment}: 1.7B, optimal trade-off (92\% of 4B performance, 50\% resources)
\item \textbf{Performance-critical} (willing to invest tuning effort): 4B, requires LR scaling expertise
\end{enumerate}

\subsection{Learning Rate Notes}

\textbf{Main setting}: $2 \times 10^{-5}$ across all primary experiments.

\textbf{Follow-ups}: For the few runs with anomalies, we used smaller LRs (e.g., $1\times10^{-5}$ or $5\times10^{-6}$) to stabilize training.

\textbf{Scope}: These are practical notes from our setup, not prescriptive guidelines.

\subsection{Token Budget Allocation}

\textbf{Optimal Token Budget}: 100M tokens sufficient when properly mixed across diverse datasets. Diminishing returns beyond this threshold for 0.6B-4B models in our experiments. Larger models ($>$ 7B) may benefit from extended training (200-500M tokens), but this remains untested.

\textbf{Mixture Composition}: Use 50cap strategy to prevent dominance. For $n$ datasets with sizes $\{s_1, s_2, ..., s_n\}$ where $s_1 > 0.5 \sum_i s_i$: cap $s_1$ at 50\% of total, sample others proportionally. This ensures diversity while respecting relative dataset informativeness.

\textbf{Sampling Strategy}: Token-level interleaving, not batch-level or epoch-level. Sample each training batch from mixture distribution with probabilities proportional to (capped) dataset sizes. Avoids sequential exposure that can cause catastrophic forgetting.

\textbf{Dataset Prioritization}: When curating datasets, prioritize: (1) Format diversity (documents, Q\&A, dialogue), (2) Size (aim for $\geq$ 100M total across sources), (3) Quality (clean text $>$ noisy text, but in-domain noisy $>$ out-of-domain clean). Don't exclude small datasets ($<$ 20M tokens) from mixtures—they contribute valuable diversity despite non-viability standalone.

\section{Limitations and Threats to Validity}

\textbf{Single Model Family}: All experiments used Qwen3 (0.6B/1.7B/4B). Observations about LR behavior may be architecture- and dataset-specific. Other decoder-only transformers (LLaMA, Gemma, Phi) could behave differently; validation required. Encoder-only (BERT) or encoder-decoder (T5) models may show different mixture effects due to bidirectional attention or different pretraining objectives.

\textbf{Fixed Mixture Strategy}: We used 50cap exclusively. Other algorithms (temperature sampling, equal mixing, DoReMi dynamic weighting) remain unexplored. The 50cap heuristic worked well but may not be optimal—ablation studies varying cap thresholds (30\%, 40\%, 60\%) could reveal improvements. Dynamic mixture strategies that adjust dataset weights during training based on validation loss may outperform static 50cap.

\textbf{Evaluation on Pretraining Distributions}: We evaluated using perplexity on held-out test sets from the same distributions as training data. This measures pretraining quality but doesn't directly assess downstream task performance. Fine-tuned performance on financial NLP tasks (sentiment classification accuracy, Q\&A F1, summarization ROUGE) may differ from pretraining perplexity rankings. Future work should validate that Mixed Financial's pretraining advantage transfers to downstream applications.

\textbf{Hardware Constraints}: Experiments limited to 0.6B-4B models due to available hardware (RTX A6000 48GB, A100 40GB, H100 80GB rented from Lambda Labs). Larger models (7B, 13B, 70B) may show different patterns; mixture benefits may increase or decrease with scale. We did not investigate LR behavior beyond the few follow-ups reported here.

\textbf{Limited hyperparameter search}. We varied learning rates but kept other settings fixed (effective batch size 8, warmup 1000 steps, cosine schedule). Broader sweeps over batch size (4, 8, 16, 32), warmup ratios (1\%, 3\%, 5\%), and schedules (linear, cosine, polynomial) may find better setups. Our compute budget constrained this.

\textbf{Financial domain specificity}. Results may not fully carry to other domains. Legal (very long documents, formal citations) or medical (heavy abbreviations, multimodal) may behave differently. The principles (in‑domain diversity; LR pragmatism) likely carry; exact ratios and settings need domain validation.

Despite these limits, the evidence for mixture effects, training dynamics, and practical choices in financial LM pretraining is strong—and likely useful elsewhere.
