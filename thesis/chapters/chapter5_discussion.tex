\chapter{Discussion}

This chapter interprets the findings from Chapter 4, providing explanations for the observed mixture effects, training dynamics, and generalization patterns. We develop practical guidance from the empirical results while acknowledging limitations. Claims are tied to evidence, and pragmatic choices are clearly identified.

\section{Key Empirical Findings}

Our 10 experiments (30 models, 237 evaluations) lead to four main findings that challenge conventional assumptions about data mixture effects in specialized-domain pretraining. First, \textbf{medium individual datasets consistently outperform mixtures on both performance and consistency}. FiQA (6.80 ppl, 19\% spread), FinGPT (7.03 ppl, 37\% spread), and Alpaca (8.73 ppl, 11.5\% spread) achieve 2.5–3.2$\times$ better perplexity AND 1.5–4.8$\times$ better cross-dataset consistency than Mixed Financial (21.55 ppl, 55\% spread). The mixture hypothesis—that diversity improves robustness—fails empirically. Individual datasets excel on all metrics, challenging the widespread belief that data mixing provides robustness benefits. \Cref{fig:scaling_comparison_all} shows mixture underperformance; cross-dataset tables (\Cref{tab:cross_fingpt,tab:cross_fiqa,tab:cross_alpaca}) demonstrate that individual medium datasets generalize better than mixtures.

Second, simple learning‑rate reductions stabilized a few runs. We used LR=2e‑5 for all main runs; in three configurations (WikiText, Financial QA, Twitter), smaller LRs (e.g., $1\times10^{-5}$ or $5\times10^{-6}$) improved stability and performance. We treat these as pragmatic fixes, not a scaling rule (see \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter}; \Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison}).

Third, \textbf{medium datasets (3.6–8.5M tokens) outperform large datasets (>100M)}. FiQA (3.6M, 6.80 ppl), FinGPT (4.1M, 7.03 ppl), and Alpaca (8.5M, 8.73 ppl) substantially beat News (194M, 32.82 ppl) and SEC (8.1M, 17.80 ppl). This non-monotonic relationship between size and performance suggests data quality, format consistency, and instruction tuning matter more than scale. The best datasets are focused, clean, and task-aligned; larger datasets accumulate noise and format inconsistencies that degrade performance despite greater volume. Small datasets (<1M) still fail due to overtraining (143–352 epochs), but medium scale appears optimal.

Fourth, dataset size shows non-monotonic effects on performance. Datasets >100M tokens undertrain ($<$1 epoch; insufficient data exposure) despite stable curves (\Cref{fig:scaling_news_articles,fig:scaling_sec_reports}); 3.6–8.5M tokens achieve optimal training (12–28 epochs) and best results; <1M tokens overtrain severely (143–352 epochs) with erratic behavior (\Cref{fig:scaling_financial_qa,fig:scaling_twitter}). This epoch-based explanation reveals why medium datasets (SEC, FiQA, FinGPT, Alpaca) outperform large datasets (News, WikiText): optimal epoch count (12–28) enables better learning than undertraining (<1 epoch) or overtraining (>100 epochs). Correlation between log(tokens) and variability is $r=-0.78$ ($p<0.01$), but epoch count matters more than raw size.

Fifth, format drives transfer more than domain vocabulary. Long‑form documents transfer well (News $\leftrightarrow$ SEC: $r=0.82$); instruction tasks cluster (FinGPT/Alpaca/FiQA: $r=0.68$–$0.73$); Twitter is isolated. A News model transfers better to SEC filings (long‑form $\leftrightarrow$ long‑form) than to Twitter (same domain label, different format). This explains why focused medium datasets outperform diverse mixtures: format consistency enables better optimization than format diversity. \Cref{tab:cross_financial_news,tab:cross_financial_report,tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa,tab:cross_twitter} show the diagonals and clusters.

These findings generalize beyond finance to any specialized‑domain pretraining scenario where researchers face similar trade‑offs: domain vs general data, mixture composition, model scaling, and format diversity.

\section{Interpretation of Data Interaction Effects}

\subsection{Why WikiText Underperforms on Financial Tasks}

WikiText's catastrophic financial transfer (41.96 ppl mean vs 21.55 ppl for Mixed Financial) stems from three fundamental mismatches.

First, vocabulary. Financial language contains specialized terminology absent in encyclopedic text. Terms like ``EBITDA'' (earnings before interest, taxes, depreciation, amortization), ``alpha'' (excess returns), ``basis points'' (0.01\%), ``volatility'' (price fluctuation measure), ``hedging'' (risk mitigation strategy), and ``P/E ratio'' (price-to-earnings valuation) rarely appear in Wikipedia. When WikiText models encounter financial evaluation texts, they face effective out-of-vocabulary scenarios despite shared syntactic structure. The model's vocabulary distribution mismatches the evaluation domain's lexical requirements.

Second, reasoning patterns. Financial analysis requires forward-looking causal reasoning: ``Company X's earnings miss will pressure the stock downward'' (cause-effect prediction), ``Rising interest rates typically compress equity valuations'' (conditional reasoning), ``The Fed's hawkish stance suggests tightening ahead'' (implicit reasoning from policy to outcomes). Wikipedia's encyclopedic, descriptive style, focused on established facts, historical narratives, and definitional content, doesn't exercise these prospective reasoning patterns. Models pretrained on WikiText learn to predict continuations based on factual descriptions, not anticipatory financial logic.

Third, discourse structures diverge. Financial news follows inverted pyramid structure (conclusion first, then supporting details); earnings reports have standardized sections (forward-looking statements, risk factors, MD\&A); analyst reports use comparison tables and numerical evidence. Wikipedia articles employ chronological narratives (biographical entries), topical organization (scientific articles), or definitional structures (concept entries). These discourse patterns create different coherence signals. WikiText models learn topic progression and factual elaboration, while financial texts require comparative analysis and evidential reasoning structures.

The asymmetry reveals hierarchical structure. General language (syntax, semantics, discourse coherence) forms a foundation; financial language adds specialized vocabulary and reasoning on top. Starting from general pretraining provides linguistic prerequisites; domain-specific training adds specialization without catastrophic forgetting of fundamentals. Conversely, general pretraining lacks domain prerequisites. Vocabulary and reasoning gaps cannot be bridged by linguistic competence alone. This asymmetry is strikingly visible in \Cref{tab:cross_wikitext}: WikiText training rows show boldface in WikiText columns (4.78-38.60 ppl across model sizes) but poor financial performance (26-58 ppl depending on dataset and LR). Financial training rows show acceptable WikiText performance (27-42 ppl) alongside superior financial metrics. The table's boldface distribution pattern, concentrated in financial rows for most columns, scattered in WikiText rows, quantitatively demonstrates that financial pretraining retains general capability while general pretraining fails to acquire domain specialization.

\subsection{Why Mixtures Fail: The Diversity Paradox}

Contrary to expectations, Mixed Financial (21.55 ppl, 55\% spread) is \textbf{inferior} to medium individual datasets on both performance and consistency. FiQA (6.80 ppl, 19\% spread), FinGPT (7.03 ppl, 37\% spread), and Alpaca (8.73 ppl, 11.5\% spread) achieve 2.5–3.2$\times$ better perplexity AND 1.5–4.8$\times$ better cross-dataset consistency. This finding contradicts the mixture hypothesis and demands explanation.

\textbf{Format Inconsistency Hurts More Than Diversity Helps}. The 7-dataset mixture spans long-form documents (News 194M, SEC 8.1M), instruction formats (FinGPT 4.1M, Alpaca 8.5M, FiQA 3.6M), and short-form text (Twitter 0.28M, Financial QA 0.7M). Rather than preventing overfitting, this format diversity creates conflicting optimization signals. The model must simultaneously optimize for document coherence (News/SEC), instruction following (Alpaca/FinGPT), and conversational Q\&A (FiQA). These objectives compete—attention patterns, discourse structures, and length distributions differ fundamentally across formats. The result: moderate performance on all formats rather than excellence on any.

\textbf{Vocabulary Breadth Trades Depth for Coverage}. Different datasets emphasize different lexical subdomains: News (market events), SEC (regulatory terminology), FinGPT (sentiment vocabulary), Alpaca (financial concepts). While the mixture encounters more unique terms (3.2$\times$ vs individual datasets), this breadth comes at the cost of depth. Individual datasets repeatedly expose the model to focused vocabulary, enabling stronger representations of task-relevant terms. Mixed models spread exposure thin across all terms, learning shallow representations that perform worse on all evaluations.

\textbf{Multi-Task Interference Degrades Performance}. Mixing datasets with different objectives (sentiment, Q\&A, document completion) was hypothesized to act as implicit multi-task learning. Instead, it creates optimization conflicts. Loss gradients from sentiment tasks (predicting ``bullish''/``bearish'') conflict with document completion tasks (predicting technical terminology). The model's parameters cannot simultaneously optimize for these distributions—averaging signals produces suboptimal solutions for each task individually. Format-consistent individual datasets avoid this interference, achieving better performance through focused optimization.

\textbf{The 50cap Strategy Amplifies Large Dataset Noise}. The 50cap strategy caps News at 50\% to prevent dominance, but this still allocates 111M tokens to News—a noisy, heterogeneous dataset (32.82 ppl). Mixing introduces noise from the dominant dataset while diluting signal from high-quality medium datasets. A pure FiQA model trains on 3.6M focused tokens; a mixed model sees only ~2M FiQA tokens diluted with 111M News noise. The noise-to-signal ratio increases, degrading performance.

\textbf{Empirical Variance Analysis Refutes Robustness Claims}. The mixture achieves 55\% relative spread, but individual datasets achieve 11.5–37\% spread—substantially BETTER consistency. Cross-dataset tables confirm this: Alpaca achieves boldface in 6/8 evaluations with 11.5\% spread; Mixed Financial achieves boldface in 2/8 with 55\% spread. The mixture is less robust, not more robust. The hypothesis that diversity improves consistency fails completely.

\textbf{The Task Coverage Argument Remains}. The only valid justification for mixtures is unknown future task requirements. If deployment scenarios are unpredictable and task coverage matters more than optimization quality, mixtures provide baseline capability across diverse tasks. But this is not a robustness advantage—it's a generalist vs specialist trade-off. For known applications, individual datasets are superior.

\subsection{The Optimal Training Regime: Why Medium Datasets Dominate}

The medium dataset superiority reflects a three-tier training regime determined by epoch count and format consistency. **Small datasets** (<1M tokens: Financial QA 0.7M, Twitter 0.28M) overtrain severely (143–352 epochs), causing memorization rather than generalization—models learn dataset-specific artifacts rather than transferable patterns. **Medium datasets** (3.6–8.5M tokens: FiQA 3.6M, FinGPT 4.1M, Alpaca 8.5M, SEC 8.1M) achieve optimal epoch counts (12–28 epochs), providing sufficient repetition for deep learning without memorization, combined with format consistency (instruction-following OR document completion, not both) that enables focused optimization. **Large datasets** (>100M tokens: News 194M, WikiText 124M) undertrain (<1 epoch), providing insufficient exposure despite large vocabulary coverage, and often accumulate heterogeneous noise within single sources (News spans market analysis, earnings reports, opinion pieces with inconsistent discourse patterns).

The complete empirical hierarchy confirms these mechanisms: medium individual datasets (6.80–8.73 ppl) achieve best performance through optimal epochs × format consistency; large mixtures (Mixed Financial 21.55 ppl) suffer from undertraining (<1 epoch per component dataset) but diversity mitigates single-source noise; large individual datasets (News 32.82 ppl) combine undertraining with heterogeneous noise. Critically, Mixed Financial (21.55 ppl) OUTPERFORMS large individual News (32.82 ppl) despite both being undertrained—demonstrating that diversity helps when undertrained. However, medium individual datasets still dominate both, proving that optimal epochs + format consistency beats diversity when training dynamics are suboptimal.

The format inconsistency penalty reflects small model capacity constraints. Our 0.6B–4B parameter models lack sufficient capacity to simultaneously learn diverse formats (long-form documents + conversational Q\&A + short-form tweets), vocabularies (regulatory terminology + sentiment expressions + technical jargon), and discourse patterns (narrative structure + question-answer pairs + abbreviated syntax). Format conflicts create optimization interference—loss gradients from different formats pull parameters in conflicting directions, preventing convergence to optimal solutions for any single format. Larger models (70B+ parameters) might handle mixture diversity better through increased capacity, but this defeats our edge deployment goal of lightweight, privacy-preserving models. The optimal training regime for small models prioritizes **epoch count × format consistency × focused diversity** over broad format coverage.

\subsection{Domain Interference Patterns}

While in-domain diversity helps, cross-domain mixing (Mixed Wiki+Financial) shows interference:

Mixed Wiki+Financial achieves 26.69 ppl (4B), 24\% worse than pure Mixed Financial (21.55 ppl), despite including WikiText. On WikiText specifically, the mixed approach improves performance modestly compared to pure Financial, but mean financial performance degrades notably. The trade-off is unfavorable for finance-focused applications: sacrificing financial performance for a small general-domain gain.

Financial and general domains create conflicting gradients. Financial texts reward predicting domain terminology (``EBITDA'' following ``reported''); general texts reward different continuations (``findings'' following ``reported''). The model's parameters cannot simultaneously optimize for both distributions without compromise. Mixed Wiki+Financial models average these signals, achieving moderate performance on both rather than excellence on either. The 62\% variance (vs 55\% pure financial) reflects this optimization conflict.

\textbf{When Mixing Hurts vs Helps}: Intra-domain mixing helps because datasets share core semantics (financial vocabulary, reasoning patterns) while differing in format and task type, diversity reinforces fundamentals. Cross-domain mixing hurts when domains diverge in vocabulary and reasoning (encyclopedic vs analytical), creating zero-sum trade‑offs. The 50cap strategy mitigates but doesn't eliminate interference: capping WikiText at 50\% limits damage but still dilutes financial specialization. This distinction is evident comparing \Cref{tab:mixed_financial_results} (pure financial mixture) and \Cref{tab:mixed_wiki_financial_results} (cross-domain mixture): the former shows consistently lower perplexity across all financial evaluation datasets, with the performance advantage increasing at larger model sizes. \Cref{fig:scaling_mixed_financial,fig:scaling_mixed_wiki_financial} visually confirm this, the pure financial mixture (first figure) shows steeper slope (22.6\% total improvement) compared to Wiki+Financial (second figure, 15.1\% improvement), indicating that domain conflict reduces scaling efficiency.

\textbf{Practical Implication}: For specialized applications, domain purity wins. Only mix cross-domain when explicit general-domain retention is required (e.g., conversational agents handling both financial and general queries). For finance-focused deployments, pure in-domain mixtures maximize performance.

\subsection{Scale-Dependent Training Notes}

Our experience suggests that larger models can be more sensitive to optimization settings on some datasets. While we kept LR=2e-5 for main runs, reducing LR in a handful of follow-ups helped stabilize training. We do not claim a general rule beyond this observation.

\section{Practical Guidelines for Financial LM Pretraining}

Synthesizing experimental findings into actionable recommendations:

\subsection{Data Mixture Strategies by Use Case}

\textbf{Task-Specific Financial Applications (RECOMMENDED)}: Use individual medium datasets for optimal performance and consistency. FiQA (6.80 ppl, 19\% spread) for Q\&A, FinGPT (7.03 ppl, 37\% spread) for sentiment, Alpaca (8.73 ppl, 11.5\% spread) for instruction-following. These achieve 2.5–3.2$\times$ better perplexity AND 1.5–4.8$\times$ better consistency than mixtures. Cross-dataset tables show individual datasets excel: Alpaca achieves 6/8 boldface, FiQA 5/8, FinGPT 4/8, versus Mixed Financial 2/8. For any known application, individual datasets are superior.

\textbf{Unknown Task Coverage (ONLY IF NECESSARY)}: Use Mixed Financial (21.55 ppl, 55\% spread) ONLY when future task requirements are completely unpredictable and task coverage matters more than optimization quality. The mixture provides baseline capability across diverse tasks but is inferior to individual datasets on both performance and consistency. This is a generalist vs specialist trade-off, not a robustness advantage. \Cref{fig:scaling_comparison_all} shows mixture underperformance—individual dataset curves lie substantially below.

\textbf{Specialized Document Analysis}: Use single large dataset if available ($>$ 100M tokens). SEC @ 4B (15.91 ppl on SEC; \~19\% relative spread across evaluations) excels for regulatory filing analysis; News @ 4B (17.47 ppl on News; \~66\% relative spread) excels for journalism. Specialization improves in-domain performance but sacrifices cross-format transfer. \Cref{fig:scaling_news_articles,fig:scaling_sec_reports} show these datasets maintain stable scaling without requiring LR adjustments. However, \Cref{tab:cross_financial_news,tab:cross_financial_report} reveal that News and SEC training rows achieve boldface primarily within document-format columns, confirming limited format diversity.

For instruction-following and Q\&A applications, use FiQA (3.6M tokens, 16.35 ppl) or FinGPT (4.1M tokens, 19.83 ppl) for specialized Q\&A, or include in mixture for general applications. Instruction formats transfer moderately within task type ($r = 0.68-0.73$) but poorly to documents. The instruction-following tables (\Cref{tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa}) show boldface clustering along the diagonal and adjacent instruction rows, visualizing the format-based transfer limitation.

\textbf{Balanced General + Financial Capabilities}: Use Mixed Wiki+Financial only if general-domain retention is explicitly required (e.g., chatbots handling both financial and general queries). Accepts 24\% financial performance cost for 16\% general improvement, unfavorable for finance-focused deployments. \Cref{fig:scaling_mixed_wiki_financial} shows reduced slope compared to pure financial mixture, and \Cref{tab:mixed_wiki_financial_results} documents the performance cost across all financial evaluation datasets.

\textbf{Avoid}: Pure WikiText for financial applications (2.0$\times$ performance degradation vs Mixed Financial on average across financial tasks), small individual datasets $<$ 20M tokens (non-viable standalone due to severe overtraining and high variability), single-format training when diverse tasks are expected (format mismatch prevents transfer). \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} provide visual evidence: WikiText requires LR adjustment and still shows poor financial transfer, while small datasets exhibit brittleness visible in both scaling curves and cross‑dataset table patterns.

\subsection{Model Size Selection}

\textbf{0.6B Models}: Fast training ($\sim$6 hours for 100M tokens on Lambda Labs GPUs), low memory (4GB), suitable for rapid prototyping. Performance is acceptable for exploratory work, but variability is high (Mixed Financial: \~98\% relative spread). Use for development, experimentation, or extremely resource-constrained deployment (mobile devices).

\textbf{1.7B Models}: Best performance-efficiency balance. Training moderate ($\sim$12 hours), memory reasonable (10GB), performance strong with improved consistency vs 0.6B (Mixed Financial: \~63\% relative spread). Recommended for most applications, strong performance at substantially lower resource cost than 4B. Optimal for production deployment balancing quality and resource constraints.

\textbf{4B Models}: Best absolute performance (21.55 ppl, 55\% relative spread) but requires careful hyperparameter tuning (LR $5 \times 10^{-6}$ in affected cases) and substantial resources (20GB memory, $\sim$24 hours training). Use when maximizing performance justifies cost, and when expertise for hyperparameter tuning is available. Critical: failure to tune learning rate can cause reverse scaling, one may need to reduce LR substantially at larger scales.

\textbf{Scaling Decision Tree}: In short, choose 0.6B under tight resource constraints (mobile, edge) and accept about a 22\% loss versus 4B; pick 1.7B for the best performance–efficiency balance (about 92\% of 4B at roughly half the resources); go to 4B only when maximum performance justifies the tuning effort.

\subsection{Learning Rate Notes}

We used LR=2e-5 for all main runs. In three follow-ups with anomalies (WikiText, Financial QA, Twitter), smaller LRs (e.g., $1\times10^{-5}$ or $5\times10^{-6}$) stabilized training. These are practical notes from our setup, not prescriptive guidelines for other contexts.

\subsection{Token Budget Allocation}

A 100M token budget proved sufficient when properly mixed, showing diminishing returns beyond that threshold for our 0.6B-4B models. Larger models ($>$ 7B) may benefit from extended training (200-500M tokens), but we did not test this.

We used 50cap to prevent dominance: if the largest dataset exceeds 50\% of the total, cap it there and sample others proportionally. This ensures diversity while respecting relative dataset informativeness. During training, sample each batch from the mixture distribution at the token level, not by example or epoch. Sequential exposure can cause catastrophic forgetting; token-level interleaving avoids that.

When curating datasets, format diversity comes first, followed by size (aim for $\geq$ 100M total), then quality. In-domain noisy data beats out-of-domain clean data in our runs. Don't exclude small datasets ($<$ 20M tokens) from mixtures; they contribute valuable diversity despite non-viability as standalone sources.

\section{Limitations and Threats to Validity}

All experiments used Qwen3 (0.6B/1.7B/4B). Observations about LR behavior may be architecture- and dataset-specific. Other decoder-only transformers (LLaMA, Gemma, Phi) could behave differently; validation required. Encoder-only (BERT) or encoder-decoder (T5) models may show different mixture effects due to bidirectional attention or different pretraining objectives.

We used 50cap exclusively. Other algorithms (temperature sampling, equal mixing, DoReMi dynamic weighting) remain unexplored. The 50cap heuristic worked well but may not be optimal. Ablation studies varying cap thresholds (30\%, 40\%, 60\%) could reveal improvements. Dynamic mixture strategies that adjust dataset weights during training based on validation loss may outperform static 50cap.

We evaluated using perplexity on held-out test sets from the same distributions as training data. This measures pretraining quality but doesn't directly assess downstream task performance. Fine-tuned performance on financial NLP tasks (sentiment classification accuracy, Q\&A F1, summarization ROUGE) may differ from pretraining perplexity rankings. Future work should validate that Mixed Financial's pretraining advantage transfers to downstream applications.

Experiments were limited to 0.6B-4B models due to available hardware (RTX A6000 48GB, A100 40GB, H100 80GB rented from Lambda Labs). Larger models (7B, 13B, 70B) may show different patterns; mixture benefits may increase or decrease with scale. We did not investigate LR behavior beyond the few follow-ups reported here.

We systematically explored learning rates but kept other hyperparameters fixed (effective batch size 8, warmup 1000 steps, cosine schedule). Larger hyperparameter sweeps over batch size (4, 8, 16, 32), warmup ratios (1\%, 3\%, 5\%), and schedules (linear, cosine, polynomial) may reveal better configurations. Computational budget constraints prevented exhaustive search.

Results may not generalize to other specialized domains with different characteristics. Legal text (extremely long documents, formal citations) or medical text (heavy abbreviations, multimodal integration) may show different mixture effects. The core principles (in-domain diversity, and our LR heuristics) may generalize, but specific mixture ratios and optimal configurations require domain-specific validation.

Despite these limitations, our findings provide solid empirical evidence for data mixture effects, training dynamics, and practical practices applicable to financial LM pretraining and likely informative for other specialized domains.
