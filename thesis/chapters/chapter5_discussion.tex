\chapter{Discussion}

In this chapter, we go back to the results in Chapter~4 and explain them in a plain way: what causes the mixture effects, how scaling behaves, and how the transfer patterns appear. Put another way, we try to say what the numbers really mean. After that, we give practical guidance and say clearly where our study has limits.

\section{Key Empirical Findings}

Across 10 experiments (30 models, 240 evaluations), four findings stand out. We state them directly first, then explain.

\textbf{Finding 1: In-domain diversity beats general quality.}

Mixed Financial reaches 21.55 ppl (4B) with 55\% variance. WikiText @ 4B averages about 32 ppl on financial sets with 26\% relative spread, a ~1.5$\times$ gap. Many in-domain datasets, even small (Twitter 0.3M) or a bit noisy, make stronger domain specialization than one large general corpus. So the idea that high quality general text is enough does not hold here. \Cref{fig:scaling_comparison_all} shows this gap grows from 0.6B to 4B. The cross-dataset tables (\Cref{tab:cross_financial_news,tab:cross_financial_qa,tab:cross_fingpt,tab:cross_fiqa}) tell the same story: Mixed Financial rows often get boldface; WikiText rows rarely do outside its own domain.

\textbf{Finding 2: Small LR cuts fixed a few runs.}

All main runs used LR = 2e-5. Three cases acted badly (WikiText, Financial QA, Twitter). Lowering LR (to $1\times10^{-5}$ or $5\times10^{-6}$) made training stable and improved results. These are practical fixes in our setup, not a rule for everyone. Solid vs dashed lines in \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} show the change; \Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison} list the numbers.

\textbf{Finding 3: Size decides standalone viability.}

Thresholds are clear. More than 100M tokens: standalone works (2--5 epochs) and generalizes more consistently. 20--100M: still works but with caveats (6--30 epochs). Less than 20M: not viable alone (67--249 epochs), 89--97\% variance. Correlation log(tokens) vs variance: $r = -0.78$ ($p < 0.01$). Put another way, the limit is data size, not only hyperparameters. Large sets show smooth scaling (\Cref{fig:scaling_news_articles,fig:scaling_sec_reports}); small sets look jumpy and often need LR changes (\Cref{fig:scaling_financial_qa,fig:scaling_twitter}). Cross-dataset tables (\Cref{tab:cross_financial_qa,tab:cross_twitter}) show brittleness: boldface appears only in the dataset's own column.

\textbf{Finding 4: Format beats vocabulary for transfer.}

Document format and task type predict transfer better than topic domain. Long-form (News $\leftrightarrow$ SEC: $r = 0.82$) transfers well. Instruction tasks cluster (FinGPT/Alpaca/FiQA: $r = 0.68$--$0.73$). Twitter is isolated (89\% variance). A News model transfers to SEC better than to Twitter, even though Twitter is ``finance''. So pretraining corpora should include format diversity, not only domain coverage. Still, the exact mix depends on use. The cross-dataset tables show diagonal boldface in long-form (\Cref{tab:cross_financial_news,tab:cross_financial_repor}) and in instruction clusters (\Cref{tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa}). \Cref{tab:cross_twitter} shows isolation: boldface only in Twitter's own column.

These points likely generalize outside finance when we face similar trade-offs: domain vs general data, mixture composition, model scaling, and format diversity. Local details can still vary by domain.

\section{Interpretation of Data Interaction Effects}

\subsection{Why WikiText Underperforms on Financial Tasks}

WikiText's very poor financial transfer (about 32 ppl mean vs 21.55 ppl for Mixed Financial) comes from three basic mismatches:

\textbf{1. Vocabulary gap}. Financial text uses many special terms that encyclopedic text does not cover well. Examples include ``EBITDA'' (earnings before interest, taxes, depreciation, amortization), ``alpha'' (excess returns), ``basis points'' (0.01\%), ``volatility'' (price fluctuation measure), ``hedging'' (risk mitigation strategy), and ``P/E ratio'' (price-to-earnings valuation). When WikiText-pretrained models face financial evaluation text, they meet effective out-of-vocabulary cases even if the syntax is similar. The vocabulary distribution learned does not match the financial domain need.

\textbf{2. Reasoning pattern mismatch}. Financial analysis needs forward-looking causal thinking: for example, ``Company X's earnings miss will pressure the stock downward'' (cause-effect), or ``Rising interest rates typically compress equity valuations'' (conditional), or reading policy signals like ``The Fed's hawkish stance suggests tightening ahead''. Wikipedia is encyclopedic and descriptive. It focuses on facts and definitions. It does not train these prospective patterns. So WikiText models learn to continue factual description, not anticipatory financial logic.

\textbf{3. Discourse structure divergence}. Financial news often uses an inverted pyramid (conclusion first). Earnings reports have fixed sections (forward-looking statements, risk factors, MD\&A). Analyst notes include tables and many numbers. Wikipedia uses other structures: time order, topic groups, or definitions. These patterns give different coherence signals. WikiText models learn topic progression and elaboration. But financial text needs comparison and evidence-driven reasoning.

\textbf{Why General to Financial fails but Financial to General works}. The asymmetry (WikiText @ 4B: about 32 ppl on financial evaluation sets; Mixed Wiki+Financial @ 4B: 27.72 ppl on WikiText) suggests a hierarchy. General language (syntax, semantics, coherence) is the base. Financial adds extra vocabulary and reasoning on top. If we start with general pretraining and then add domain data, we keep the base and gain specialization. But starting from general alone cannot fill the domain gaps. Not enough. \Cref{tab:cross_wikitext} shows this pattern. WikiText training rows get boldface in the WikiText columns (9--32 ppl after LR change) but do badly on financial (40--60 ppl, rarely bold). Financial training rows keep acceptable WikiText performance (30--35 ppl) and also do better on financial metrics. In short, boldface concentrates in financial rows across most columns, and is scattered in WikiText rows.

\subsection{Benefits of In-Domain Diversity}

Mixed Financial works better (21.55 ppl, 55\% CV) than single datasets (mean 24.8 ppl, 65\% CV) and WikiText (about 32 ppl on financial, 26\% CV at 4B). A simple reason: diversity builds consistency.

\textbf{Cross-format exposure}. The 7-dataset mix covers long documents (News 197M, SEC 80M), instruction formats (FinGPT 19M, Alpaca 17M, FiQA 4M), and short text (Twitter 0.3M, Financial QA 3.5M). This format spread reduces overfitting to one structure. Pure News training learns long-form coherence but fails on dialog Q\&A (41\% worse on FiQA). And the mixed model handles both, with about 30\% average drop across formats.

\textbf{Vocabulary coverage}. Different financial datasets stress different words: News has market events and company names; SEC has regulatory terms (``10-K'', ``forward-looking statements''); FinGPT has sentiment words (``bullish'', ``bearish''); Alpaca has finance concepts (``compound interest'', ``diversification''). The mixture gives much broader coverage. Mixed models encounter 3.2$\times$ more unique financial terms during training than the largest individual dataset (News), which helps reduce brittleness.

\textbf{Task-diversity regularization}. Mixing tasks (sentiment, Q\&A, document completion) acts like implicit multi-task learning. The model is less able to overfit to any one task's surface cues (like sentiment keywords in FinGPT or formulaic question patterns in Alpaca) because the loss averages across different data. Put another way, the model learns underlying financial semantics, not just shortcuts.

\textbf{Preventing data memorization}. Small datasets tend to memorize. Financial QA (3.5M tokens, 67--100 epochs) gets 8.09 ppl in-domain but 41.7 ppl cross-dataset. That looks like memorization, not generalization. Mixing helps by capping each dataset's share (50cap limits News to 50\% so others still appear) and by diversifying batches. Mixed models see fewer repeats from any one source, so they need to learn transferable features.

\textbf{Quantitative evidence}. Variance drops as mixture diversity rises: 7-dataset mix (55\% CV) < largest single set (News: 26\% CV in-domain, 65\% cross) < small singles (89--97\% CV). The mixture shows 12.7\% lower variance than same-scale single-dataset training and better ppl (21.55 vs 24.8). When we read the cross-dataset tables, we see a simple pattern: Mixed Financial rows get boldface across many columns; single-dataset rows mostly get boldface only near their own format.

\subsection{Domain Interference Patterns}

While in-domain diversity helps, cross-domain mixing (Mixed Wiki+Financial) shows interference:

\textbf{Performance--diversity trade-off}. Mixed Wiki+Financial reaches 26.69 ppl (4B), which is 24\% worse than pure Mixed Financial (21.55 ppl), even though it includes WikiText. On WikiText itself, Wiki+Financial gets 27.72 ppl; the pure financial mixture was not evaluated on WikiText. Meanwhile, mean financial performance drops from 20.2 to 26.1 ppl (29.2\% worse). The trade-off is not good for finance-focused use: lose 29\% financial to gain 16\% general.

\textbf{Competing optimization signals}. Financial and general domains push in different directions. Financial text rewards domain terms (``EBITDA'' after ``reported''); general text rewards other words (like ``findings'' after ``reported''). One set of parameters cannot perfectly do both at once. Mixed Wiki+Financial averages these signals and ends up medium on both. Still, if you need general retention, this may be acceptable. The 62\% variance (vs 55\% for pure financial) also shows this conflict.

\textbf{When mixing hurts vs helps}. Intra-domain mixing helps because the sets share core semantics (financial vocabulary and reasoning) but differ in format and task. Diversity then strengthens the fundamentals. Cross-domain mixing can hurt when the vocabulary and reasoning styles diverge (encyclopedic vs analytical), creating a trade-off. The 50cap rule reduces but does not remove the interference: capping WikiText at 50\% limits the damage but still dilutes specialization. Compare \Cref{tab:mixed_financial_results} and \Cref{tab:mixed_wiki_financial_results}: the pure financial mix shows lower perplexity on all financial evaluations, and the advantage grows for larger models. \Cref{fig:scaling_mixed_financial,fig:scaling_mixed_wiki_financial} show the same trend—the pure financial mixture has a steeper slope (22.6\% total improvement) than Wiki+Financial (15.1\% improvement).

\textbf{Practical implication}. For specialized finance applications, keep the domain pure. Mix cross-domain only when you must keep general-domain ability (for example, a chatbot that handles both financial and general queries). For finance-only use, pure in-domain mixtures work best. Simple and effective.

\subsection{Scale-Dependent Training Notes}

In our runs, larger models sometimes were more sensitive to optimization settings for some datasets. We kept LR = 2e-5 for main runs, but reducing LR in a few follow-ups stabilized training. We do not claim a broader rule beyond this observation.

\section{Practical Guidelines for Financial LM Pretraining}

We now turn the findings into actions.

\subsection{Data Mixture Strategies by Use Case}

\textbf{General-purpose financial NLP}. Use Mixed Financial (7 datasets, 50cap). It gives the best all-around results (21.55 ppl, 55\% CV) with stable cross-task behavior. Suitable for many tasks: sentiment, document summarization, Q\&A, information extraction. \Cref{fig:scaling_mixed_financial,fig:scaling_comparison_all} show reliable scaling and consistent gains. The cross-dataset tables also support this: Mixed Financial rows take boldface more often than any single dataset across all eight evaluations.

\textbf{Specialized document analysis}. Use a single large dataset if available ($>$ 100M tokens). SEC @ 4B (15.91 ppl on SEC) is strong for filings; News @ 4B (17.47 ppl on News) is strong for journalism. Specialization helps in-domain but reduces cross-format transfer. \Cref{fig:scaling_news_articles,fig:scaling_sec_reports} show stable scaling without LR changes. \Cref{tab:cross_financial_news,tab:cross_financial_repor} show News and SEC rows get boldface mainly within their own format columns.

\textbf{Instruction-following / Q\&A}. Use FiQA (4M tokens, 7.08 ppl) or FinGPT (19M tokens, 5.67 ppl) for focused Q\&A, or include them in the mix for general use. Instruction formats transfer moderately within the same task type ($r = 0.68$--$0.73$) but poorly to long documents. \Cref{tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa} show boldface clustering on the diagonal and nearby instruction rows.

\textbf{Balanced general + financial}. Use Mixed Wiki+Financial only if you truly need general-domain retention (for example, a chatbot handling both financial and general queries). It pays a 24\% financial cost for a 16\% general gain, which is not good for finance-focused use. \Cref{fig:scaling_mixed_wiki_financial} shows a smaller slope than the pure financial mixture, and \Cref{tab:mixed_wiki_financial_results} lists the costs across all financial evaluations.

\textbf{Avoid}. Pure WikiText for financial tasks (2.3$\times$ worse), small single datasets $<$ 20M tokens (89--97\% variance, not viable alone), and training on one format when many tasks are expected. \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} show this clearly: WikiText needs heavy LR changes and still transfers poorly; small sets are brittle in both scaling curves and cross-dataset tables.

\subsection{Model Size Selection}

\textbf{0.6B models}. Fast training ($\sim$6 hours for 100M tokens on Lambda Labs GPUs), low memory (4GB). Good for quick prototypes. If you just need a baseline, this size is convenient. Performance is acceptable (130.30 ppl Mixed Financial average across evaluation sets) but variance is high (63\% CV).

\textbf{1.7B models}. Best balance of performance and efficiency. Training time is moderate ($\sim$12 hours), memory is reasonable (10GB), performance is strong (34.49 ppl Mixed Financial average), 58\% CV. About 92\% of 4B's quality with 2.4$\times$ less memory and training about 2$\times$ faster. In practice, we recommend this for most cases.

\textbf{4B models}. Best absolute performance (21.55 ppl, 55\% CV) but needs careful tuning (LR $5 \times 10^{-6}$) and more resources (about 20GB memory, $\sim$24 hours training). Use when the higher cost is acceptable and tuning expertise is available. To be fair, if LR is not reduced from the 0.6B baseline (by about 75\%), reverse scaling can appear.

\textbf{Scaling decision tree}:
\begin{enumerate}
\item \textbf{Resource-constrained} (mobile, edge devices): choose 0.6B, accept about 22\% loss vs 4B
\item \textbf{Balanced production}: choose 1.7B, about 92\% of 4B quality with about half the resources
\item \textbf{Performance-critical}: choose 4B, but plan LR tuning effort
\end{enumerate}

\subsection{Learning Rate Notes}

\textbf{Main setting}. $2 \times 10^{-5}$ across all primary experiments.

\textbf{Follow-ups}. For the few runs with anomalies, we used smaller LRs (e.g., $1\times10^{-5}$ or $5\times10^{-6}$) to stabilize training.

\textbf{Scope}. These are practical notes from our setup, not strict rules.

\subsection{Token Budget Allocation}

\textbf{Optimal token budget}. 100M tokens is enough when mixed properly across diverse datasets. For 0.6B--4B models in our experiments, returns get smaller beyond that. Larger models ($>$ 7B) might benefit from 200--500M tokens, but we did not test that here.

\textbf{Mixture composition}. Use the 50cap strategy to prevent dominance. For $n$ datasets with sizes $\{s_1, s_2, ..., s_n\}$ where $s_1 > 0.5 \sum_i s_i$, cap $s_1$ at 50\% of total and sample others proportionally. This keeps diversity while still respecting relative informativeness.

\textbf{Sampling strategy}. Do token-level interleaving, not batch-level or epoch-level. Sample each batch from the mixture distribution with probabilities proportional to (capped) sizes. This avoids long sequential exposure that can cause forgetting.

\textbf{Dataset prioritization}. When curating datasets, prioritize: (1) format diversity (documents, Q\&A, dialogue), (2) total size (aim $\geq$ 100M across sources), (3) quality (clean text preferred, but in-domain noisy is better than out-of-domain clean). Do not remove small datasets ($<$ 20M tokens) from mixtures; they still add useful diversity even if not viable alone.

\section{Limitations and Threats to Validity}

\textbf{Single model family}. All experiments used Qwen3 (0.6B/1.7B/4B). LR behavior can depend on architecture and data. Other decoder-only transformers (LLaMA, Gemma, Phi) may behave differently. Encoder-only (BERT) or encoder--decoder (T5) could show different mixture effects due to their objectives. We admit this is a narrow slice.

\textbf{Fixed mixture strategy}. We used 50cap only. Other algorithms (temperature sampling, equal mixing, DoReMi) were not explored. 50cap worked well for us but may not be optimal. Changing caps (30\%, 40\%, 60\%) or dynamic weighting by validation loss might help. Still, we did not test them.

\textbf{Evaluation on pretraining distributions}. We used perplexity on held-out test sets from the same distributions. This measures pretraining quality but not downstream task performance directly. Fine-tuned results (e.g., sentiment accuracy, Q\&A F1, summarization ROUGE) may differ from perplexity ranks. Future work should check how Mixed Financial's advantage transfers.

\textbf{Hardware constraints}. We limited to 0.6B--4B models due to hardware (RTX A6000 48GB, A100 40GB, H100 80GB rented from Lambda Labs). Larger models (7B, 13B, 70B) may show different patterns; mixture benefits could change with scale. We did not study LR behavior beyond the few follow-ups above.

\textbf{Limited hyperparameter search}. We varied learning rates but kept other settings fixed (effective batch size 8, warmup 1000 steps, cosine schedule). Broader sweeps over batch size (4, 8, 16, 32), warmup ratios (1\%, 3\%, 5\%), and schedules (linear, cosine, polynomial) may find better setups. Our compute budget limited this.

\textbf{Financial domain specificity}. Results may not carry fully to other domains. Legal (very long documents, formal citations) or medical (heavy abbreviations, multimodal) may behave differently. The general ideas (in-domain diversity; LR pragmatism) likely carry; exact ratios and settings need to be validated per domain.

Even with these limits, the evidence for mixture effects, training dynamics, and practical choices in financial LM pretraining is strong. And likely useful elsewhere.
