\chapter{Discussion}

This chapter interprets the findings from Chapter 4, and provides explanations for the observed mixture effects, training dynamics, and generalization patterns.

\section{Key Empirical Findings}

Our 10 experiments (36 models, 288 evaluations) lead to five main findings that challenge conventional assumptions about data mixture effects in specialized-domain pretraining. First, \textbf{medium individual datasets consistently outperform mixtures on both performance and consistency}. FiQA (6.80 ppl, 19\% spread), FinGPT (7.03 ppl, 37\% spread), and Alpaca (8.73 ppl, 11.5\% spread) achieve 2.5–3.2$\times$ better perplexity and 1.5–4.8$\times$ better cross-dataset consistency than Mixed Financial (21.55 ppl, 55\% spread). The mixture hypothesis—that diversity improves robustness—fails empirically. Individual datasets excel on all metrics, challenging the widespread belief that data mixing provides robustness benefits. \Cref{fig:scaling_comparison_all} shows mixture underperformance. Cross-dataset tables (\Cref{tab:cross_fingpt,tab:cross_fiqa,tab:cross_alpaca}) demonstrate that individual medium datasets generalize better than mixtures.

Second, \textbf{simple learning‑rate reductions stabilized a few runs}. We used LR=2e‑5 for all main runs. In three configurations (WikiText, Financial QA, Twitter), smaller LRs (e.g., $1\times10^{-5}$ or $5\times10^{-6}$) improved stability and performance. These heuristic rules help us stabilize training (see \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter}; \Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison}).

Third, \textbf{medium datasets (3.6–8.5M tokens) outperform large datasets ($>$100M)}. FiQA (3.6M, 6.80 ppl), FinGPT (4.1M, 7.03 ppl), and Alpaca (8.5M, 8.73 ppl) substantially beat News (194M, 32.82 ppl) and SEC (8.1M, 17.80 ppl). This non-monotonic relationship between size and performance suggests data quality, format consistency, and instruction tuning matter more than scale. The best datasets are focused, clean, and task-aligned, as larger datasets accumulate noise and format inconsistencies that degrade performance despite greater volume. Small datasets ($<$1M) still fail due to overtraining (143–352 epochs), but medium scale appears optimal.

Fourth, \textbf{dataset size shows non-monotonic effects on performance}. We argue that datasets $>$100M tokens are undertrained ($<$1 epoch; insufficient data exposure) despite stable curves (\Cref{fig:scaling_news_articles,fig:scaling_sec_reports}); 3.6–8.5M tokens achieve optimal training (12–28 epochs) and best results, while $<$1M tokens overtrain severely (143–352 epochs) with erratic behavior (\Cref{fig:scaling_financial_qa,fig:scaling_twitter}). This epoch-based explanation reveals why medium datasets (SEC, FiQA, FinGPT, Alpaca) outperform large datasets (News, WikiText): optimal epoch count (12–28) enables better learning than undertraining ($<$1 epoch) or overtraining ($>$100 epochs). A reasonable training scheme of 12-28 epochs matters more than raw size.

Fifth, \textbf{format drives transfer more than domain vocabulary}. Long‑form documents transfer well (News $\leftrightarrow$ SEC); instruction tasks cluster (FinGPT/Alpaca/FiQA); Twitter is isolated. A News model transfers better to SEC filings (long‑form $\leftrightarrow$ long‑form) than to Twitter (same domain label, different format). This explains why focused medium datasets outperform diverse mixtures: format consistency enables better optimization than format diversity. \Cref{tab:cross_financial_news,tab:cross_financial_report,tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa,tab:cross_twitter} show the diagonals and clusters.

These findings generalize beyond finance to any specialized‑domain pretraining scenario where researchers face similar trade‑offs: domain vs general data, mixture composition, model scaling, and format diversity.

Besides, our experience suggests that larger models can be more sensitive to optimization settings on some datasets. While we kept LR=2e-5 for main runs, reducing LR in a handful of follow-ups helped stabilize training. We do not claim a general rule beyond this observation.

\section{Practical Guidelines for Financial LM Pretraining}

We summarize our findings during experiments into the following points:

\subsection{Data Mixture Strategies by Use Case}

\textbf{Task-Specific Financial Applications}: Use individual medium datasets for optimal performance and consistency. FiQA (6.80 ppl, 19\% spread) for Q\&A, FinGPT (7.03 ppl, 37\% spread) for sentiment, Alpaca (8.73 ppl, 11.5\% spread) for instruction-following. These achieve 2.5–3.2$\times$ better perplexity and 1.5–4.8$\times$ better consistency than mixtures. Cross-dataset tables show individual datasets excel: Alpaca achieves 6/8 boldface, FiQA 5/8, FinGPT 4/8, versus Mixed Financial 2/8. For any known application, individual datasets are superior.

\textbf{Unknown Task Coverage}: Use Mixed Financial (21.55 ppl, 55\% spread) ONLY when future task requirements are completely unpredictable and task coverage matters more than optimization quality. The mixture provides baseline capability across diverse tasks but is inferior to individual datasets on both performance and consistency. \Cref{fig:scaling_comparison_all} shows mixture underperformance—individual dataset curves lie substantially below.

\textbf{Specialized Document Analysis}: Use single large dataset if available ($>$ 100M tokens). SEC @ 4B (15.91 ppl on SEC; \~19\% relative spread across evaluations) excels for regulatory filing analysis; News @ 4B (17.47 ppl on News; \~66\% relative spread) excels for journalism. Specialization improves in-domain performance but sacrifices cross-format transfer. \Cref{fig:scaling_news_articles,fig:scaling_sec_reports} show these datasets maintain stable scaling without requiring LR adjustments. However, \Cref{tab:cross_financial_news,tab:cross_financial_report} reveal that News and SEC training rows achieve boldface primarily within document-format columns, confirming limited format diversity.

For instruction-following and Q\&A applications, use FiQA (3.6M tokens, 16.35 ppl) or FinGPT (4.1M tokens, 19.83 ppl) for specialized Q\&A, or include in mixture for general applications. Instruction formats transfer moderately within task type ($r = 0.68-0.73$) but poorly to documents. The instruction-following tables (\Cref{tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa}) show boldface clustering along the diagonal and adjacent instruction rows, visualizing the format-based transfer limitation.

\textbf{Balanced General + Financial Capabilities}: Use Mixed Wiki+Financial only if general-domain performance is explicitly required (e.g., chatbots handling both financial and general queries). \Cref{fig:scaling_mixed_wiki_financial} shows reduced slope compared to pure financial mixture, and \Cref{tab:mixed_wiki_financial_results} documents the performance cost across all financial evaluation datasets.

% \textbf{Avoid}: Pure WikiText for financial applications (2.0$\times$ performance degradation vs Mixed Financial on average across financial tasks), small individual datasets $<$ 20M tokens (non-viable standalone due to severe overtraining and high variability), single-format training when diverse tasks are expected (format mismatch prevents transfer). \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} provide visual evidence: WikiText requires LR adjustment and still shows poor financial transfer, while smaller datasets exhibit brittleness visible in both scaling curves and cross‑dataset table patterns.

\subsection{Model Size Selection}

\textbf{0.6B Models}: Fast training ($\sim$6 hours for 100M tokens on Lambda Labs GPUs), low memory (4GB), suitable for rapid prototyping. Performance is acceptable for exploratory work, but variability is high (Mixed Financial: \~98\% relative spread). We should only use this small model for development, experimentation, or extremely resource-constrained deployment (for example, on mobile devices).

\textbf{1.7B Models}: Best performance-efficiency balance. Training moderate ($\sim$12 hours), memory reasonable (10GB), performance strong with improved consistency vs 0.6B (Mixed Financial: \~63\% relative spread). We recommend models of similar sizes for most applications, for their strong performance at substantially lower resource cost than 4B. We believe these models are optimal for production deployment balancing quality and resource constraints.

\textbf{4B Models}: Best absolute performance (21.55 ppl, 55\% relative spread) but requires careful hyperparameter tuning (LR $5 \times 10^{-6}$ in affected cases) and substantial resources (20GB memory, $\sim$24 hours training). Using such models should only be possible when one wants to maximize performance over cost, and when sufficient compute resources for hyperparameter tuning is available. We observe that failure to tune learning rate can cause reverse scaling, one may need to reduce LR substantially at larger scales.

\subsection{Token Budget Allocation}

A 100M token budget proved sufficient when we choose the pretraining dataset setup properly. We suspect that larger models with larger datasets may benefit from extended training (200-500M tokens), but we did not test this for limited compute.

We used 50cap to prevent a single dataset dominating the mixture: if the largest dataset exceeds 50\% of the total, we cap it there and sample others proportionally. This ensures diversity while respecting relative dataset informativeness.