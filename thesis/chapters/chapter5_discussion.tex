\chapter{Discussion}

This chapter interprets the experimental findings from Chapter 4, explaining mechanisms behind data mixture effects, training dynamics, and generalization patterns. We synthesize empirical observations into actionable guidelines and acknowledge methodological limitations. Still, we keep conclusions tied to our evidence. Where the data are thin, we say so.

\section{Key Empirical Findings}

Our 10 experiments (30 models, 237 evaluations) lead to four main findings about data mixture effects in specialized-domain language model pretraining:

\textbf{Finding 1: In-Domain Diversity Outperforms General Corpus Quality}

Mixed Financial datasets achieved 21.55 ppl (4B) with 55\% relative spread, substantially better than WikiText's 41.96 ppl mean financial performance (\~53\% relative spread, after LR adjustment). This 1.95$\times$ performance gap demonstrates that multiple in-domain datasets—even if individually small (Twitter 0.3M tokens) or noisy (social media text)—provide superior domain specialization compared to large, curated general corpora. The result challenges conventional wisdom that high-quality general pretraining suffices for domain adaptation. \Cref{fig:scaling_comparison_all} visually confirms this hierarchy: the performance gap between Mixed Financial (blue line) and WikiText (green line) widens from 0.6B to 4B, indicating that in-domain diversity scales better than general quality. The cross-dataset tables (\Cref{tab:cross_financial_news,tab:cross_financial_qa,tab:cross_fingpt,tab:cross_fiqa}) further validate this through boldface patterns—Mixed Financial rows consistently capture best-performance positions across evaluation datasets, while WikiText rows rarely achieve boldface except in their own domain.

\textbf{Finding 2: Simple LR Reductions Stabilized a Few Runs}

All main runs used LR=2e-5. In three configurations that showed abnormalities (WikiText, Financial QA, Twitter), we retried with smaller learning rates (e.g., $1\times10^{-5}$ or $5\times10^{-6}$) and observed improved stability and performance. We treat these reductions as pragmatic fixes in our setup rather than a general scaling rule. The differences between solid (original LR) and dashed (reduced LR) lines in \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} illustrate these context-specific improvements; \Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison} quantify them.

\textbf{Finding 3: Dataset Size Critically Affects Pretraining Viability}

Clear thresholds emerged: datasets $>$ 100M tokens support standalone pretraining (2–5 epochs, consistent generalization); 20–100M tokens are viable with caveats (6–30 epochs, moderate generalization); $<$ 20M tokens are not viable alone (67–249 epochs, severe overtraining and high cross-dataset variability). Correlation between log(tokens) and variability: $r = -0.78$ ($p < 0.01$). Small datasets require mixing regardless of optimization quality—data scarcity, not hyperparameters, limits performance. The scaling figures show this: \Cref{fig:scaling_news_articles,fig:scaling_sec_reports} (large datasets) have smooth curves with small gaps between sizes; \Cref{fig:scaling_financial_qa,fig:scaling_twitter} (small datasets) are erratic and need LR interventions. \Cref{tab:cross_financial_qa,tab:cross_twitter} reveal the brittleness: these rows achieve boldface mainly in their own columns (specialization) while showing 30–50 ppl elsewhere (transfer failure).

\textbf{Finding 4: Format Drives Transfer More Than Domain Vocabulary}

Document format and task structure predict cross-dataset transfer better than topical domain. Long-form documents (News $\leftrightarrow$ SEC: $r = 0.82$) transfer well despite style differences; instruction tasks cluster (FinGPT/Alpaca/FiQA: $r = 0.68-0.73$); short-form Twitter is isolated with high variability. A News model transfers better to regulatory SEC filings (both long-form, different domains) than to Twitter finance posts (same domain, different format). This suggests pretraining corpora should prioritize format diversity alongside domain coverage. The cross-dataset tables provide striking visual evidence: \Cref{tab:cross_financial_news,tab:cross_financial_repor} show boldface clustering along the News-SEC diagonal, confirming bidirectional long-form transfer. \Cref{tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa} exhibit similar diagonal boldface patterns plus adjacency (instruction-trained models capturing boldface in each other's columns), demonstrating format-based clustering. In contrast, \Cref{tab:cross_twitter} shows complete isolation—boldface appears only in Twitter's own column regardless of which training dataset is used, visualizing the distributional uniqueness of short-form social media text.

These findings generalize beyond finance to any specialized-domain pretraining scenario where practitioners face similar trade-offs: domain vs general data, mixture composition, model scaling, and format diversity.

\section{Interpretation of Data Interaction Effects}

\subsection{Why WikiText Underperforms on Financial Tasks}

WikiText's catastrophic financial transfer (41.96 ppl mean vs 21.55 ppl for Mixed Financial) stems from three fundamental mismatches:

\textbf{1. Vocabulary Gap}: Financial language contains specialized terminology absent in encyclopedic text. Terms like ``EBITDA'' (earnings before interest, taxes, depreciation, amortization), ``alpha'' (excess returns), ``basis points'' (0.01\%), ``volatility'' (price fluctuation measure), ``hedging'' (risk mitigation strategy), and ``P/E ratio'' (price-to-earnings valuation) rarely appear in Wikipedia. When WikiText models encounter financial evaluation texts, they face effective out-of-vocabulary scenarios despite shared syntactic structure. The model's vocabulary distribution mismatches the evaluation domain's lexical requirements.

\textbf{2. Reasoning Pattern Mismatch}: Financial analysis requires forward-looking causal reasoning: ``Company X's earnings miss will pressure the stock downward'' (cause-effect prediction), ``Rising interest rates typically compress equity valuations'' (conditional reasoning), ``The Fed's hawkish stance suggests tightening ahead'' (implicit reasoning from policy to outcomes). Wikipedia's encyclopedic, descriptive style—focused on established facts, historical narratives, and definitional content—doesn't exercise these prospective reasoning patterns. Models pretrained on WikiText learn to predict continuations based on factual descriptions, not anticipatory financial logic.

\textbf{3. Discourse Structure Divergence}: Financial news follows inverted pyramid structure (conclusion first, then supporting details); earnings reports have standardized sections (forward-looking statements, risk factors, MD\&A); analyst reports use comparison tables and numerical evidence. Wikipedia articles employ chronological narratives (biographical entries), topical organization (scientific articles), or definitional structures (concept entries). These discourse patterns create different coherence signals—WikiText models learn topic progression and factual elaboration, while financial texts require comparative analysis and evidential reasoning structures.

\textbf{Why General → Financial Transfer Fails But Financial → General Succeeds}: The asymmetry (WikiText @ 4B: 41.96 ppl financial vs Mixed Financial @ 4B: 27.72 ppl WikiText) reveals hierarchical structure. General language (syntax, semantics, discourse coherence) forms a foundation; financial language adds specialized vocabulary and reasoning on top. Starting from general pretraining provides linguistic prerequisites; domain-specific training adds specialization without catastrophic forgetting of fundamentals. Conversely, starting from general pretraining lacks domain prerequisites—vocabulary and reasoning gaps cannot be bridged by linguistic competence alone. This asymmetry is strikingly visible in \Cref{tab:cross_wikitext}: WikiText training rows show boldface in WikiText columns (4.78-38.60 ppl across model sizes) but poor financial performance (26-58 ppl depending on dataset and LR). Financial training rows show acceptable WikiText performance (27-42 ppl) alongside superior financial metrics. The table's boldface distribution pattern—concentrated in financial rows for most columns, scattered in WikiText rows—quantitatively demonstrates that financial pretraining retains general capability while general pretraining fails to acquire domain specialization.

\subsection{Benefits of In-Domain Diversity}

Mixed Financial's advantage (21.55 ppl, 55\% relative spread) over individual datasets (mean: 24.8 ppl, \~65\% relative spread) and WikiText (41.96 ppl financial, \~53\% relative spread after LR adjustment) stems from diversity-driven stability:

\textbf{Cross-Format Exposure}: The 7-dataset mixture spans long-form documents (News 197M, SEC 80M), instruction formats (FinGPT 19M, Alpaca 17M, FiQA 4M), and short-form text (Twitter 0.3M, Financial QA 3.5M). This format diversity prevents overfitting to structural artifacts. Models trained on pure News learn long-form coherence but fail on dialogic Q\&A (41\% worse on FiQA); mixed models handle both, averaging only 30\% degradation across all formats.

\textbf{Vocabulary Coverage}: Different financial datasets emphasize different lexical subdomains: News covers market events and company names; SEC covers regulatory terminology (``10-K'', ``forward-looking statements''); FinGPT covers sentiment vocabulary (``bullish'', ``bearish''); Alpaca covers financial concepts (``compound interest'', ``diversification''). The mixture creates broad vocabulary coverage—no single dataset provides this breadth. Mixed models encounter 3.2$\times$ more unique financial terms during training than the largest individual dataset (News), improving lexical stability.

\textbf{Task Diversity Regularization}: Mixing datasets with different objectives (sentiment classification, Q\&A, document completion) acts as implicit multi-task learning. The model cannot overfit to any single task's superficial cues (e.g., specific sentiment indicators in FinGPT, formulaic question structures in Alpaca) because the loss function averages across diverse distributions. This produces representations that capture underlying financial semantics rather than task-specific shortcuts.

\textbf{Preventing Data Memorization}: Small datasets suffer from memorization—Financial QA (3.5M tokens, 67-100 epochs) achieves 8.09 ppl in-domain but 41.7 ppl cross-dataset. The model memorizes training examples rather than learning generalizable patterns. Mixing prevents memorization by capping each dataset's contribution (50cap strategy limits News to 50\%, ensuring others get exposure) and diversifying the training distribution. Mixed models see fewer repeated examples from any single source, forcing extraction of transferable features.

\textbf{Quantitative Evidence}: Variability reduction correlates with mixture diversity: the 7-dataset mixture (\~55\% relative spread) compares favorably to individual datasets (often \~65\% or higher). The mixture improves both performance (21.55 vs 24.8 ppl mean) and consistency simultaneously. The cross-dataset tables illustrate this: Mixed Financial rows appear most frequently in boldface across evaluation columns. Individual dataset rows (News, SEC, FinGPT, etc.) capture boldface mainly in their own or nearby columns, while Mixed Financial remains competitive across the board. This broad vs narrow boldface distribution visualizes how diversity enables more stable generalization across heterogeneous evaluation scenarios.

\subsection{Domain Interference Patterns}

While in-domain diversity helps, cross-domain mixing (Mixed Wiki+Financial) shows interference:

\textbf{Performance-Diversity Trade-off}: Mixed Wiki+Financial achieves 26.69 ppl (4B), 24\% worse than pure Mixed Financial (21.55 ppl), despite including WikiText. On WikiText specifically, the mixed approach improves performance modestly compared to pure Financial, but mean financial performance degrades notably. The trade-off is unfavorable for finance-focused applications: sacrificing financial performance for a small general-domain gain.

\textbf{Competing Optimization Signals}: Financial and general domains create conflicting gradients. Financial texts reward predicting domain terminology (``EBITDA'' following ``reported''); general texts reward different continuations (``findings'' following ``reported''). The model's parameters cannot simultaneously optimize for both distributions without compromise. Mixed Wiki+Financial models average these signals, achieving moderate performance on both rather than excellence on either. The 62\% variance (vs 55\% pure financial) reflects this optimization conflict.

\textbf{When Mixing Hurts vs Helps}: Intra-domain mixing helps because datasets share core semantics (financial vocabulary, reasoning patterns) while differing in format and task type—diversity reinforces fundamentals. Cross-domain mixing hurts when domains diverge in vocabulary and reasoning (encyclopedic vs analytical), creating zero-sum trade-offs. The 50cap strategy mitigates but doesn't eliminate interference: capping WikiText at 50\% limits damage but still dilutes financial specialization. This distinction is evident comparing \Cref{tab:mixed_financial_results} (pure financial mixture) and \Cref{tab:mixed_wiki_financial_results} (cross-domain mixture): the former shows consistently lower perplexity across all financial evaluation datasets, with the performance advantage increasing at larger model sizes. \Cref{fig:scaling_mixed_financial,fig:scaling_mixed_wiki_financial} visually confirm this—the pure financial mixture (first figure) shows steeper slope (22.6\% total improvement) compared to Wiki+Financial (second figure, 15.1\% improvement), indicating that domain conflict reduces scaling efficiency.

\textbf{Practical Implication}: For specialized applications, domain purity wins. Only mix cross-domain when explicit general-domain retention is required (e.g., conversational agents handling both financial and general queries). For finance-focused deployments, pure in-domain mixtures maximize performance.

\subsection{Scale-Dependent Training Notes}

Our experience suggests that larger models can be more sensitive to optimization settings on some datasets. While we kept LR=2e-5 for main runs, reducing LR in a handful of follow-ups helped stabilize training. We do not claim a general rule beyond this observation.

\section{Practical Guidelines for Financial LM Pretraining}

Synthesizing experimental findings into actionable recommendations:

\subsection{Data Mixture Strategies by Use Case}

\textbf{General-Purpose Financial NLP}: Use Mixed Financial (7 datasets, 50cap). Achieves best all-around performance (21.55 ppl, 55\% relative spread) with stable cross-task generalization. Suitable for applications requiring diverse financial capabilities: sentiment analysis, document summarization, Q\&A, information extraction. As shown in \Cref{fig:scaling_mixed_financial,fig:scaling_comparison_all}, this approach scales reliably across model sizes and consistently outperforms alternatives. The cross-dataset tables also support this choice: Mixed Financial rows capture boldface positions more often than any individual dataset across the eight evaluation scenarios.

\textbf{Specialized Document Analysis}: Use single large dataset if available ($>$ 100M tokens). SEC @ 4B (15.91 ppl on SEC; \~19\% relative spread across evaluations) excels for regulatory filing analysis; News @ 4B (17.47 ppl on News; \~66\% relative spread) excels for journalism. Specialization improves in-domain performance but sacrifices cross-format transfer. \Cref{fig:scaling_news_articles,fig:scaling_sec_reports} show these datasets maintain stable scaling without requiring LR adjustments. However, \Cref{tab:cross_financial_news,tab:cross_financial_repor} reveal that News and SEC training rows achieve boldface primarily within document-format columns, confirming limited format diversity.

\textbf{Instruction-Following / Q\&A Applications}: Use FiQA (4M tokens, 16.35 ppl) or FinGPT (19M tokens, 19.83 ppl) for specialized Q\&A, or include in mixture for general applications. Instruction formats transfer moderately within task type ($r = 0.68-0.73$) but poorly to documents. The instruction-following tables (\Cref{tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa}) show boldface clustering along the diagonal and adjacent instruction rows, visualizing the format-based transfer limitation.

\textbf{Balanced General + Financial Capabilities}: Use Mixed Wiki+Financial only if general-domain retention is explicitly required (e.g., chatbots handling both financial and general queries). Accepts 24\% financial performance cost for 16\% general improvement—unfavorable for finance-focused deployments. \Cref{fig:scaling_mixed_wiki_financial} shows reduced slope compared to pure financial mixture, and \Cref{tab:mixed_wiki_financial_results} documents the performance cost across all financial evaluation datasets.

\textbf{Avoid}: Pure WikiText for financial applications (2.0$\times$ performance degradation vs Mixed Financial on average across financial tasks), small individual datasets $<$ 20M tokens (non-viable standalone due to severe overtraining and high variability), single-format training when diverse tasks are expected (format mismatch prevents transfer). \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} provide visual evidence: WikiText requires LR adjustment and still shows poor financial transfer, while small datasets exhibit brittleness visible in both scaling curves and cross-dataset table patterns.

\subsection{Model Size Selection}

\textbf{0.6B Models}: Fast training ($\sim$6 hours for 100M tokens on Lambda Labs GPUs), low memory (4GB), suitable for rapid prototyping. Performance is acceptable for exploratory work, but variability is high (Mixed Financial: \~98\% relative spread). Use for development, experimentation, or extremely resource-constrained deployment (mobile devices).

\textbf{1.7B Models}: Best performance-efficiency balance. Training moderate ($\sim$12 hours), memory reasonable (10GB), performance strong with improved consistency vs 0.6B (Mixed Financial: \~63\% relative spread). Recommended for most applications—strong performance at substantially lower resource cost than 4B. Optimal for production deployment balancing quality and resource constraints.

\textbf{4B Models}: Best absolute performance (21.55 ppl, 55\% relative spread) but requires careful hyperparameter tuning (LR $5 \times 10^{-6}$ in affected cases) and substantial resources (20GB memory, $\sim$24 hours training). Use when maximizing performance justifies cost, and when expertise for hyperparameter tuning is available. Critical: failure to tune learning rate can cause reverse scaling—practitioners may need to reduce LR substantially at larger scales.

\textbf{Scaling Decision Tree}:
\begin{enumerate}
\item \textbf{Resource-constrained} (mobile, edge devices): 0.6B, accept 22\% performance loss vs 4B
\item \textbf{Balanced production deployment}: 1.7B, optimal trade-off (92\% of 4B performance, 50\% resources)
\item \textbf{Performance-critical} (willing to invest tuning effort): 4B, requires LR scaling expertise
\end{enumerate}

\subsection{Learning Rate Notes}

\textbf{Main setting}: $2 \times 10^{-5}$ across all primary experiments.

\textbf{Follow-ups}: For the few runs with anomalies, we used smaller LRs (e.g., $1\times10^{-5}$ or $5\times10^{-6}$) to stabilize training.

\textbf{Scope}: These are practical notes from our setup, not prescriptive guidelines.

\subsection{Token Budget Allocation}

\textbf{Optimal Token Budget}: 100M tokens sufficient when properly mixed across diverse datasets. Diminishing returns beyond this threshold for 0.6B-4B models in our experiments. Larger models ($>$ 7B) may benefit from extended training (200-500M tokens), but this remains untested.

\textbf{Mixture Composition}: Use 50cap strategy to prevent dominance. For $n$ datasets with sizes $\{s_1, s_2, ..., s_n\}$ where $s_1 > 0.5 \sum_i s_i$: cap $s_1$ at 50\% of total, sample others proportionally. This ensures diversity while respecting relative dataset informativeness.

\textbf{Sampling Strategy}: Token-level interleaving, not batch-level or epoch-level. Sample each training batch from mixture distribution with probabilities proportional to (capped) dataset sizes. Avoids sequential exposure that can cause catastrophic forgetting.

\textbf{Dataset Prioritization}: When curating datasets, prioritize: (1) Format diversity (documents, Q\&A, dialogue), (2) Size (aim for $\geq$ 100M total across sources), (3) Quality (clean text $>$ noisy text, but in-domain noisy $>$ out-of-domain clean). Don't exclude small datasets ($<$ 20M tokens) from mixtures—they contribute valuable diversity despite non-viability standalone.

\section{Limitations and Threats to Validity}

\textbf{Single Model Family}: All experiments used Qwen3 (0.6B/1.7B/4B). Observations about LR behavior may be architecture- and dataset-specific. Other decoder-only transformers (LLaMA, Gemma, Phi) could behave differently; validation required. Encoder-only (BERT) or encoder-decoder (T5) models may show different mixture effects due to bidirectional attention or different pretraining objectives.

\textbf{Fixed Mixture Strategy}: We used 50cap exclusively. Other algorithms (temperature sampling, equal mixing, DoReMi dynamic weighting) remain unexplored. The 50cap heuristic worked well but may not be optimal—ablation studies varying cap thresholds (30\%, 40\%, 60\%) could reveal improvements. Dynamic mixture strategies that adjust dataset weights during training based on validation loss may outperform static 50cap.

\textbf{Evaluation on Pretraining Distributions}: We evaluated using perplexity on held-out test sets from the same distributions as training data. This measures pretraining quality but doesn't directly assess downstream task performance. Fine-tuned performance on financial NLP tasks (sentiment classification accuracy, Q\&A F1, summarization ROUGE) may differ from pretraining perplexity rankings. Future work should validate that Mixed Financial's pretraining advantage transfers to downstream applications.

\textbf{Hardware Constraints}: Experiments limited to 0.6B-4B models due to available hardware (RTX A6000 48GB, A100 40GB, H100 80GB rented from Lambda Labs). Larger models (7B, 13B, 70B) may show different patterns; mixture benefits may increase or decrease with scale. We did not investigate LR behavior beyond the few follow-ups reported here.

\textbf{Limited Hyperparameter Search}: We systematically explored learning rates but kept other hyperparameters fixed (effective batch size 8, warmup 1000 steps, cosine schedule). Larger hyperparameter sweeps over batch size (4, 8, 16, 32), warmup ratios (1\%, 3\%, 5\%), and schedules (linear, cosine, polynomial) may reveal better configurations. Computational budget constraints prevented exhaustive search.

\textbf{Financial Domain Specificity}: Results may not generalize to other specialized domains with different characteristics. Legal text (extremely long documents, formal citations) or medical text (heavy abbreviations, multimodal integration) may show different mixture effects. The core principles (in-domain diversity, and our LR heuristics) may generalize, but specific mixture ratios and optimal configurations require domain-specific validation.

Despite these limitations, our findings provide solid empirical evidence for data mixture effects, training dynamics, and practical practices applicable to financial LM pretraining and likely informative for other specialized domains.
