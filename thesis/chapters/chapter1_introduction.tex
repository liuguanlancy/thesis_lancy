\chapter{Introduction}

\section{Motivation}

Large language models (LLMs) have rapidly changed how we do natural language processing \parencite{vaswani2017attention,radford2019language,brown2020language,touvron2023llama}. Yet using them in finance still brings practical hurdles. Financial institutions and individuals handle highly sensitive data—transactions, portfolios, trading strategies—that cannot be sent to external APIs for privacy and competitive reasons (e.g., GDPR) \parencite{eu2016gdpr}. We therefore need lightweight, locally runnable financial language models that maintain reasonable performance while protecting data, with no exceptions.

In practice, domain adaptation tends to follow two paths: train very large models from scratch or fine-tune general models on domain data. Most teams cannot afford the first; the second often misses domain nuances \parencite{gururangan2020don}. And there is a related belief: high quality general corpora (e.g., Wikipedia, The Pile) always help specialized applications. Not always; evidence is thinner than many assume \parencite{gao2020pile,raffel2020exploring,longpre2023pretrainer}.

This thesis studies how different data sources—both in‑domain financial data and out‑of‑domain high‑quality corpora—interact during pretraining. We focus on models in the 0.6B to 4B parameter range, which are realistic for laptops and some mobile devices while keeping acceptable performance \parencite{yang2024qwen2,xia2023sheared,team2024gemma,javaheripi2023phi}. Through systematic experiments across 10 pretraining configurations and three model sizes, we present evidence about data mixture strategies for specialized domains \parencite{wu2023bloomberggpt}. In practical terms, we test simple choices that many teams can replicate.

This study is timely. Regulations such as GDPR and emerging financial data protection standards push for on‑device processing \parencite{eu2016gdpr}. As more teams adopt AI with limited compute, insights on 0.6B to 4B models are practically useful. Still, we keep the scope modest.

Beyond applications, we also add to understanding how models learn from different data distributions. We observed a surprising pattern sometimes called ``reverse scaling'', in some regimes smaller models outperform larger ones. In our runs, these cases point to hyperparameter choices rather than fundamental limits \parencite{kaplan2020scaling,hoffmann2022training,mccandlish2018empirical}. Tuning matters.

\section{Research Questions}

This thesis investigates the following core research questions. We keep them narrow and testable. We do not chase new theory or SOTA here.

\textbf{RQ1: Data Mixture Composition}
How do combinations of in domain financial datasets and out of domain general corpora affect model performance and generalization? Specifically, does mixing multiple financial datasets improve consistency compared to single dataset training, and does adding high quality general text (WikiText) help or hurt financial tasks? Our results (\Cref{fig:scaling_comparison_all,tab:mixed_financial_results,tab:mixed_wiki_financial_results}) indicate that mixed financial datasets achieve 21.55 ppl (mean across financial evaluations), compared to 26.69 ppl for Wiki+Financial mixtures (overall mean across eight evaluations) and 41.96 ppl for pure WikiText (mean across financial evaluations after LR adjustment), suggesting in domain diversity is the better choice.

\textbf{RQ2: Model Size and Training Dynamics}
How do optimal training configurations vary across model sizes (0.6B, 1.7B, 4B parameters)? What is the relationship between size and learning rate sensitivity? In our setup, we trained all main runs with LR=2e-5; for a few abnormal cases, we reduced LR pragmatically (e.g., to $1\times10^{-5}$ or $5\times10^{-6}$) and saw improved stability. We do not claim a general scaling rule.

\textbf{RQ3: Dataset Size Effects}
What is the minimum dataset size for effective standalone pretraining, and how does size affect overtraining and cross dataset generalization? At what point do small datasets need mixing? With our data, datasets $>$100M tokens enable stable training (\Cref{fig:scaling_news_articles,fig:scaling_sec_reports}), while datasets $<$20M tokens require mixing due to extreme overtraining and high cross dataset variability (\Cref{fig:scaling_financial_qa,fig:scaling_twitter,tab:cross_financial_qa,tab:cross_twitter}).

\textbf{RQ4: Domain Transfer Patterns}
How well do models pretrained on financial data transfer across task types (sentiment, question answering, document understanding), and how much does document format matter? Cross dataset comparison tables (\Cref{tab:cross_financial_news,tab:cross_financial_repor,tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa,tab:cross_twitter}) suggest that format consistency (long form, instruction, short form) drives transfer more than domain vocabulary, with boldface patterns clustering along format based diagonals.

These questions are addressed through a detailed experimental framework with 30 trained models and 237 evaluation results across eight held-out test sets (Mixed Financial excludes WikiText evaluation), providing systematic evidence on data mixture effects in specialized-domain pretraining.

\section{Contributions}

This thesis makes six primary contributions to understanding data mixture effects and training dynamics for language model pretraining:

\textbf{1. Empirical Data Mixture Guidelines}
We provide concrete, evidence-based recommendations for financial language model pretraining, showing that in-domain diversity outperforms high-quality general corpora for specialized domains. Our experiments show that mixed financial datasets achieve 21.55 perplexity at 4B parameters compared to 41.96 perplexity (mean across financial evaluations after LR adjustment) for WikiText pretraining, a 1.95$\times$ gap. These findings challenge the assumption that general high-quality text always helps domain adaptation. We support this with 11 scaling figures and 18 tables (10 per-training-dataset and 8 cross-dataset comparisons).

\textbf{2. Learning Rate Notes}
All main experiments used LR=2e-5. In three follow-ups with abnormalities (WikiText, Financial QA, Twitter), we reduced LR (e.g., to $1\times10^{-5}$ or $5\times10^{-6}$) and observed improved stability and results. We present these as pragmatic fixes in our setup, not a general rule. Figures \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} show recovery in these runs; \Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison} give the metrics.

\textbf{3. Dataset Size Effects on Pretraining}
We summarize empirical relationships between dataset size and training viability:
\begin{itemize}
    \item Small datasets (< 20K samples): Extreme overtraining (67-249 epochs), high variance (70-97\% relative spread), require mixing
    \item Medium datasets (20-100K samples): Moderate overtraining (6-30 epochs), acceptable for specific use cases
    \item Large datasets (> 100K samples): Minimal overtraining (2-24 epochs), viable for standalone pretraining
\end{itemize}
These findings provide practical guidance on when mixing is necessary versus when individual datasets suffice, with direct implications for practitioners planning limited data collection and annotation budgets.

\textbf{4. Cross-Domain Interaction Analysis}
We study how high-quality general corpora (WikiText) interact with domain-specific financial data during pretraining. Contrary to common expectations, WikiText provides little benefit and sometimes degrades financial task performance. Mixed WikiText+Financial pretraining achieves 26.69 perplexity compared to 21.55 for pure financial mixing, a 24\% degradation. This suggests domain-specific data strategies are better for specialized applications. Cross-dataset tables show this pattern: WikiText rows rarely capture best-performance positions across financial evaluation columns, while mixed financial rows often do.

\textbf{5. Lightweight Financial Model Feasibility}
We show that 0.6B to 4B parameter models can achieve practical financial NLP performance with appropriate mixtures and tuning, enabling privacy-preserving edge deployment. Our 4B model achieves 21.55 perplexity on diverse financial tasks, competitive with much larger models while remaining deployable on consumer hardware. This addresses the need for locally runnable financial AI systems.

\textbf{6. Open-Source Training Pipeline}
We provide a reproducible codebase for mixture-based pretraining with a detailed evaluation framework across 10 experiments and 30 trained models. The pipeline supports automatic mixture composition, multi-dataset evaluation, and systematic hyperparameter tuning, enabling future research on domain-specific language model training.

\section{Thesis Organization}

What follows is simple and practical.

\textbf{Chapter 2: Background and Related Work} gives only what we need: key ideas from financial NLP, pretraining basics, mixture strategies, and domain adaptation. Enough context to place our study in transfer learning and scaling work.

\textbf{Chapter 3: Methodology} explains the design we used: Qwen3 models, the datasets (7 financial sets, 321.4M tokens, plus WikiText), the 50cap mixture rule, and training settings. We also note where we had to adjust learning rate for stability.

\textbf{Chapter 4: Results} presents the evidence with plots and tables (11 scaling figures, 18 tables). First mixtures, then single datasets, then training dynamics, and finally transfer. The plots show trends across sizes; the tables show which setup works where.

\textbf{Chapter 5: Discussion} connects the results to prior work and day to day practice. We explain the weak financial transfer of WikiText (see boldface in cross tables), what in domain diversity buys, how learning rate tweaks changed stability, and we close with practical guidelines with exact references.

\textbf{Chapter 6: Conclusion} sums up contributions, notes implications, and lists next steps: try larger models, test dynamic mixing, and check downstream tasks.

\section{Scope and Limitations}

We study causal language model pretraining on financial text in the 0.6B to 4B range. Three notes help read the results fairly.

\textbf{Model architecture.} We used Qwen3 for all runs. The learning rate and mixture lessons should carry over, but checking LLaMA, Gemma, and Phi would add confidence.

\textbf{Mixture rule and evaluation.} We kept one mixture rule (50cap, the largest dataset capped at half the mixture) and measured perplexity on held out test sets. This fits our goal of studying pretraining dynamics. It does not give downstream task accuracy, so claims about applications stay limited.

\textbf{Scale and domain.} We worked between 0.6B and 4B for hardware reasons. Larger models can behave differently. Findings about learning rate and dataset size likely transfer to other domains, but the weak benefit of WikiText is specific to finance and may not hold elsewhere.

Across 30 models and more than 240 evaluations, the patterns are consistent. Where evidence is thin, we say so.
