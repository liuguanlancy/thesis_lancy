\chapter{Introduction}

\section{Motivation}

Large language models (LLMs) have changed how we do natural language processing \parencite{vaswani2017attention,radford2019language,brown2020language,touvron2023llama}. Fast. But using them in finance still brings practical hurdles. Financial institutions and individuals handle highly sensitive data—transactions, portfolios, trading strategies—that cannot be sent to external APIs for privacy and competitive reasons (e.g., GDPR) \parencite{eu2016gdpr}. So we need lightweight, locally runnable financial language models that keep reasonable performance while protecting data. No exceptions there.

In practice, domain adaptation tends to follow two paths: train very large models from scratch or fine-tune general models on domain data. Most teams cannot afford the first; the second often misses domain nuances \parencite{gururangan2020don}. And there is a related belief: high‑quality general corpora (e.g., Wikipedia, The Pile) always help specialized applications. Not always; evidence is thinner than many assume \parencite{gao2020pile,raffel2020exploring,longpre2023pretrainer}.

This thesis studies how different data sources—both in‑domain financial data and out‑of‑domain high‑quality corpora—interact during pretraining. We focus on models in the 0.6B to 4B parameter range, which are realistic for laptops and some mobile devices while keeping acceptable performance \parencite{yang2024qwen2,xia2023sheared,team2024gemma,javaheripi2023phi}. Through systematic experiments across 10 pretraining configurations and three model sizes, we present evidence about data mixture strategies for specialized domains \parencite{wu2023bloomberggpt}. Put another way, we test simple choices that many teams can replicate.

This study is timely. Regulations such as GDPR and emerging financial data protection standards push for on‑device processing \parencite{eu2016gdpr}. And as more teams adopt AI with limited compute, insights on 0.6B–4B models are practically useful. Still, we keep scope modest.

Beyond applications, we also add to understanding how models learn from different data distributions. We observed a surprising pattern sometimes called ``reverse scaling''—in some regimes smaller models outperform larger ones. In our runs, these cases point to hyperparameter choices rather than fundamental limits \parencite{kaplan2020scaling,hoffmann2022training,mccandlish2018empirical}. Tuning matters.

\section{Research Questions}

This thesis investigates the following core research questions. We keep them narrow and testable.

\textbf{RQ1: Data Mixture Composition}
How do combinations of in‑domain financial datasets and out‑of‑domain general corpora affect model performance and generalization? Specifically, does mixing multiple financial datasets improve consistency compared to single‑dataset training, and does adding high‑quality general text (WikiText) help or hurt financial tasks? Our results (\Cref{fig:scaling_comparison_all,tab:mixed_financial_results,tab:mixed_wiki_financial_results}) indicate that mixed financial datasets achieve 21.55 ppl (mean across financial evaluations), compared to 26.69 ppl for Wiki+Financial mixtures (overall mean across eight evaluations) and 41.96 ppl for pure WikiText (mean across financial evaluations after LR adjustment)—suggesting in‑domain diversity is the better choice.

\textbf{RQ2: Model Size and Training Dynamics}
How do optimal training configurations vary across model sizes (0.6B, 1.7B, 4B parameters)? What is the relationship between size and learning rate sensitivity? In our setup, we trained all main runs with LR=2e‑5; for a few abnormal cases, we reduced LR pragmatically (e.g., to $1\times10^{-5}$ or $5\times10^{-6}$) and saw improved stability. We do not claim a general scaling rule.

\textbf{RQ3: Dataset Size Effects}
What is the minimum dataset size for effective standalone pretraining, and how does size affect overtraining and cross‑dataset generalization? At what point do small datasets need mixing? With our data, datasets $>$100M tokens enable stable training (\Cref{fig:scaling_news_articles,fig:scaling_sec_reports}), while datasets $<$20M tokens require mixing due to extreme overtraining and high cross‑dataset variability (\Cref{fig:scaling_financial_qa,fig:scaling_twitter,tab:cross_financial_qa,tab:cross_twitter}).

\textbf{RQ4: Domain Transfer Patterns}
How well do models pretrained on financial data transfer across task types (sentiment, question answering, document understanding), and how much does document format matter? Cross‑dataset comparison tables (\Cref{tab:cross_financial_news,tab:cross_financial_repor,tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa,tab:cross_twitter}) suggest that format consistency (long‑form, instruction, short‑form) drives transfer more than domain vocabulary, with boldface patterns clustering along format‑based diagonals.

These questions are addressed through a detailed experimental framework with 30 trained models and 237 evaluation results across eight held-out test sets (Mixed Financial excludes WikiText evaluation), providing systematic evidence on data mixture effects in specialized-domain pretraining.

\section{Contributions}

This thesis makes six primary contributions to understanding data mixture effects and training dynamics for language model pretraining:

\textbf{1. Empirical Data Mixture Guidelines}
We provide concrete, evidence-based recommendations for financial language model pretraining, showing that in-domain diversity outperforms high-quality general corpora for specialized domains. Our experiments show that mixed financial datasets achieve 21.55 perplexity at 4B parameters compared to 41.96 perplexity (mean across financial evaluations after LR adjustment) for WikiText pretraining—a 1.95$\times$ gap. These findings challenge the assumption that general high-quality text always helps domain adaptation. We support this with 11 scaling figures and 18 tables (10 per-training-dataset and 8 cross-dataset comparisons).

\textbf{2. Learning Rate Notes}
All main experiments used LR=2e-5. In three follow-ups with abnormalities (WikiText, Financial QA, Twitter), we reduced LR (e.g., to $1\times10^{-5}$ or $5\times10^{-6}$) and observed improved stability and results. We present these as pragmatic fixes in our setup, not a general rule. Figures \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} show recovery in these runs; \Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison} give the metrics.

\textbf{3. Dataset Size Effects on Pretraining}
We summarize empirical relationships between dataset size and training viability:
\begin{itemize}
    \item Small datasets (< 20K samples): Extreme overtraining (67-249 epochs), high variance (70-97\% relative spread), require mixing
    \item Medium datasets (20-100K samples): Moderate overtraining (6-30 epochs), acceptable for specific use cases
    \item Large datasets (> 100K samples): Minimal overtraining (2-24 epochs), viable for standalone pretraining
\end{itemize}
These findings provide practical guidance on when mixing is necessary versus when individual datasets suffice, with direct implications for practitioners planning limited data collection and annotation budgets.

\textbf{4. Cross-Domain Interaction Analysis}
We study how high-quality general corpora (WikiText) interact with domain-specific financial data during pretraining. Contrary to common expectations, WikiText provides little benefit and sometimes degrades financial task performance. Mixed WikiText+Financial pretraining achieves 26.69 perplexity compared to 21.55 for pure financial mixing—a 24\% degradation. This suggests domain-specific data strategies are better for specialized applications. Cross-dataset tables show this pattern: WikiText rows rarely capture best-performance positions across financial evaluation columns, while mixed financial rows often do.

\textbf{5. Lightweight Financial Model Feasibility}
We show that 0.6B–4B parameter models can achieve practical financial NLP performance with appropriate mixtures and tuning, enabling privacy-preserving edge deployment. Our 4B model achieves 21.55 perplexity on diverse financial tasks, competitive with much larger models while remaining deployable on consumer hardware. This addresses the need for locally runnable financial AI systems.

\textbf{6. Open-Source Training Pipeline}
We provide a reproducible codebase for mixture-based pretraining with a detailed evaluation framework across 10 experiments and 30 trained models. The pipeline supports automatic mixture composition, multi-dataset evaluation, and systematic hyperparameter tuning, enabling future research on domain-specific language model training.

\section{Thesis Organization}

The remainder of this thesis is organized as follows:

\textbf{Chapter 2: Background and Related Work} reviews existing literature on financial NLP, language model pretraining objectives, data mixture strategies, and domain adaptation approaches. We position our work within the broader context of transfer learning and scaling laws research.

\textbf{Chapter 3: Methodology} describes our experimental design in detail, including model architecture (Qwen3 family), dataset characteristics (7 financial datasets totaling 207M tokens, plus WikiText), mixture strategies (50cap algorithm), and training setup. We document the iterative process of discovering and resolving learning rate sensitivity issues, demonstrating the scientific rigor underlying our empirical findings.

\textbf{Chapter 4: Results} presents experimental findings organized by theme, supported by visual evidence (11 scaling figures and 18 detailed tables). We begin with data mixture effects (the core finding), move to individual dataset analysis (component effects), examine training dynamics and learning-rate sensitivity, and conclude with domain transfer patterns. Scaling figures show performance trends across model sizes, while cross-dataset tables identify which training approaches perform best for each evaluation scenario.

\textbf{Chapter 5: Discussion} interprets our findings in light of existing theory and practice, leveraging the visual evidence from Chapter 4. We explain why WikiText underperforms on financial tasks (analyzing cross-dataset table boldface patterns), analyze the benefits of in-domain diversity (interpreting scaling figure trends), discuss practical aspects of learning rate sensitivity (connecting LR adjustment figures to observed stability), and provide concrete guidelines for practitioners training financial language models (supported by specific figure and table references).

\textbf{Chapter 6: Conclusion} summarizes contributions, discusses implications for research and practice, and outlines promising directions for future work, including extension to larger models, exploration of dynamic mixing strategies, and evaluation on downstream financial tasks.

\section{Scope and Limitations}

This thesis focuses specifically on pretraining dynamics for causal language models in the 0.6B-4B parameter range applied to financial text. Several important scope limitations should be noted:

\textbf{Model Architecture:} All experiments use the Qwen3 model family. While we believe our findings on learning rate scaling and data mixture effects are generalizable, validation on other architectures (LLaMA, Gemma, Phi) would strengthen confidence in universality.

\textbf{Data Mixture Strategy:} We use a single mixture algorithm (50cap, which caps the largest dataset at 50\% of the mixture). We did not explore square-root sampling, temperature-based sampling, or dynamic curriculum learning; these might lead to different results.

\textbf{Evaluation Methodology:} We evaluate models by perplexity on held-out test sets from the pretraining distribution. While perplexity often correlates with downstream performance, we do not directly measure accuracy on financial NLP tasks (sentiment, NER, QA). This is because we focus on pretraining dynamics, not downstream systems. But it limits how far we can generalize to applications.

\textbf{Scale Range:} Our experiments cover 0.6B to 4B parameters due to hardware limits. Larger models (7B+) may show different training dynamics and data sensitivity. Still, the range we study is relevant for edge deployment.

\textbf{Domain Specificity:} We focus on financial text. Many findings—especially about learning rate and dataset size—likely transfer to other domains, but the claim that WikiText helps little is domain-specific and may not hold elsewhere.

Despite these limits, our experiments across 30 models and 240+ evaluations provide solid evidence for the claims here. We try to separate clearly what we know from what needs more study.
