\chapter{Introduction}

\section{Motivation}

Large language models (LLMs) have rapidly changed how we do natural language processing \parencite{vaswani2017attention,radford2019language,brown2020language,touvron2023llama}. Yet using them in finance still brings practical hurdles. Financial institutions and individuals handle highly sensitive data—transactions, portfolios, trading strategies—that cannot be sent to external APIs for privacy and competitive reasons (e.g., GDPR) \parencite{eu2016gdpr}. We therefore need lightweight, locally runnable financial language models that maintain reasonable performance while protecting data, with no exceptions.

In practice, domain adaptation tends to follow two paths: train very large models from scratch or fine-tune general models on domain data. Most teams cannot afford the first; the second often misses domain nuances \parencite{gururangan2020don}. And there is a related belief: high quality general corpora (e.g., Wikipedia, The Pile) always help specialized applications. Not always; evidence is thinner than many assume \parencite{gao2020pile,raffel2020exploring,longpre2023pretrainer}.

This thesis studies how different data sources—both in‑domain financial data and out‑of‑domain high‑quality corpora—interact during pretraining. We focus on models in the 0.6B to 4B parameter range, which are realistic for laptops and some mobile devices while keeping acceptable performance \parencite{yang2024qwen2,xia2023sheared,team2024gemma,javaheripi2023phi}. Through systematic experiments across 10 pretraining configurations and three model sizes, we present evidence about data mixture strategies for specialized domains \parencite{wu2023bloomberggpt}. In practical terms, we test simple choices that many teams can replicate.

This study is timely. Regulations such as GDPR and emerging financial data protection standards push for on‑device processing \parencite{eu2016gdpr}. As more teams adopt AI with limited compute, insights on 0.6B to 4B models are practically useful. Still, we keep the scope modest.

Beyond applications, we also add to understanding how models learn from different data distributions. We observed a surprising pattern sometimes called ``reverse scaling'', in some regimes smaller models outperform larger ones. In our runs, these cases point to hyperparameter choices rather than fundamental limits \parencite{kaplan2020scaling,hoffmann2022training,mccandlish2018empirical}. Tuning matters.

\section{Research Questions}

This thesis investigates the following core research questions. We keep them narrow and testable. We do not chase new theory or SOTA here.

\textbf{RQ1: Data Mixture Composition}
How do combinations of in domain financial datasets and out of domain general corpora affect model performance and generalization? Specifically, does mixing multiple financial datasets improve consistency compared to single dataset training, and does adding high quality general text (WikiText) help or hurt financial tasks? Our results (\Cref{fig:scaling_comparison_all,tab:mixed_financial_results,tab:mixed_wiki_financial_results}) indicate that mixed financial datasets achieve 21.55 ppl (mean across financial evaluations), compared to 26.69 ppl for Wiki+Financial mixtures (overall mean across eight evaluations) and 41.96 ppl for pure WikiText (mean across financial evaluations after LR adjustment), suggesting in domain diversity is the better choice.

\textbf{RQ2: Model Size and Training Dynamics}
How do optimal training configurations vary across model sizes (0.6B, 1.7B, 4B parameters)? What is the relationship between size and learning rate sensitivity? In our setup, we trained all main runs with LR=2e-5; for a few abnormal cases, we reduced LR pragmatically (e.g., to $1\times10^{-5}$ or $5\times10^{-6}$) and saw improved stability. We do not claim a general scaling rule.

\textbf{RQ3: Dataset Size Effects}
What is the minimum dataset size for effective standalone pretraining, and how does size affect overtraining and cross dataset generalization? At what point do small datasets need mixing? With our data, datasets $>$100M tokens enable stable training (\Cref{fig:scaling_news_articles,fig:scaling_sec_reports}), while datasets $<$20M tokens require mixing due to extreme overtraining and high cross dataset variability (\Cref{fig:scaling_financial_qa,fig:scaling_twitter,tab:cross_financial_qa,tab:cross_twitter}).

\textbf{RQ4: Domain Transfer Patterns}
How well do models pretrained on financial data transfer across task types (sentiment, question answering, document understanding), and how much does document format matter? Cross dataset comparison tables (\Cref{tab:cross_financial_news,tab:cross_financial_repor,tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa,tab:cross_twitter}) suggest that format consistency (long form, instruction, short form) drives transfer more than domain vocabulary, with boldface patterns clustering along format based diagonals.

These questions are addressed through a detailed experimental framework with 30 trained models and 237 evaluation results across eight held-out test sets (Mixed Financial excludes WikiText evaluation), providing systematic evidence on data mixture effects in specialized-domain pretraining.

\section{Related Work}

A concise summary; Chapter 2 provides details.

\textbf{Financial NLP.} The domain covers sentiment analysis, question answering, numerical reasoning, and information extraction from regulatory documents \parencite{araci2019finbert, chen2021finqa}. Challenges include specialized vocabulary (``alpha,'' ``EBITDA''), domain reasoning patterns, and privacy constraints that push toward local deployment \parencite{wu2023bloomberggpt}. Existing models range from FinBERT variants \parencite{araci2019finbert, yang2020finbert} to BloombergGPT (50B, mixed 51\% financial and 49\% general) \parencite{wu2023bloomberggpt}. Most focus on single large models; few study mixture effects across sizes.

\textbf{Language model pretraining.} Modern LMs use causal language modeling (next-token prediction) on transformer architectures \parencite{vaswani2017attention, radford2019language, brown2020language}. Scaling laws \parencite{kaplan2020scaling, hoffmann2022training} show that model size, data size, and compute follow power-law relationships. Bigger models are more sample-efficient. But hyperparameter sensitivity at intermediate scales (0.6B–4B) is less studied. Chapter 2 covers training dynamics and memory constraints.

\textbf{Data mixture strategies.} Pretraining can be sequential (curriculum) or simultaneous; we use a simple “50cap” mixture and refer to Chapter 2 for rationale.

\textbf{Domain adaptation and transfer.} Transfer learning assumes general pretraining helps specialized tasks \parencite{devlin2019bert, pan2010transfer}. But \textcite{gururangan2020don} showed domain-adaptive pretraining (continued training on domain text) improves performance. Key challenges include catastrophic forgetting \parencite{kirkpatrick2017overcoming} and distribution shift \parencite{quinonero2009dataset}. Chapter 2 reviews the evidence; we test it empirically.

\textbf{Research gaps.} We address three gaps: in‑domain mixtures vs general corpora across sizes, dataset size thresholds for when mixing is necessary, and whether transfer follows format (long‑form, instruction, short‑form) more than vocabulary.

\section{Contributions}

This thesis makes six primary contributions to understanding data mixture effects and training dynamics for language model pretraining. First, we provide empirical data‑mixture guidelines: in‑domain diversity outperforms high‑quality general corpora for financial applications. Mixed financial datasets reach 21.55 perplexity at 4B versus 41.96 for WikiText (mean across financial evaluations after LR adjustment), a $1.95\times$ gap supported by 11 scaling figures and 18 tables.

Second, we document simple learning‑rate notes. All main runs used LR=2e‑5; in three abnormal cases (WikiText, Financial QA, Twitter), lowering LR (e.g., to $1\times10^{-5}$ or $5\times10^{-6}$) stabilized training and improved results (see \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} and \Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison}). These are pragmatic fixes, not a rule.

Third, we summarize dataset‑size effects: small datasets (<20K samples) overtrain badly (67–249 epochs) with high variance (70–97\% spread) and need mixing; medium datasets (20–100K) overtrain less (6–30 epochs) and can work for narrow use; large datasets (>100K) train stably (2–24 epochs) and can stand alone. These thresholds help decide when to mix under limited data budgets.

Fourth, we analyze cross‑domain interactions. Adding WikiText to financial mixtures (Mixed Wiki+Financial) worsens financial performance (26.69 ppl vs 21.55) while improving general performance only slightly—an unfavorable trade‑off for finance.

Fifth, we show feasibility of lightweight financial models. With the right mixtures and tuning, 0.6B–4B models deliver practical performance for edge deployment (our 4B model achieves 21.55 ppl on diverse financial tasks).

Sixth, we release an open‑source training pipeline with a multi‑dataset evaluation harness across 10 experiments and 30 models, supporting reproducible mixture composition and tuning.

\section{Thesis Organization}

What follows is simple and practical.

Chapter 2 (Background) gives only what we need: key ideas from financial NLP, pretraining basics, mixture strategies, and domain adaptation—enough context to place our study. Chapter 3 (Methodology) explains the design: Qwen3 models, datasets (7 financial sets, 321.4M tokens, plus WikiText), the 50cap rule, and training settings, including when we lowered LR. Chapter 4 (Results) presents the evidence with 11 figures and 18 tables, moving from mixtures to single datasets, then dynamics and transfer. Chapter 5 (Discussion) links results to prior work and practice, including why WikiText transfers weakly to finance and where LR tweaks helped. Chapter 6 (Conclusion) summarizes contributions, implications, and next steps.

\section{Scope and Limitations}

We study causal language model pretraining on financial text in the 0.6B to 4B range. Three notes help read the results fairly.

Model architecture: we used Qwen3 for all runs. The learning‑rate and mixture lessons should carry over, though checking LLaMA, Gemma, and Phi would add confidence. For mixture and evaluation, we kept a single rule (50cap) and reported perplexity on held‑out test sets—appropriate for studying pretraining dynamics but not downstream accuracy. Finally, we worked between 0.6B and 4B for hardware reasons; larger models may behave differently. Learning‑rate and dataset‑size takeaways likely transfer to other domains, but the weak benefit of WikiText appears finance‑specific.

Across 30 models and more than 240 evaluations, the patterns are consistent. Where evidence is thin, we say so.
