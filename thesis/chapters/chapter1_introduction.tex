\chapter{Introduction}

\section{Motivation}

Large language models (LLMs) have advanced very quickly in recent years \parencite{vaswani2017attention,radford2019language,brown2020language,touvron2023llama}. But using them in finance is still hard. Financial institutions and individual users work with sensitive information -- transactions, portfolios, even trading ideas -- that should not be sent to external APIs because of privacy rules and competition (e.g., GDPR) \parencite{eu2016gdpr}. So we need small financial language models that run locally, keep data safe, and still perform reasonably. Simple as that.

There are two common paths. Train a very large model from scratch, or fine-tune a general model on domain data. The first needs too much compute. The second often misses domain details \parencite{gururangan2020don}. People often assume that high-quality general text (such as Wikipedia or The Pile) always helps specialized applications. We were not fully convinced. So we check this point carefully \parencite{gao2020pile,raffel2020exploring,longpre2023pretrainer}.

The goal is simple: better data-mixing choices for a specialized domain \parencite{wu2023bloomberggpt}. In this work, we study how different data sources -- both in-domain financial text and out-of-domain high-quality corpora -- work together during pretraining. We focus on models with 0.6B to 4B parameters. They run on laptops and even some mobile devices. And they perform well enough for practice \parencite{yang2024qwen2,xia2023sheared,team2024gemma,javaheripi2023phi}. We run 10 pretraining setups across three model sizes.

This topic is timely because finance needs privacy-preserving AI systems. Rules like GDPR and new standards push for on-device processing \parencite{eu2016gdpr}. Put another way, as more teams try to train useful models with limited compute, it is important to understand what actually works in the 0.6B--4B range. And what does not.

Besides the practical value, the work also helps basic understanding of how models learn from different data distributions. We did not expect one thing: ``reverse scaling'' -- smaller models beating larger ones in some data regimes. We show these cases come from hyperparameter choices rather than a hard limitation \parencite{kaplan2020scaling,hoffmann2022training,mccandlish2018empirical}. This point matters for how we think about scaling laws and training behavior. In short: tune first, then judge size.

\section{Research Questions}

This thesis asks four concrete questions. Briefly. And plainly:

\textbf{RQ1: Data Mixture Composition}
Mixed financial datasets reach 21.55 ppl. Wiki+Financial mixtures reach 26.69 ppl. Pure WikiText reaches 48.7 ppl (\Cref{fig:scaling_comparison_all,tab:mixed_financial_results,tab:mixed_wiki_financial_results}). The pattern is clear. In our setting, in-domain diversity works best. The underlying question is simple: how do different combinations of in-domain financial datasets and out-of-domain general corpora affect performance and generalization? In particular, does mixing several financial datasets make the model more robust than training on only one, and does adding high-quality general text (WikiText) help or hurt financial tasks?

\textbf{RQ2: Model Size and Training Dynamics}
How do good training settings change across model sizes (0.6B, 1.7B, 4B)? And how sensitive are they to hyperparameters, especially the learning rate? We trained all main runs with LR=2e-5. In a few abnormal cases, we lowered LR in practice (e.g., to $1\times10^{-5}$ or $5\times10^{-6}$). Training stabilized. But we do not claim a general scaling rule.

\textbf{RQ3: Dataset Size Effects}
What is the minimum dataset size for effective standalone pretraining? How does dataset size shape overtraining patterns and cross-dataset generalization? And when do very small datasets need mixing? We report two practical thresholds: datasets $>$100M tokens train stably (\Cref{fig:scaling_news_articles,fig:scaling_sec_reports}), while datasets $<$20M tokens tend to overtrain and show high variance (89--97\%). In practice, mixing is then needed (\Cref{fig:scaling_financial_qa,fig:scaling_twitter,tab:cross_financial_qa,tab:cross_twitter}).

\textbf{RQ4: Domain Transfer Patterns}
Look at the cross-dataset comparison tables (\Cref{tab:cross_financial_news,tab:cross_financial_repor,tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa,tab:cross_twitter}). The bold cells line up along format-based diagonals, not domain boundaries. So format consistency (long-form, instruction, short-form) matters more than domain vocabulary. How well do models pretrained on financial data transfer to different task types (sentiment analysis, question answering, document understanding)? And how much do document format and task structure matter in this transfer?

To answer these questions, we trained 30 models and ran 240 evaluations on eight held-out test sets. The results provide careful evidence on data-mixing effects for a specialized domain. Not perfect. Informative. Enough to act on.

\section{Contributions}

Six things stand out about data mixing and training behavior for language model pretraining:

\textbf{1. Empirical Data Mixture Guidelines}
We give concrete recommendations for financial language model pretraining. In our setting, in-domain diversity beats high-quality general corpora. Mixed financial datasets reach 21.55 perplexity at 4B parameters. WikiText pretraining reaches 48.7 perplexity (mean across financial evaluations). That is a 2.3$\times$ gap. This questions the belief that general high-quality text always helps. We support the point with visual evidence: 11 scaling figures and 18 tables (10 per-training-dataset tables and 8 cross-dataset comparison tables).

\textbf{2. Learning Rate Notes}
All main experiments used LR=2e-5. In three follow-up runs that behaved abnormally (WikiText, Financial QA, Twitter), we lowered LR (e.g., to $1\times10^{-5}$ or $5\times10^{-6}$). Training stabilized and improved. These are practical fixes in our setting, not a general rule. Plots in \Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter} show this recovery; \Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison} list the metrics.

\textbf{3. Dataset Size Effects on Pretraining}
We describe empirical relationships between dataset size and training viability:
\begin{itemize}
    \item Small datasets (< 20K samples): extreme overtraining (67--249 epochs) and high variance (70--97\%); mixing is needed
    \item Medium datasets (20--100K samples): moderate overtraining (6--30 epochs); acceptable for some use cases
    \item Large datasets (> 100K samples): minimal overtraining (2--24 epochs); feasible for standalone pretraining
\end{itemize}
These results give practical guidance on when mixing is necessary versus when a single dataset is enough. They also help practitioners plan limited collection and annotation budgets. To be fair, the ranges are approximate and tied to our data.

\textbf{4. Cross-Domain Interaction Analysis}
We study how high-quality general corpora (WikiText) interact with domain-specific financial data during pretraining. Against the common view, WikiText gives limited benefit and sometimes hurts financial task performance. Mixed WikiText+Financial pretraining reaches 26.69 perplexity compared to 21.55 for pure financial mixing (about 24\% worse). Cross-dataset tables show this visually: WikiText rows rarely hold the best (boldface) cells across financial evaluations, while mixed financial rows often do better. Still, the precise balance depends on goals.

\textbf{5. Lightweight Financial Model Feasibility}
We show that 0.6B--4B parameter models can reach practical financial NLP performance with good data mixtures and careful hyperparameters, which makes edge deployment possible. Our 4B model reaches 21.55 perplexity on diverse financial tasks. It is competitive with much larger models, yet still runs on consumer hardware. In practice, that matters.

\textbf{6. Open-Source Training Pipeline}
We share a codebase for mixture-based pretraining with an evaluation framework across 10 experiments and 30 trained models. The pipeline supports automatic mixture composition, multi-dataset evaluation, and structured hyperparameter search.

\section{Thesis Organization}

In short, the thesis is organized as follows.

\textbf{Chapter 2: Background and Related Work} reviews financial NLP, pretraining objectives, data mixture strategies, and domain adaptation. It also places this work in the context of transfer learning and scaling laws.

\textbf{Chapter 3: Methodology} explains the experimental design: model (Qwen3 family), datasets (7 financial datasets totaling 207M tokens, plus WikiText), mixture rule (50cap), and training setup. It also notes how we found and handled learning rate sensitivity during development. In short: what we did and why.

\textbf{Chapter 4: Results} presents the main findings with figures and tables (11 scaling figures and 18 tables). We start with data-mixing effects (the central point), then analyze individual datasets, examine training dynamics and learning rate sensitivity, and finish with domain transfer patterns. The scaling figures show trends over model sizes, and the cross-dataset tables show which training choices work best in each evaluation. The story comes first; the sequence is secondary.

\textbf{Chapter 5: Discussion} interprets the results with prior work. We discuss why WikiText underperforms on financial tasks (using the table patterns), the benefit of in-domain diversity (reading the scaling trends), and practical notes on learning rate sensitivity. We end with actionable guidelines for practitioners.

\textbf{Chapter 6: Conclusion} summarizes contributions, discusses what they mean for research and practice, and outlines future directions: larger models, dynamic mixing, and downstream task evaluation.

\section{Scope and Limitations}

This thesis studies pretraining dynamics for causal language models in the 0.6B--4B range on financial text. The scope and limits are as follows.

\textbf{Model Architecture:} All experiments use the Qwen3 family. We expect the notes on learning rate and data mixing to generalize, but checking on other architectures (LLaMA, Gemma, Phi) would strengthen the claim.

\textbf{Data Mixture Strategy:} We use one mixture rule (50cap, capping the largest dataset at 50\%). Other strategies—square-root sampling, temperature sampling, or curriculum—are not explored and could behave differently.

\textbf{Evaluation Methodology:} We evaluate by perplexity on held-out test sets from the pretraining distribution. Perplexity correlates with downstream task quality, but we do not directly measure task accuracy (sentiment classification, NER, QA). This keeps the focus on pretraining dynamics, but limits direct application claims. Still, the correlation is useful.

\textbf{Scale Range:} We cover 0.6B to 4B parameters due to hardware limits. Larger models (7B+) could show different dynamics and data sensitivity. The studied range is still relevant for edge deployment.

\textbf{Domain Specificity:} While we work on financial text, some findings—especially learning rate and dataset size effects—are likely domain-agnostic. The point that WikiText gives limited benefit is domain-specific and may not hold in other fields.

Even with these limits, training 30 models and running 240 evaluations gives useful evidence for the claims. We mark what needs checking versus what is solid. Not everything is final. And that is fine.
