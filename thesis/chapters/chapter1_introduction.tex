\chapter{Introduction}

\section{Motivation}

Large language models have progressed rapidly \parencite{vaswani2017attention,radford2019language,brown2020language,touvron2023llama}. But finance is still hard. Banks and funds hold sensitive data: transactions, positions, internal notes. Sending this to external APIs is usually not acceptable. Privacy rules like GDPR prohibit such transfers \parencite{eu2016gdpr}. Competition matters too. So models should run locally. We target small models that keep data on device and still give useful performance. Put another way, the constraint shapes our design.

Two paths are common. Train a huge model from scratch, or fine-tune a general model on financial text. The first is costly—most teams cannot pay that compute bill. The second often misses domain details \parencite{gururangan2020don}. There is also a belief that adding high-quality general text (e.g., Wikipedia, The Pile) always helps. We test this claim directly \parencite{gao2020pile,raffel2020exploring,longpre2023pretrainer}.

The goal is straightforward: find effective data mixing for a specialized domain \parencite{wu2023bloomberggpt}. We study how in-domain financial text and out-of-domain general corpora interact during pretraining. We focus on 0.6B–4B parameter models. This range fits on laptops (some even on phones) and is practically useful \parencite{yang2024qwen2,xia2023sheared,team2024gemma,javaheripi2023phi}. We ran 10 pretraining configurations across three sizes. The setup is simple on purpose.

This topic matters now. Regulations tighten each year \parencite{eu2016gdpr}. Teams want on-device processing, and many groups have limited compute. Understanding what works in the 0.6B--4B range is critical. And knowing what does not work is also important. These constraints are real.

One observation stands out. In a few settings we saw ``reverse scaling'': smaller models beating larger ones. Not a fundamental limit—this came from hyperparameters \parencite{kaplan2020scaling,hoffmann2022training,mccandlish2018empirical}. The practical lesson: tune learning rate first, then judge model size.

\section{Research Questions}

Four questions drive this thesis. We state them simply.

\textbf{RQ1: Data Mixture Composition}
Start with the facts. Mixed financial datasets: 21.55 ppl. Wiki+Financial mixtures: 26.69 ppl. Pure WikiText: 48.7 ppl (\Cref{fig:scaling_comparison_all,tab:mixed_financial_results,tab:mixed_wiki_financial_results}). In our setup, in-domain diversity helps. The deeper question: how do different combinations of financial datasets and general corpora change performance? Does mixing several financial datasets improve stability versus a single dataset? And when we add high-quality general text (WikiText), does it help financial tasks—or does it hurt? We measure this directly.

\textbf{RQ2: Model Size and Training Dynamics}
Training setups change with model size (0.6B, 1.7B, 4B). How much, and how sensitive are the results to hyperparameters—especially learning rate? We used LR=2e-5 for the main runs, and in a few cases reduced LR to $1\times10^{-5}$ or $5\times10^{-6}$ to stabilize training. We do not claim a universal rule; these were practical fixes in our runs. We report the observed sensitivity.

\textbf{RQ3: Dataset Size Effects}
When is a dataset big enough for standalone pretraining? How does size affect overtraining and cross-dataset generalization? For small datasets, when is mixing not optional? We found two practical thresholds. Over 100M tokens: training is stable (\Cref{fig:scaling_news_articles,fig:scaling_sec_reports}). Below 20M: severe overtraining. Variance goes up to 89--97\%. Mixing becomes necessary (\Cref{fig:scaling_financial_qa,fig:scaling_twitter,tab:cross_financial_qa,tab:cross_twitter}). These cutoffs are practical, not theoretical.

\textbf{RQ4: Domain Transfer Patterns}
See the cross-dataset tables (\Cref{tab:cross_financial_news,tab:cross_financial_repor,tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa,tab:cross_twitter}). Bold cells line up by format, not by domain. In our results, format matters more than vocabulary. Long-form transfers to long-form; instructions to instructions; short-form stays isolated. The question: how well do financial-pretrained models transfer across task types—sentiment, Q\&A, document understanding—and how much do document format and task structure control that transfer? We quantify the role of format.

We trained 30 models to address these questions and ran 240 evaluations on eight held-out test sets. The evidence is not perfect, but it is useful for specialized domains. We treat it as guidance for practice.

\section{Contributions}

Six findings matter most.

\textbf{1. Empirical Data Mixture Guidelines}
We give concrete recommendations for financial pretraining. In our experiments, in-domain diversity beats high-quality general corpora. Mixed financial datasets reach 21.55 perplexity at 4B. WikiText pretraining yields 48.7 mean perplexity on financial evaluations—about 2.3$\times$ worse. This challenges the belief that general high-quality text always helps. We support the claim with visuals: 11 scaling figures and 18 tables (ten per-training-dataset; eight cross-dataset). The pattern is consistent.

\textbf{2. Learning Rate Notes}
All primary experiments used LR=2e-5. Three follow-ups behaved oddly (WikiText, Financial QA, Twitter), and we lowered LR to $1\times10^{-5}$ or $5\times10^{-6}$. Training stabilized and performance improved. These are practical fixes in our setting, not universal rules. The plots show the recovery (\Cref{fig:scaling_wikitext,fig:scaling_financial_qa,fig:scaling_twitter}); the tables list exact metrics (\Cref{tab:financial_qa_lr_comparison,tab:twitter_lr_comparison}). Small reductions were enough here.

\textbf{3. Dataset Size Effects on Pretraining}
We describe empirical thresholds linking dataset size to training viability:
\begin{itemize}
    \item Small datasets (< 20K samples): extreme overtraining (67--249 epochs), high variance (70--97\%); mixing required
    \item Medium datasets (20--100K samples): moderate overtraining (6--30 epochs); acceptable for narrow use cases
    \item Large datasets (> 100K samples): minimal overtraining (2--24 epochs); viable for standalone pretraining
\end{itemize}
These results offer practical guidance. When is mixing necessary? When is a single dataset enough? They also help teams plan limited annotation budgets. The ranges are approximate and tied to our data, and they match what we observed across runs.

\textbf{4. Cross-Domain Interaction Analysis}
We examined how high-quality general corpora (WikiText) interact with domain-specific financial data during pretraining. Conventional wisdom says they help. Our results are mixed. Sometimes WikiText adds little benefit; sometimes it hurts financial performance. Mixed WikiText+Financial pretraining reaches 26.69 perplexity; pure financial mixing 21.55—about 24\% worse when WikiText is added. Cross-dataset tables show this clearly. WikiText rows rarely have bold cells in financial evaluations; mixed financial rows often do. The right balance depends on the application.

\textbf{5. Lightweight Financial Model Feasibility}
Models in the 0.6B--4B range can reach practical financial NLP performance with good mixtures and careful tuning. This enables edge deployment. Our 4B model reaches 21.55 perplexity on diverse financial tasks, competitive with much larger models, while running on consumer hardware. For deployment, this size is a practical choice.

\textbf{6. Open-Source Training Pipeline}
We release a complete codebase for mixture-based pretraining. It includes an evaluation suite with 10 experiments and 30 trained models. It supports automatic mixture composition, multi-dataset evaluation, and structured hyperparameter search.

\section{Thesis Organization}

The thesis is organized as follows.

\textbf{Chapter 2: Background and Related Work} covers financial NLP, pretraining objectives, data mixture strategies, and domain adaptation. It also situates this work in transfer learning and scaling laws.

\textbf{Chapter 3: Methodology} details the experimental design. Model family (Qwen3). Datasets (7 financial datasets, 207M tokens total, plus WikiText). Mixture strategy (50cap rule). Training setup. We also explain how we discovered and addressed learning rate sensitivity during development. In short: what we did and why. And where we adjusted.

\textbf{Chapter 4: Results} presents findings with visuals: 11 scaling figures and 18 tables. We start with data-mixing effects—the core finding—then analyze individual datasets, examine training dynamics and learning rate sensitivity, and end with domain transfer patterns. Scaling figures show trends across model sizes. Cross-dataset tables show which approach works best for each evaluation scenario.

\textbf{Chapter 5: Discussion} interprets results against prior work. Why does WikiText underperform on financial tasks? We analyze table patterns. What are the benefits of in-domain diversity? We read the scaling trends. Learning rate sensitivity? Practical notes. We end with guidelines for practitioners.

\textbf{Chapter 6: Conclusion} summarizes contributions, discusses implications for research and practice, and outlines future directions: larger models, dynamic mixing strategies, and downstream task evaluation.

\section{Scope and Limitations}

This thesis examines pretraining dynamics for causal language models in the 0.6B--4B range on financial text. Below are the scope and limitations.

\textbf{Model Architecture:} All experiments use Qwen3. We expect the learning-rate and data-mixing observations to generalize, but validating on other architectures (LLaMA, Gemma, Phi) would strengthen the claim.

\textbf{Data Mixture Strategy:} We use one strategy, 50cap, which caps the largest dataset at 50\% of the mixture. Other strategies exist (square-root sampling, temperature sampling, curriculum). We did not test them; they could behave differently.

\textbf{Evaluation Methodology:} We evaluate using perplexity on held-out test sets from the pretraining distribution. Perplexity correlates with downstream quality, but we do not directly measure task accuracy (sentiment classification, NER, Q\&A). This keeps focus on pretraining dynamics and limits direct application claims. Still, the correlation is known to hold.

\textbf{Scale Range:} We cover 0.6B to 4B parameters due to hardware limits. Larger models (7B+) might show different dynamics and data sensitivity. The range we study remains relevant for edge deployment.

\textbf{Domain Specificity:} We work on financial text. Some findings likely generalize (learning-rate effects, dataset-size effects). Others are domain-specific. The limited benefit of WikiText, for instance, may not hold in other fields.

Training 30 models and running 240 evaluations provides evidence for our claims. We separate what needs further validation from what is well supported. Not everything is conclusive. Still, the patterns look consistent.
