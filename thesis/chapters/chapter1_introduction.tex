\chapter{Introduction}

\section{Motivation}

Large language models (LLMs) have rapidly changed how we do natural language processing \parencite{vaswani2017attention,radford2019language,brown2020language,touvron2023llama}. Yet using them in finance still brings practical hurdles. Financial institutions and individuals handle highly sensitive data—transactions, portfolios, trading strategies—that cannot be sent to external APIs for privacy and competitive reasons (e.g., GDPR) \parencite{eu2016gdpr}. We therefore need lightweight, locally runnable financial language models that maintain reasonable performance while protecting data.

In practice, domain adaptation tends to follow two paths: train very large models from scratch or fine-tune general models on domain data. Most teams cannot afford the first; the second often misses domain nuances \parencite{gururangan2020don}. And there is common practice showing that high quality general corpora (e.g., Wikipedia, The Pile) always help specialized applications. However, researchers also show that exceptions do appear \parencite{gao2020pile,raffel2020exploring,longpre2023pretrainer}.

This thesis studies how different data sources, both in‑domain financial data and out‑of‑domain high‑quality corpora, interact during pretraining. We focus on models in the 0.6B to 4B parameter range, which are realistic for laptops and some mobile devices while keeping acceptable performance \parencite{yang2024qwen2,xia2023sheared,team2024gemma,javaheripi2023phi}. Through systematic experiments across 10 pretraining configurations and three model sizes, we present evidence about data mixture strategies for specialized domains.

\section{Research Questions}

This thesis investigates the following core research questions.

\textbf{RQ1: Individual datasets versus mixtures}
Do data mixtures improve performance and consistency compared to individual dataset training? Contrary to conventional wisdom, our results show that \textbf{medium individual datasets (3.6–8.5M tokens) strictly dominate mixtures on both metrics}. FiQA (6.80 ppl, 19\% spread), FinGPT (7.03 ppl, 37\% spread), and Alpaca (8.73 ppl, 11.5\% spread) achieve 2.5–3.2$\times$ better perplexity AND 1.5–4.8$\times$ better cross-dataset consistency than Mixed Financial (21.55 ppl, 55\% spread). This finding (\Cref{fig:scaling_comparison_all,tab:fingpt_results,tab:fiqa_results,tab:alpaca_results,tab:mixed_financial_results}) overturns the mixture hypothesis—data diversity degrades both performance and robustness at fixed token budgets (100M).

\textbf{RQ2: Model size and training dynamics}
How do optimal training configurations vary across model sizes (0.6B, 1.7B, 4B parameters)? What is the relationship between size and learning rate sensitivity? In our setup, we trained all main runs with LR=2e-5; for a few abnormal cases, we reduced LR pragmatically (e.g., to $1\times10^{-5}$ or $5\times10^{-6}$) and saw improved stability.

\textbf{RQ3: Dataset size and quality effects}
What is the relationship between dataset size and performance? Surprisingly, we find a \textbf{non-monotonic relationship}: medium datasets (3.6-8.5M tokens) achieve the best results, outperforming both small (<1M) and large (>100M) datasets. FiQA (3.6M, 6.80 ppl), FinGPT (4.1M, 7.03 ppl), and Alpaca (8.5M, 8.73 ppl) substantially beat News (194M, 32.82 ppl) and SEC (8.1M, 17.80 ppl). This suggests data quality, focus, and format consistency matter more than scale. Medium datasets achieve optimal training (12–28 epochs per dataset) with format consistency, enabling focused learning. Large datasets undertrain (<1 epoch), while large mixtures combine undertraining with format inconsistency that small models (0.6B–4B) struggle to resolve. Small datasets overtrain (143–352 epochs), causing memorization. Only very small datasets (<1M tokens) exhibit severe overtraining (\Cref{fig:scaling_financial_qa,fig:scaling_twitter}), while medium datasets achieve optimal performance without mixing.

\textbf{RQ4: Domain transfer patterns}
How well do models pretrained on financial data transfer across task types (sentiment, question answering, document understanding), and how much does document format matter? Cross dataset comparison tables (\Cref{tab:cross_financial_news,tab:cross_financial_repor,tab:cross_alpaca,tab:cross_fingpt,tab:cross_fiqa,tab:cross_twitter}) suggest that format consistency (long form, instruction, short form) drives transfer more than domain vocabulary, with boldface patterns clustering along format based diagonals.

These questions are addressed through a detailed experimental framework with more than 30 trained models and 230 evaluation results across eight held-out test sets, providing systematic evidence on data mixture effects in specialized-domain pretraining.

\section{Related Work}

\textbf{Financial NLP}. The domain covers sentiment analysis, question answering, numerical reasoning, and information extraction from regulatory documents \parencite{araci2019finbert, chen2021finqa}. Challenges include specialized vocabulary (``alpha,'' ``EBITDA''), domain reasoning patterns, and privacy constraints that push toward local deployment \parencite{wu2023bloomberggpt}. Existing models range from FinBERT variants \parencite{araci2019finbert, yang2020finbert} to BloombergGPT (50B, mixed 51\% financial and 49\% general) \parencite{wu2023bloomberggpt}. Most focus on single large models, while few study mixture effects across sizes.

\textbf{Language model pretraining}. Modern LMs use causal language modeling (next-token prediction) on transformer architectures \parencite{vaswani2017attention, radford2019language, brown2020language}. Scaling laws \parencite{kaplan2020scaling, hoffmann2022training} show that model size, data size, and compute follow power-law relationships. Bigger models are more sample-efficient. But hyperparameter sensitivity at intermediate scales (0.6B–4B) is less studied. Chapter 2 covers training dynamics and memory constraints.

\textbf{Data mixture strategies}. Pretraining can be sequential (curriculum) or simultaneous. In our study, we use a simple “50cap” mixture and refer to Chapter 2 for rationale.

\textbf{Domain adaptation and transfer}. Transfer learning assumes general pretraining helps specialized tasks \parencite{devlin2019bert, pan2010transfer}. But \textcite{gururangan2020don} showed domain-adaptive pretraining (continued training on domain text) improves performance. Key challenges include catastrophic forgetting \parencite{kirkpatrick2017overcoming} and distribution shift \parencite{quinonero2009dataset}. 

\section{Contributions}

This thesis makes six primary contributions that challenge conventional assumptions about data mixture strategies for language model pretraining.

First, we overturn the mixture hypothesis: \textbf{medium individual datasets (3.6–8.5M tokens) strictly dominate mixtures on both performance and consistency}. FiQA (6.80 ppl, 19\% spread), FinGPT (7.03 ppl, 37\% spread), and Alpaca (8.73 ppl, 11.5\% spread) achieve 2.5–3.2$\times$ better perplexity AND 1.5–4.8$\times$ better cross-dataset consistency than Mixed Financial (21.55 ppl, 55\% spread). This finding contradicts widespread assumptions that data diversity improves robustness. We explain why mixtures fail through analysis of format inconsistency, vocabulary dilution, and multi-task interference, supported by 11 scaling figures and 18 detailed evaluation tables.

Second, we establish a \textbf{non-monotonic relationship between dataset size and performance}: medium datasets (3.6–8.5M) substantially outperform large datasets (>100M). FiQA (3.6M), FinGPT (4.1M), and Alpaca (8.5M) beat News (194M) by 2–5$\times$ on average perplexity, suggesting data quality, focus, and format consistency matter more than scale. SEC (8.1M) also performs well as a medium dataset. Only very small datasets (<1M) exhibit severe overtraining and require careful tuning or exclusion.

Third, we document pragmatic learning-rate adjustments. All main runs used LR=2e-5; in three cases with training instabilities (WikiText, Financial QA, Twitter), reducing LR (to $1\times10^{-5}$ or $5\times10^{-6}$) stabilized optimization. These are empirical fixes, not theoretical scaling laws.

Fourth, we analyze domain transfer patterns, finding format consistency drives transfer more than vocabulary overlap. Long-form documents transfer well (News $\leftrightarrow$ SEC), instruction tasks cluster (FinGPT/Alpaca/FiQA), but cross-format transfer fails despite shared domain.

Fifth, we demonstrate WikiText shows scale-dependent behavior: competitive at 0.6B (9.68 ppl) but reverse scaling at 4B (31.54 ppl) due to training instability, not domain mismatch. Adding WikiText to financial mixtures degrades performance (26.69 ppl vs 21.55 for pure financial).

Lastly, we show lightweight financial models (0.6B-4B) deliver practical performance for edge deployment when trained on focused medium datasets, enabling privacy-preserving financial NLP without external API dependencies.
