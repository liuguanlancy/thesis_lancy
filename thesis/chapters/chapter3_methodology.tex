\chapter{Methodology}

This chapter explains how we ran the study. First, the design. Then models, datasets, training setup, and finally evaluation. The goal is simple: fair, repeatable comparisons. We keep the setup practical—what we could run reliably with our compute. No more, no less.

\section{Experimental Design Overview}

We evaluate \textbf{10 pretraining configurations}. Two mixtures (Financial; Wiki+Financial). Eight single-dataset baselines. Each configuration trains at three sizes (0.6B/1.7B/4B). All with a fixed \textbf{100M-token budget}. We evaluate on \textbf{8 held-out test sets}. We also run six follow-ups with smaller learning rates to address stability at larger scale. \Cref{tab:exp_settings} summarizes the settings. In short: fix tokens, vary size and data. Simple design, clear comparisons.

\input{tables/table_experimental_settings}

This design targets our questions on mixture composition, model scale, dataset size, and domain transfer. Results appear in Chapter 4.

\section{Model Architecture}

We use the \textbf{Qwen3} family \parencite{yang2024qwen2}. Qwen3 is an open-source, decoder-only transformer family pretrained on diverse multilingual corpora. It uses grouped-query attention (GQA) for memory efficiency and supports standard and flash attention. We select three sizes from the Qwen3-Base series (pretrained checkpoints without post-training alignment). Specs are in \Cref{tab:model_specs}.

\begin{table}[h]
\centering
\caption[Qwen3 Model Specifications]{Qwen3 model specifications across three scales. All models use the same tokenizer (151,643 tokens) and support 32K context length. Training memory shown for bfloat16 precision.}
\label{tab:model_specs}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Parameters} & \textbf{Layers} & \textbf{Hidden} & \textbf{Heads} & \textbf{GQA} & \textbf{Memory} \\
\midrule
Qwen3-0.6B & 600M & 16 & 1024 & 16 & 4 & $\sim$4GB \\
Qwen3-1.7B & 1.7B & 24 & 2048 & 16 & 4 & $\sim$10GB \\
Qwen3-4B & 4.0B & 40 & 2560 & 20 & 4 & $\sim$20GB \\
\bottomrule
\end{tabular}
\end{table}

Why Qwen3? Three reasons. Consistent architecture across sizes enables clean comparisons. Strong baselines on general and domain tasks. And inference is efficient—these models fit on consumer hardware for edge deployment. So we can test ideas quickly.

\section{Datasets}

\subsection{Financial Datasets}

We curate seven financial datasets covering multiple tasks, document types, and scales (total: 207M tokens), summarized in \Cref{tab:financial_datasets}. Sizes vary from 0.3M to 197M tokens. Formats include news, reports, Q\&A, and social media. Formality ranges from regulatory filings to tweets. This lets us study intra-domain diversity.

\begin{table}[h]
\centering
\caption[Financial Dataset Characteristics]{Financial dataset characteristics. Total: 207M tokens across 7 datasets with diverse genres and scales.}
\label{tab:financial_datasets}
\small
\begin{tabular}{p{3cm}cccp{5.5cm}}
\toprule
\textbf{Dataset} & \textbf{Examples} & \textbf{Tokens} & \textbf{Genre} & \textbf{Description} \\
\midrule
Lettria Financial News & 300K & 197M & Journalism & Long-form articles on markets, earnings, policy \\
\midrule
SEC Financial Reports & 54.3K & 80M & Regulatory & 10-K/10-Q excerpts with formal disclosures, legal language \\
\midrule
FinGPT Sentiment & 76.8K & 19.1M & Instruction & Headlines + sentiment labels in conversational format \\
\midrule
Finance Alpaca & 68.9K & 17.2M & Q\&A & Instruction-response pairs on financial concepts \\
\midrule
FiQA & 17.4K & 4.3M & Forum & User-generated Q\&A from forums and microblogs \\
\midrule
Financial QA 10K & 7.1K & 3.5M & Document & Questions on 10-K filings requiring tabular reasoning \\
\midrule
Twitter Sentiment & 1.1K & 0.3M & Social Media & Labeled tweets ($<$280 chars) with informal language \\
\bottomrule
\end{tabular}
\end{table}

\subsection{WikiText}

We use \textbf{WikiText-103} \parencite{merity2016pointer} as a general-domain baseline (\Cref{tab:wikitext_dataset}). It serves two purposes: evaluate domain transfer (general $\leftrightarrow$ financial), and test whether high-quality general text complements financial pretraining in mixtures.

\begin{table}[h]
\centering
\caption[WikiText Dataset Characteristics]{WikiText-103 characteristics. Similar scale to SEC; smaller than News.}
\label{tab:wikitext_dataset}
\small
\begin{tabular}{p{3cm}cccp{5.5cm}}
\toprule
\textbf{Dataset} & \textbf{Examples} & \textbf{Tokens} & \textbf{Genre} & \textbf{Description} \\
\midrule
WikiText-103 & 103K & 103M & Encyclopedia & Verified Wikipedia articles with formal register, broad topical coverage, clean preprocessing \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Mixture Strategies}

We employ a \textbf{50\% capping strategy} (``50cap'') for dataset mixing to balance diversity with data efficiency. The algorithm works as follows:

\textbf{Step 1 — Cap dominant datasets}: Identify the largest dataset in the mixture. If its token count exceeds 50\% of the total mixture, cap it at exactly 50\%. This prevents any single dataset from dominating the mixture.

\textbf{Step 2 - Proportional sampling}: For remaining datasets (below 50\% threshold), sample tokens proportionally to their original sizes. This preserves relative contributions while ensuring diversity.

\textbf{Step 3 - Token-level interleaving}: During training, sample batches from the mixed distribution at the token level (not example level). This ensures fine-grained mixing throughout training rather than sequential block exposure.

\textbf{Example}: For the 7-dataset financial mixture (News 197M, SEC 80M, FinGPT 19M, Alpaca 17M, FiQA 4M, Financial QA 3.5M, Twitter 0.3M; total 321M tokens):
\begin{itemize}
\item News exceeds 50\% (61.4\%), capped at 50\% (160.5M tokens)
\item Remaining datasets sampled proportionally from 160.5M token budget
\item Final mixture: $\sim$321M tokens with News contributing exactly 50\%
\end{itemize}

For the 8-dataset WikiText+Financial mixture, WikiText (100M) and News (197M) are both large; we apply 50cap to ensure neither dominates, then proportionally sample the other 6 financial datasets.

This strategy contrasts with temperature sampling (which requires tuning hyperparameters) and equal mixing (which severely undersamples large datasets). The 50cap approach is deterministic, requires no tuning, and empirically performs well in production settings \parencite{longpre2023pretrainer}.

\section{Training Setup and Hyperparameter Tuning}

\subsection{Initial Configuration}

All models were trained with uniform hyperparameters across scales to establish baseline performance. The configuration follows standard practices for causal language modeling:

\textbf{Optimizer}: AdamW with $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=10^{-8}$, weight decay $0.01$

\textbf{Learning Rate}: $2 \times 10^{-5}$ (used for all main settings)

\textbf{LR Schedule}: Cosine decay with 1,000 warmup steps, minimum LR $10^{-6}$

\textbf{Batch Configuration}: Effective batch size 8 across all runs. When device memory was insufficient for a given model/sequence length, we used gradient accumulation to maintain the same effective batch size.

\textbf{Sequence Length}: 1,024 tokens (fixed for all runs)

\textbf{Precision}: bfloat16 mixed precision for memory efficiency

\textbf{Training Duration}: Dataset-dependent. Small datasets ($<$20K samples) trained for maximum epochs to reach $\sim$100M token budget; large datasets trained for 2-5 epochs. All models exposed to approximately 100M training tokens for fair comparison.

\textbf{Hardware}: NVIDIA RTX A6000 (48GB), A100 (40GB), and H100 (80GB) GPUs rented from Lambda Labs. Gradient accumulation was applied as needed to fit memory constraints.

When we observed abnormalities in a few experiments, we reran those specific cases with smaller LRs as a simple heuristic to stabilize training. We do not claim any theoretical scaling rule for LR; these adjustments were pragmatic.

\subsection{Pragmatic Learning Rate Adjustments}

In three configurations we observed abnormal behavior (e.g., larger models underperforming smaller ones). For these few cases, we retried with smaller learning rates (e.g., $1\times 10^{-5}$ or $5\times 10^{-6}$) purely as a practical heuristic to stabilize training. We do not propose or rely on a learning-rate scaling theory in this work. LR-comparison tables for the affected settings are reported in Chapter~4.

\subsection{Other Hyperparameters}

Beyond learning rate, we maintained consistent hyperparameters across experiments:

\textbf{Batch Size and Accumulation}: Effective batch size 8 across all runs. We used gradient accumulation only when necessary to fit models and sequence lengths into GPU memory.

\textbf{Warmup Steps}: 1,000 steps (3.1\% of training for 32K total steps) stabilized the initial phase. Longer warmup did not improve final performance.

\textbf{Training Epochs}: Varied by dataset size to normalize token exposure. Small datasets (Twitter, Financial QA) trained for 67--249 epochs to reach the 100M-token budget; medium datasets (FiQA, FinGPT, Alpaca) for 6--30 epochs; large datasets (SEC, News) for 2--24 epochs. This normalization ensures fair comparison across datasets of different sizes.

\textbf{Maximum Sequence Length}: 1,024 tokens. Financial documents often exceed this length (SEC filings: 10K+ tokens), but longer sequences quadratically increase memory and slow training. We accept truncation as a practical trade-off.

\textbf{Dropout}: 0.0 (no dropout), following common practice for large-scale pretraining where overfitting is rarely observed.

\subsection{Computational Budget}

For fairness, every run uses \textbf{100M tokens}, regardless of dataset size or model scale. This controls data exposure while we study model size and data characteristics.

\textbf{Experimental scale}. We ran 36 training jobs:
\begin{itemize}
    \item \textbf{2 mixture experiments}: Mixed Financial (7 datasets combined), Mixed Wiki+Financial (7 financial + WikiText)
    \item \textbf{8 individual datasets}: WikiText, Financial News, SEC Reports, FinGPT, Finance Alpaca, FiQA, Financial QA 10K, Twitter Financial
    \item \textbf{3 model sizes per configuration}: 0.6B, 1.7B, 4B parameters across all 10 settings = 30 baseline runs
    \item \textbf{6 additional learning rate adjustment runs}: Upon observing abnormalities in the baseline results, we conducted follow-up experiments with adjusted learning rates for three datasets (WikiText 1.7B \& 4B, Financial QA 1.7B \& 4B, Twitter 1.7B \& 4B) to investigate hyperparameter sensitivity at scale
\end{itemize}

\textbf{Total compute}. $36 \times 100\text{M} = 3.6\text{B}$ tokens processed. On a single NVIDIA A100 (40GB) from Lambda Labs, each 100M-token run took 2--8 hours by size (0.6B: $\sim$2h; 1.7B: $\sim$4h; 4B: $\sim$8h). Roughly 150 GPU-hours in total.

This token-controlled design helps isolate model--data interactions rather than compute artifacts. Variable epoch counts (2--249) come from dataset size differences while keeping token exposure fixed. Put another way, same tokens, different stories.

\section{Evaluation Protocol}

\subsection{Multi-Dataset Evaluation}

Each trained model is evaluated on \textbf{8 held-out test sets} to measure in-domain and out-of-domain generalization:

\textbf{Financial} (7 datasets): Test splits from all seven financial training datasets (News, SEC, FinGPT, Alpaca, FiQA, Financial QA, Twitter). Measures generalization to unseen examples within each financial domain.

\textbf{General} (1 dataset): WikiText test split. Measures retention of general language capability and cross-domain transfer (financial $\to$ general and general $\to$ financial).

For a model trained on dataset $D$, evaluating on $D$ measures in-domain generalization; other datasets measure cross-dataset transfer. For mixed models, all eight test sets probe generalization across the mixture.

\subsection{Metrics}

We report three complementary metrics:

\textbf{Cross-Entropy Loss}. Primary metric; average negative log-likelihood per token.
\begin{equation*}
    \mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} \log P\bigl(w_i \,\mid\, w_{<i}\bigr)
\end{equation*}
Lower is better.

\textbf{Perplexity}. Interpretable transform of cross-entropy: $\text{PPL} = \exp(\mathcal{L})$. Roughly, the effective vocabulary size per prediction. PPL = 10 means the model effectively chooses among ten tokens on average. Lower is better. This is our primary comparison metric.

\textbf{Relative Spread (Coefficient of Variation)}. Cross-dataset variability. Compute CV\% on the eight perplexities (one per evaluation set) as
\begin{equation*}
    \text{CV}\% = 100\,\frac{\text{sample std. dev. of PPL}}{\text{mean PPL}}\, .
\end{equation*}
Lower CV\% means more consistent performance across datasets; higher CV\% means specialization or brittleness.

All metrics use full test sets (no subsampling), the same sequence length (1,024 tokens), and the same batch size as training. We evaluate the final checkpoint. No selection by validation, since we lack task-specific validation sets. Still, the comparisons remain fair.
