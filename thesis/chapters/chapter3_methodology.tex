\chapter{Methodology}

This chapter explains how we ran the experiments: first an overview of the design, then model architecture, datasets, training setup with tuning, and the evaluation protocol. We favor simple choices that are easy to reproduce. When trade‑offs appear, we keep the setup stable—deliberately straightforward by design.

\section{Experimental Design Overview}

We evaluate 10 pretraining configurations: 2 mixtures (Financial; Wiki+Financial) and 8 single‑dataset baselines. Each configuration is trained at three model sizes (0.6B/1.7B/4B) with a fixed 100M‑token budget and evaluated on eight held‑out test sets. We also run 6 follow‑up runs with adjusted learning rates to address training stability at larger scales. We kept other factors fixed where possible. \Cref{tab:exp_settings} summarizes the settings used throughout.

\input{tables/table_experimental_settings}

This design supports our research questions on mixture composition, model scale, dataset size, and domain transfer. Results appear in Chapter 4 with figures and tables. We report measurements; we avoid theory claims.

\section{Model Architecture}

We use the Qwen3 model family \parencite{yang2024qwen2,qwen3}, a series of open‑source transformer‑based decoder‑only language models pretrained on diverse multilingual corpora. Qwen3 employs grouped query attention (GQA) for memory efficiency and supports both standard and flash attention. We select three sizes from the Qwen3 Base series (pretrained checkpoints without post‑training alignment), detailed in \Cref{tab:model_specs}. In our runs, these sizes allow clean comparisons without changing tokenizers or context limits.

\begin{table}[h]
\centering
\caption[Qwen3 Model Specifications]{Qwen3 model specifications across three scales. All models use the same tokenizer (151,643 tokens) and support 32K context length. Training memory shown for bfloat16 precision.}
\label{tab:model_specs}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Parameters} & \textbf{Layers} & \textbf{Hidden} & \textbf{Heads} & \textbf{GQA} & \textbf{Memory} \\
\midrule
Qwen3-0.6B & 600M & 16 & 1024 & 16 & 4 & $\sim$4GB \\
Qwen3-1.7B & 1.7B & 24 & 2048 & 16 & 4 & $\sim$10GB \\
Qwen3-4B & 4.0B & 40 & 2560 & 20 & 4 & $\sim$20GB \\
\bottomrule
\end{tabular}
\end{table}

We chose Qwen3 for three reasons: (1) architectural consistency across scales enables clean size comparisons, (2) stable baseline performance on general and domain‑specific benchmarks, and (3) efficient inference suitable for edge deployment (all models fit on consumer hardware). Stated differently, it lets us study scale without changing too many other factors and reduces engineering noise.

\section{Datasets}

\subsection{Financial Datasets}

We curate 7 financial datasets spanning diverse tasks, document types, and data scales (total: 222.69M tokens), summarized in \Cref{tab:financial_datasets}. These datasets vary in size (0.28M to 197.38M tokens), format (news, reports, Q\&A, social media), and formality (regulatory filings vs tweets). This diversity lets us examine intra domain effects without changing models.

\begin{table}[h]
\centering
\caption[Financial Dataset Characteristics]{Financial dataset characteristics. Total: 222.69M tokens across 7 datasets with diverse genres and scales. Dataset identifiers listed in footnotes.}
\label{tab:financial_datasets}
\small
\begin{tabular}{p{3.4cm}cccp{5.5cm}}
\toprule
\textbf{Dataset} & \textbf{Examples} & \textbf{Tokens} & \textbf{Genre} & \textbf{Description} \\
\midrule
Financial News Articles$^1$ & 306.2K & 197.4M & Journalism & Long-form articles on markets, earnings, policy \\
\midrule
SEC Financial Reports$^2$ & 200K & 8.1M & Regulatory & 10-K annual filings with formal disclosures, legal language \\
\midrule
FinGPT Sentiment$^3$ & 76.8K & 4.1M & Instruction & Headlines + sentiment labels in conversational format \\
\midrule
Finance Alpaca$^4$ & 68.9K & 8.5M & Q\&A & Instruction-response pairs on financial concepts \\
\midrule
FiQA$^5$ & 14.5K & 3.6M & Forum & User-generated Q\&A from Stack Exchange Investment topic \\
\midrule
Financial QA 10K$^6$ & 7.0K & 0.7M & Document & Questions on recent 10-K filings requiring tabular reasoning \\
\midrule
Twitter Financial Sentiment$^7$ & 9.5K & 0.28M & Social Media & Labeled tweets ($<$280 chars) with informal language \\
\bottomrule
\multicolumn{5}{l}{\footnotesize $^1$\texttt{ashraq/financial-news-articles}, $^2$\texttt{JanosAudran/financial-reports-sec:small\_lite}, $^3$\texttt{FinGPT/fingpt-sentiment-train},} \\
\multicolumn{5}{l}{\footnotesize $^4$\texttt{gbharti/finance-alpaca}, $^5$\texttt{LLukas22/fiqa}, $^6$\texttt{virattt/financial-qa-10K}, $^7$\texttt{zeroshot/twitter-financial-news-sentiment}}
\end{tabular}
\end{table}

Our financial datasets cover diverse genres and formats. SEC reports (8.1M tokens, 200K filings) are 10‑K annual filings—no quarterly 10‑Qs—with formal regulatory language. FiQA (3.6M tokens, 14.5K examples) captures Stack Exchange investment discussions with user‑generated Q\&A. FinGPT headlines (4.1M tokens, 76.8K examples) provide sentiment labels in conversational format. The Twitter dataset (0.28M tokens, 9.5K tweets) includes Bearish/Bullish/Neutral labels with informal language. Financial QA (0.7M tokens, 7K pairs) draws from recent 10‑K filings requiring tabular reasoning. Finance Alpaca (8.5M tokens, 68.9K pairs) is synthetic instruction data—didactic Q\&A without time‑stamped grounding. WikiText (103M tokens, 1.8M articles) provides the general‑domain baseline.

This diversity in genres (journalism, regulatory, instruction, forum, social media, document Q\&A) and formality levels lets us test how models handle different financial communication styles. WikiText provides a general‑domain contrast.

\subsection{WikiText}

We use WikiText‑103 \parencite{merity2016pointer} as a general‑domain baseline, summarized in \Cref{tab:wikitext_dataset}. WikiText serves two purposes: (1) evaluating domain transfer (general $\leftrightarrow$ financial), and (2) testing whether high‑quality general corpora complement financial pretraining in mixtures.

\begin{table}[h]
\centering
\caption[WikiText Dataset Characteristics]{WikiText-103 characteristics. Similar scale to SEC; smaller than News. Dataset identifier in footnote.}
\label{tab:wikitext_dataset}
\small
\begin{tabular}{p{3.4cm}cccp{5.5cm}}
\toprule
\textbf{Dataset} & \textbf{Examples} & \textbf{Tokens} & \textbf{Genre} & \textbf{Description} \\
\midrule
WikiText-103$^8$ & 1.8M & 103M & Encyclopedia & Verified Wikipedia articles with formal register, broad topical coverage, clean preprocessing \\
\bottomrule
\multicolumn{5}{l}{\footnotesize $^8$\texttt{wikitext:wikitext-103-v1}}
\end{tabular}
\end{table}

\subsection{Mixture Strategies}

We use a 50\% capping strategy (``50cap'') for dataset mixing to balance diversity with data efficiency. If the largest dataset in a mixture exceeds 50\% of total tokens, we cap it at exactly 50\%; this prevents single‑source dominance. The remaining datasets are sampled proportionally to their original sizes, preserving relative contributions while ensuring diversity. During training, we sample batches from the mixed distribution at the token level, not by example. Fine‑grained interleaving throughout training beats sequential block exposure.

Example. For the 7‑dataset financial mixture (News 197.38M, SEC 8.12M, FinGPT 4.14M, Alpaca 8.46M, FiQA 3.60M, Financial QA 0.70M, Twitter 0.28M; total 222.69M tokens), News exceeds 50\% (88.6\%) and is capped at 50\% (111.34M tokens); the remaining datasets are sampled proportionally from the other 111.34M‑token budget; the final mixture stays at $\sim$222.69M tokens with News contributing exactly half.

Dataset alignment. The financial datasets vary in formality (regulatory SEC filings vs informal tweets), source type (news articles vs forum discussions), and task format (sentiment labels vs Q\&A pairs). WikiText represents general encyclopedic knowledge with a different topical distribution. We accept these distribution differences as realistic: real applications mix diverse sources with varying formality and topical coverage.

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{figures/diagram_50cap.png}
\caption[50cap Mixture Strategy Visualization]{Token allocation in Mixed Financial dataset using 50cap strategy. News Articles is capped to contribute at most 50\% in sampling (illustrative normalization; raw corpus remains 197.38M). The remaining six datasets are sampled proportionally from the other 50\%, ensuring diversity while preventing dominance. Left panel shows pie chart view; right panel shows stacked bar view with total allocation.}
\label{fig:diagram_50cap}
\end{figure}

\Cref{fig:diagram_50cap} visualizes the 50cap sampling policy: News Articles (red) is capped to at most 50\% of sampled tokens; the remaining 50\% is distributed proportionally among the other six datasets. The pie (left) shows percentage composition; the stacked bar (right) normalizes absolute counts to a 50/50 split (\(\approx\!111.34\,\text{M}+\approx\!111.34\,\text{M}\) if scaled to the 222.69M corpus total) for illustration only — 50cap does not modify raw corpus sizes.

For the 8-dataset WikiText+Financial mixture, WikiText (103M) and News (197.38M) are both large; we apply 50cap to ensure neither dominates, then proportionally sample the other 6 financial datasets.

This strategy contrasts with temperature sampling (which requires tuning hyperparameters) and equal mixing (which severely undersamples large datasets). The 50cap approach is deterministic, requires no tuning, and empirically performs well in production settings \parencite{longpre2023pretrainer}.

\section{Training Setup and Hyperparameter Tuning}

\subsection{Initial Configuration}

We trained all models with a single hyperparameter template to set a baseline. Standard causal LM choices:

We used AdamW ($\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=10^{-8}$, weight decay $0.01$) with an initial learning rate of $2\times10^{-5}$, cosine decay, 1{,}000 warmup steps, and minimum LR $10^{-6}$. The effective batch size was 8 across all runs; when memory was tight, we used gradient accumulation to maintain that size. Sequences were 1{,}024 tokens with bfloat16 mixed precision. Training duration was dataset‑dependent: small datasets ($<$20K samples) trained many epochs to reach $\sim$100M tokens, while large datasets trained for 2–5 epochs. We used NVIDIA RTX A6000 (48GB), A100 (40GB), and H100 (80GB) GPUs rented from Lambda Labs.

When we observed abnormalities in a few experiments, we reran those specific cases with smaller LRs as a simple heuristic to stabilize training. We do not claim any theoretical scaling rule for LR; these adjustments were pragmatic.

\subsection{Pragmatic Learning Rate Adjustments}

In three configurations we observed abnormal behavior (e.g., larger models underperforming smaller ones). For these few cases, we retried with smaller learning rates (e.g., $1\times 10^{-5}$ or $5\times 10^{-6}$) purely as a practical heuristic to stabilize training. We do not propose or rely on a learning-rate scaling theory in this work. LR-comparison tables for the affected settings are reported in Chapter~4.

\subsection{Other Hyperparameters}

Beyond learning rate, we kept other hyperparameters consistent: effective batch size 8 (using gradient accumulation as needed), warmup of 1{,}000 steps (3.1\% of 32K total steps), and dropout 0.0. Training epochs varied by dataset size to normalize token exposure: small datasets (Twitter, Financial QA) needed 67–249 epochs to reach 100M tokens; medium ones (FiQA, FinGPT, Alpaca) 6–30; large ones (SEC, News) 2–24. We fixed maximum sequence length at 1{,}024 tokens; although financial documents often exceed this, longer sequences increase memory quadratically, so we accepted truncation as a practical trade‑off.

\subsection{Computational Budget}

To ensure fair comparison across experiments, we normalized the token budget to 100M tokens per training run, regardless of dataset size or model scale. In total we ran 36 trainings: two mixture settings (Mixed Financial; Mixed Wiki+Financial), eight single‑dataset baselines (WikiText, Financial News, SEC, FinGPT, Finance Alpaca, FiQA, Financial QA 10K, Twitter), each at three sizes (0.6B/1.7B/4B) for 30 baselines, plus six follow‑ups with reduced learning rates on the three problematic datasets (WikiText, Financial QA, Twitter) to probe sensitivity at larger scales. The total computational cost was $36\times100\text{M}=3.6\text{B}$ tokens. On a single NVIDIA A100 (40GB), each 100M‑token run required 2–8 hours depending on model size (0.6B: $\sim$2h; 1.7B: $\sim$4h; 4B: $\sim$8h), about 150 GPU‑hours overall.

This token controlled design helps ensure that performance differences reflect model data interactions rather than unequal training compute. Variable epoch counts (2 to 249 across experiments) follow from dataset size while keeping token exposure constant. But it also means small datasets see many passes. A trade off we accept.

\section{Evaluation Protocol}

\subsection{Multi-Dataset Evaluation}

Each trained model is evaluated on eight held‑out test sets to measure both in‑domain and out‑of‑domain generalization: the seven financial test splits (News, SEC, FinGPT, Alpaca, FiQA, Financial QA, Twitter) plus the WikiText test split to gauge retention of general language capabilities and cross‑domain transfer.

For models trained on dataset $D$, evaluation on $D$'s test set measures in-domain generalization; evaluation on other datasets measures cross-dataset transfer. For mixed models, all 8 test sets measure generalization across the mixture distribution.

\subsection{Metrics}

We report three complementary metrics. Cross‑entropy loss is the average negative log‑likelihood per token,
\begin{equation*}
    \mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} \log P\bigl(w_i \,\mid\, w_{<i}\bigr)
\end{equation*}
with lower being better. Perplexity is the interpretable transformation $\text{PPL}=\exp(\mathcal{L})$, representing the effective vocabulary size considered at each prediction (lower is better). Relative spread measures cross‑dataset variability:
\begin{equation*}
    \text{Relative Spread}\% = 100\,\frac{\max(\text{PPL}) - \min(\text{PPL})}{\text{mean PPL}}\, ,
\end{equation*}
computed over evaluation perplexities (one per dataset); lower values indicate more consistent generalization.

All metrics are computed on full test sets (no subsampling) with the same sequence length (1,024 tokens) and batch size used during training. Evaluation uses the final checkpoint from training (no checkpoint selection based on validation performance, as we lack task-specific validation sets).
