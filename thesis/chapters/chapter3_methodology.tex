\chapter{Methodology}

This chapter explains how we ran the experiments: we first provide an overview of the design, then model architecture, datasets, training setup with tuning, and lastly evaluation protocol.

\section{Experimental Design Overview}

We evaluate 10 pretraining configurations: 2 mixtures (Financial; Wiki+Financial) and 8 single‑dataset baselines. Each configuration is trained at three model sizes (0.6B/1.7B/4B) with a fixed 100M‑token budget and evaluated on eight held‑out test sets. We also run 6 follow‑up runs with adjusted learning rates to address training stability at larger scales. We kept other factors fixed where possible. \Cref{tab:exp_settings} summarizes the settings used throughout.

\input{tables/table_experimental_settings}

This design supports our research questions on mixture composition, model scale, dataset size, and domain transfer. We detailed the results in Chapter 4.

\section{Model Architecture}

We use the Qwen3 model family \parencite{yang2024qwen2,qwen3}, a series of open‑source transformer‑based decoder‑only language models pretrained on diverse multilingual corpora. Qwen3 employs grouped query attention (GQA) for memory efficiency and supports both standard and flash attention. We select three sizes from the Qwen3 Base series (pretrained checkpoints without post‑training alignment), detailed in \Cref{tab:model_specs}. In our experiments, these different model sizes allow clean comparisons without changing tokenizers or context limits.

\begin{table}[htbp]
\centering
\caption[Qwen3 Model Specifications]{Qwen3 model specifications across three scales. All models use the same tokenizer (151,643 tokens) and support 32K context length. Training memory shown for bfloat16 precision.}
\label{tab:model_specs}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Parameters} & \textbf{Layers} & \textbf{Hidden} & \textbf{Heads} & \textbf{GQA} & \textbf{Memory} \\
\midrule
Qwen3-0.6B & 600M & 16 & 1024 & 16 & 4 & $\sim$4GB \\
Qwen3-1.7B & 1.7B & 24 & 2048 & 16 & 4 & $\sim$10GB \\
Qwen3-4B & 4.0B & 40 & 2560 & 20 & 4 & $\sim$20GB \\
\bottomrule
\end{tabular}
\end{table}

We chose Qwen3 for four reasons: (1) architectural consistency across scales enables clean size comparisons, (2) stable baseline performance on general and domain‑specific benchmarks, (3) efficient inference suitable for edge deployment (all models fit on consumer hardware, (4) SOTA performance of open-weight language models.

\section{Datasets}

\subsection{Financial Datasets}

We use 7 financial datasets spanning diverse tasks, document types, and data scales (total: 222.69M tokens), summarized in \Cref{tab:financial_datasets}. These datasets vary in size (0.28M to 197.38M tokens), genre (news, reports, Q\&A, social media), and formality (regulatory filings vs tweets). This diversity lets us examine intra domain effects without changing models.

\begin{table}[htbp]
\centering
\caption[Financial Dataset Characteristics]{Financial dataset characteristics. Total: 222.69M tokens across 7 datasets with diverse genres and scales. Dataset identifiers listed in footnotes.}
\label{tab:financial_datasets}
\small
\begin{tabular}{p{3.4cm}cccp{5.5cm}}
\toprule
\textbf{Dataset} & \textbf{Examples} & \textbf{Tokens} & \textbf{Genre} & \textbf{Description} \\
\midrule
Financial News Articles$^1$ & 306.2K & 194.5M & Journalism & Long-form articles on markets, earnings, policy \\
\midrule
SEC Financial Reports$^2$ & 200K & 8.1M & Regulatory & 10-K annual filings with formal disclosures, legal language \\
\midrule
FinGPT Sentiment$^3$ & 76.8K & 4.1M & Instruction & Headlines + sentiment labels in conversational format \\
\midrule
Finance Alpaca$^4$ & 68.9K & 8.5M & Q\&A & Instruction-response pairs on financial concepts \\
\midrule
FiQA$^5$ & 14.5K & 3.6M & Forum & User-generated Q\&A from Stack Exchange Investment topic \\
\midrule
Financial QA 10K$^6$ & 7.0K & 0.7M & Document & Questions on recent 10-K filings requiring tabular reasoning \\
\midrule
Twitter Financial Sentiment$^7$ & 9.5K & 0.28M & Social Media & Labeled tweets ($<$280 chars) with informal language \\
\bottomrule
\multicolumn{5}{l}{\footnotesize $^1$\texttt{ashraq/financial-news-articles}, $^2$\texttt{JanosAudran/financial-reports-sec:small\_lite}, $^3$\texttt{FinGPT/fingpt-sentiment-train},} \\
\multicolumn{5}{l}{\footnotesize $^4$\texttt{gbharti/finance-alpaca}, $^5$\texttt{LLukas22/fiqa}, $^6$\texttt{virattt/financial-qa-10K}, $^7$\texttt{zeroshot/twitter-financial-news-sentiment}}
\end{tabular}
\end{table}

Our financial datasets cover diverse genres and formats. SEC reports (8.1M tokens, 200K filings) are 10‑K annual filings with formal regulatory language. FiQA (3.6M tokens, 14.5K examples) captures Stack Exchange investment discussions with user‑generated Q\&A. FinGPT headlines (4.1M tokens, 76.8K examples) provide sentiment labels in conversational format. The Twitter dataset (0.28M tokens, 9.5K tweets) includes Bearish/Bullish/Neutral labels with informal language. Financial QA (0.7M tokens, 7K pairs) draws from recent 10‑K filings requiring tabular reasoning. Finance Alpaca (8.5M tokens, 68.9K pairs) is synthetic instruction data—didactic Q\&A without time‑stamped grounding. WikiText (124M tokens, 1.8M articles) provides the general‑domain baseline.

This diversity in genres (journalism, regulatory, instruction, forum, social media, document Q\&A) and formality levels lets us test how models handle different financial communication styles. WikiText provides a general‑domain contrast.

\subsection{WikiText}

We use WikiText-103 \parencite{merity2016pointer} as a general‑domain baseline, summarized in \Cref{tab:wikitext_dataset}. WikiText serves two purposes: (1) evaluating domain transfer (general $\leftrightarrow$ financial), and (2) testing whether high‑quality general corpora complement financial pretraining in mixtures.

\begin{table}[htbp]
\centering
\caption[WikiText Dataset Characteristics]{WikiText-103 characteristics. Similar scale to SEC; smaller than News. Dataset identifier in footnote.}
\label{tab:wikitext_dataset}
\small
\begin{tabular}{p{3.4cm}cccp{5.5cm}}
\toprule
\textbf{Dataset} & \textbf{Examples} & \textbf{Tokens} & \textbf{Genre} & \textbf{Description} \\
\midrule
WikiText-103$^8$ & 1.8M & 124M & Encyclopedia & Verified Wikipedia articles with formal register, broad topical coverage, clean preprocessing \\
\bottomrule
\multicolumn{5}{l}{\footnotesize $^8$\texttt{wikitext:wikitext-103-v1}}
\end{tabular}
\end{table}

\subsection{Mixture Strategies}

We use a 50\% capping strategy (``50cap'') for dataset mixing to balance diversity with data efficiency. If the largest dataset in a mixture exceeds 50\% of total tokens, we cap it at exactly 50\%. This prevents single‑source dominance. The remaining datasets are sampled proportionally to their original sizes in token counts, preserving relative contributions while ensuring diversity. During training, we sample batches from the mixed distribution at the token level, not by example.

We demonstrate our strategy with an example. For the 7‑dataset financial mixture (News 194.47M, SEC 8.12M, FinGPT 4.14M, Alpaca 8.46M, FiQA 3.60M, Financial QA 0.70M, Twitter 0.28M; total 219.77M tokens), News exceeds 50\% (88.5\%) and is capped at 50\% (109.89M tokens); the remaining datasets are sampled proportionally from the other 109.89M‑token budget; the final mixture stays at $\sim$219.77M tokens with News contributing exactly half.

For dataset alignment, the financial datasets vary in formality (regulatory SEC filings vs informal tweets), source type (news articles vs forum discussions), and task format (sentiment labels vs Q\&A pairs). WikiText represents general encyclopedic knowledge with a different topical distribution. We accept these distribution differences, as we believe that real applications mix diverse sources with varying formality and topical coverage.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/diagram_50cap.png}
\caption[50cap Mixture Strategy Visualization]{Token allocation in Mixed Financial dataset using 50cap strategy. News Articles is capped to contribute at most 50\% in sampling (illustrative normalization; raw corpus remains 194.47M). The remaining six datasets are sampled proportionally from the other 50\%, ensuring diversity while preventing dominance. Left panel shows pie chart view; right panel shows stacked bar view with total allocation.}
\label{fig:diagram_50cap}
\end{figure}

\Cref{fig:diagram_50cap} visualizes the 50cap sampling policy: News Articles (red) is capped to at most 50\% of sampled tokens; the remaining 50\% is distributed proportionally among the other six datasets. The pie (left) shows percentage composition; the stacked bar (right) normalizes absolute counts to a 50/50 split (\(\approx\!109.89\,\text{M}+\approx\!109.89\,\text{M}\) if scaled to the 219.77M corpus total) for illustration only — 50cap does not modify raw corpus sizes.

For the 8-dataset WikiText+Financial mixture, WikiText (123.58M) and News (194.47M) are both large; we apply 50cap to ensure neither dominates, then proportionally sample the other 6 financial datasets.

This strategy contrasts with temperature sampling (which requires tuning hyperparameters) and equal mixing (which severely undersamples large datasets). Our 50cap approach is deterministic, and requires no tuning.

\section{Training Setup and Hyperparameter Tuning}

\subsection{Initial Configuration}

We trained all models with a single hyperparameter template to set a baseline. 

We used AdamW ($\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=10^{-8}$, weight decay $0.01$) with an initial learning rate of $2\times10^{-5}$, cosine decay, 1{,}000 warmup steps, and minimum LR $10^{-6}$. The effective batch size was 8 across all runs; when memory was tight, we used gradient accumulation to maintain that size. Sequences were 1{,}024 tokens with bfloat16 mixed precision. Training duration was dataset‑dependent: large datasets ($>$100M tokens) trained for $<$1 epoch (News: 0.5 epochs, WikiText: 0.8 epochs), medium datasets (3.6–8.5M) for 12–28 epochs, and small datasets ($<$1M) for 143–352 epochs to reach the fixed 100M token budget.

When we observed abnormalities in a few experiments, we reran those specific cases with smaller LRs as a simple heuristic to stabilize training.

\subsection{Pragmatic Learning Rate Adjustments}

In three configurations we observed abnormal behavior (e.g., larger models underperforming smaller ones). For these few cases, we retried with smaller learning rates (e.g., $1\times 10^{-5}$ or $5\times 10^{-6}$) purely as a practical heuristic to stabilize training. We do not propose or rely on a learning-rate scaling theory in this work. LR-comparison tables for the affected settings are reported in Chapter~4.

\subsection{Other Hyperparameters}

Beyond learning rate, we kept other hyperparameters consistent: effective batch size 8 (using gradient accumulation as needed), warmup of 1{,}000 steps (8\% of 12K total steps), and dropout 0.0. Training epochs varied by dataset size to normalize token exposure: small datasets (Twitter, Financial QA) needed 143–352 epochs to reach 100M tokens; medium ones (SEC, FiQA, FinGPT, Alpaca) 12–28 epochs; large ones (News, WikiText) 0.5–0.8 epochs. We fixed maximum sequence length at 1{,}024 tokens; although financial documents often exceed this, longer sequences increase memory quadratically, so we accepted truncation as a practical trade‑off.

\subsection{Computational Budget}

To ensure fair comparison across experiments, we normalized the token budget to 100M tokens per training run, regardless of dataset size or model scale. In total we ran 36 trainings: two mixture settings (Mixed Financial; Mixed Wiki+Financial), eight single‑dataset baselines (WikiText, Financial News, SEC, FinGPT, Finance Alpaca, FiQA, Financial QA 10K, Twitter), each at three sizes (0.6B/1.7B/4B) for 30 baselines, plus six follow‑ups with reduced learning rates on the three problematic datasets (WikiText, Financial QA, Twitter) to probe sensitivity at larger scales. The total computational cost was $36\times100\text{M}=3.6\text{B}$ tokens.

This token controlled design helps ensure that performance differences reflect model data interactions rather than unequal training compute. Variable epoch counts (2 to 249 across experiments) follow from dataset size while keeping token exposure constant. But it also means small datasets see many passes. We accept this trade-off for fair comparisons across different settings.

\section{Evaluation Protocol}

\subsection{Multi-Dataset Evaluation}

Each trained model is evaluated on eight held‑out test sets to measure both in‑domain and out‑of‑domain generalization: seven financial test splits (News, SEC, FinGPT, Alpaca, FiQA, Financial QA, Twitter) plus WikiText test split to evaluate general language capabilities and cross‑domain transfer.

For models trained on dataset $D$, evaluation on $D$'s test set measures in-domain generalization; evaluation on other datasets measures cross-dataset transfer. For mixed models, all 8 test sets measure generalization across the mixture distribution.

\subsection{Metrics}

We report three complementary metrics. We first use Cross‑entropy loss, which is the average negative log‑likelihood per token,
\begin{equation*}
    \mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} \log P\bigl(w_i \,\mid\, w_{<i}\bigr)
\end{equation*}
with lower being better. 

We then use \textbf{Perplexity} as a interpretable transformation, where $\text{PPL}=\exp(\mathcal{L})$. Lower PPL indicates better performance.

We also use \textbf{Relative Spread} to measure cross‑dataset variability:
\begin{equation*}
    \text{Relative Spread}\% = 100\,\frac{\max(\text{PPL}) - \min(\text{PPL})}{\text{mean PPL}}\, ,
\end{equation*}
computed over evaluation perplexities (one per dataset); lower values indicate more consistent generalization.

All metrics are computed on full test sets (no subsampling) with the same sequence length (1,024 tokens) and batch size used during training. Evaluation uses the final checkpoint from training (no checkpoint selection based on validation performance).
