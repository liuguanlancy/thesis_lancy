\documentclass[11pt,a4paper,english,oneside]{book}

%----------------------------------------------------------------------------------------
% THESIS SETTINGS - ADAPT
%----------------------------------------------------------------------------------------
\newif\ifQF % default behaviour is false, so NOT in QF
% \QFtrue % Uncomment if you are in the QF program

\newcommand{\thesis}{Master}

\input{preamble} % Main preamble file

\begin{document}

%----------------------------------------------------------------------------------------
% TITLE PAGE
%----------------------------------------------------------------------------------------
\thispagestyle{empty}
\titleGP

\newpage

\doublespacing
\setcounter{page}{1}
\pagenumbering{Roman}

%----------------------------------------------------------------------------------------
% TASK ASSIGNMENT
%----------------------------------------------------------------------------------------
\section*{Task Assignment}
\thispagestyle{firststyle}
\newpage

%----------------------------------------------------------------------------------------
% EXECUTIVE SUMMARY
%----------------------------------------------------------------------------------------
\section*{Executive Summary}
\thispagestyle{firststyle}

This thesis investigates how different data sources interact during language model pretraining, focusing on financial domain applications. Through comprehensive experiments with 10 pretraining configurations across three model sizes (0.6B, 1.7B, 4B parameters), we demonstrate that in-domain data diversity outweighs high-quality general corpora for specialized domains.

Key findings include: (1) mixed financial datasets achieve best performance (21.55 perplexity at 4B) compared to general text pretraining (31.54 perplexity), (2) learning rate must scale down 50-85\% as model size increases from 0.6B to 4B to avoid training instabilities, (3) datasets smaller than 20K samples exhibit extreme overtraining and require mixing, and (4) WikiText provides minimal benefit for financial tasks despite being high-quality text.

These findings provide practical guidelines for training privacy-preserving financial language models on local devices while contributing generalizable insights on hyperparameter scaling and data mixture strategies for 0.6B-4B parameter models.

\newpage

%----------------------------------------------------------------------------------------
% TABLE OF CONTENTS
%----------------------------------------------------------------------------------------
\tableofcontents
\listoffigures
\listoftables

\newpage
\pagenumbering{arabic}

%----------------------------------------------------------------------------------------
% MAIN CHAPTERS
%----------------------------------------------------------------------------------------

\input{chapters/chapter1_introduction}
\input{chapters/chapter2_background}
\input{chapters/chapter3_methodology}
\input{chapters/chapter4_results}
\input{chapters/chapter5_discussion}
\input{chapters/chapter6_conclusion}

%----------------------------------------------------------------------------------------
% APPENDIX
%----------------------------------------------------------------------------------------

\appendix
\begingroup
\makeatletter
\let\ps@plain\ps@empty
\appendixpage
\makeatother
\endgroup
\noappendicestocpagenum
\addappheadtotoc

\renewcommand{\theequation}{A.\arabic{equation}}

\chapter{Experimental Details\label{chp:experimental_details}}

\section{Complete Hyperparameter Tables}

\section{Additional Results Tables}

\section{Dataset Preprocessing Details}

%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
\printbibliography

\newpage

%----------------------------------------------------------------------------------------
% DECLARATION
%----------------------------------------------------------------------------------------
\thispagestyle{firststyle}

\section*{Eidesstattliche Erklärung}
Der/Die Verfasser/in erklärt an Eides statt, dass er/sie die vorliegende Arbeit selbständig, ohne fremde Hilfe und ohne Benutzung anderer als die angegebenen Hilfsmittel angefertigt hat. Die aus fremden Quellen (einschliesslich elektronischer Quellen) direkt oder indirekt übernommenen Gedanken sind ausnahmslos als solche kenntlich gemacht. Die Arbeit ist in gleicher oder ähnlicher Form oder auszugsweise im Rahmen einer anderen Prüfung noch nicht vorgelegt worden.\\[2cm]
\dotbox{Ort, Datum} \hfill \dotbox{Unterschrift des/der Verfassers/in}

\end{document}