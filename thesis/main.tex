\documentclass[11pt,a4paper,english,oneside]{book}

%----------------------------------------------------------------------------------------
% THESIS SETTINGS - ADAPT
%----------------------------------------------------------------------------------------
\newif\ifQF % default behaviour is false, so NOT in QF
% \QFtrue % Uncomment if you are in the QF program

\newcommand{\thesis}{Master}

\input{preamble} % Main preamble file

\begin{document}

%----------------------------------------------------------------------------------------
% TITLE PAGE
%----------------------------------------------------------------------------------------
\thispagestyle{empty}
\titleGP

\newpage

% \doublespacing
\setcounter{page}{1}
\pagenumbering{Roman}

%----------------------------------------------------------------------------------------
% ABSTRACT
%----------------------------------------------------------------------------------------
\section*{Abstract}
\thispagestyle{firststyle}

We present a compute-normalized study of pretraining data composition for financial language models. We adapt the decoder-only Qwen3 Base architecture to a fixed 100M-token budget with heterogeneous financial texts and general texts with a unified eight-dataset evaluation. We further develop systematic comparisons of individual datasets versus mixtures to evaluate optimal pretraining strategies. In particular, we find that \textbf{medium individual datasets (3.6–8.5M tokens) consistently outperform mixtures on both performance and consistency}. FiQA (6.80 ppl, 19\% spread), FinGPT (7.03 ppl, 37\% spread), and Alpaca (8.73 ppl, 11.5\% spread) achieve 2.5–3.2$\times$ better perplexity AND 1.5–4.8$\times$ better cross-dataset consistency than our seven-source financial mixture (21.55 ppl, 55\% spread). This finding challenges conventional wisdom that data diversity improves robustness. This occurs through a three-way interaction: medium datasets achieve optimal epoch counts (12–28 epochs) with format consistency, while large datasets undertrain (<1 epoch) and large mixtures add format conflicts that small models (0.6B–4B) cannot reconcile. Small datasets (<1M tokens) overtrain (143–352 epochs), leading to memorization. WikiText shows competitive performance at small scales (0.6B: 9.68 ppl) but reverse scaling at larger sizes due to training instability.

Our contributions in this work are three-folded:

\begin{itemize}
    \item[a.] We systematically compare individual datasets versus mixtures via token-matched training and unified eight-dataset evaluation, revealing that medium individual datasets (3.6–8.5M tokens) consistently outperform mixtures on both performance and consistency metrics.

    \item[b.] To understand why mixtures fail, we analyze format inconsistency, vocabulary dilution, and multi-task interference effects. We find that focused optimization on single datasets beats diverse mixing—format consistency and concentrated vocabulary exposure outweigh anticipated diversity benefits.

    \item[c.] We establish that data quality and focus matter more than scale: medium datasets (FiQA 3.6M, FinGPT 4.1M, Alpaca 8.5M, SEC 8.1M) substantially outperform large datasets (News 194M, WikiText 124M). This non-monotonic size-performance relationship reflects optimal epoch counts (12–28) combined with format consistency, outperforming both undertrained large datasets (<1 epoch) and overtrained small datasets (143–352 epochs).
\end{itemize}

\newpage

%----------------------------------------------------------------------------------------
% TABLE OF CONTENTS
%----------------------------------------------------------------------------------------
\tableofcontents
\listoffigures
\listoftables

\newpage
\pagenumbering{arabic}

%----------------------------------------------------------------------------------------
% MAIN CHAPTERS
%----------------------------------------------------------------------------------------

\input{chapters/chapter1_introduction}
\input{chapters/chapter2_background}
\input{chapters/chapter3_methodology}
\input{chapters/chapter4_results}
\input{chapters/chapter5_discussion}
\input{chapters/chapter6_conclusion}

%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
\printbibliography

\newpage

%----------------------------------------------------------------------------------------
% DECLARATION
%----------------------------------------------------------------------------------------
\thispagestyle{firststyle}

\section*{Eidesstattliche Erklärung}
Der/Die Verfasser/in erklärt an Eides statt, dass er/sie die vorliegende Arbeit selbständig, ohne fremde Hilfe und ohne Benutzung anderer als die angegebenen Hilfsmittel angefertigt hat. Die aus fremden Quellen (einschliesslich elektronischer Quellen) direkt oder indirekt übernommenen Gedanken sind ausnahmslos als solche kenntlich gemacht. Die Arbeit ist in gleicher oder ähnlicher Form oder auszugsweise im Rahmen einer anderen Prüfung noch nicht vorgelegt worden.\\[2cm]
\dotbox{Ort, Datum} \hfill \dotbox{Unterschrift des/der Verfassers/in}

\end{document}
