\documentclass[11pt,a4paper,english,oneside]{book}

%----------------------------------------------------------------------------------------
% THESIS SETTINGS - ADAPT
%----------------------------------------------------------------------------------------
\newif\ifQF % default behaviour is false, so NOT in QF
% \QFtrue % Uncomment if you are in the QF program

\newcommand{\thesis}{Master}

\input{preamble} % Main preamble file

\begin{document}

%----------------------------------------------------------------------------------------
% TITLE PAGE
%----------------------------------------------------------------------------------------
\thispagestyle{empty}
\titleGP

\newpage

% \doublespacing
\setcounter{page}{1}
\pagenumbering{Roman}

%----------------------------------------------------------------------------------------
% ABSTRACT
%----------------------------------------------------------------------------------------
\section*{Abstract}
\thispagestyle{firststyle}

We present a detailed study of pretraining data composition for financial language models. We adapt the decoder-only Qwen3 Base architecture to a fixed 100M-token budget with heterogeneous financial texts and general texts with a unified eight-dataset evaluation. We further develop systematic comparisons of individual datasets versus mixtures to evaluate optimal pretraining strategies. In particular, we find that \textbf{medium-sized individual datasets (3.6–8.5M tokens) consistently outperform mixtures on both performance and consistency}. FiQA, FinGPT, and Alpaca achieve better perplexity and cross-dataset consistency than our seven-source financial mixture. This finding challenges conventional belief that data diversity improves robustness. This occurs through a three-way interaction: medium datasets achieve optimal epoch counts with format consistency, while large datasets undertrain and large mixtures add format conflicts that small models (0.6B–4B) cannot effectively learn within 100M tokens. Small datasets ($<$1M tokens) overtrain, leading to overfit. WikiText shows competitive performance at small scales but reverse scaling at larger sizes due to training instability.

Our contributions in this work are three-folded. First, we systematically compare individual datasets versus mixtures via token-matched training and unified eight-dataset evaluation, revealing that medium-sized individual datasets (3.6–8.5M tokens) consistently outperform mixtures on both performance and consistency metrics. Second, to understand why mixtures fail, we find that focused optimization on single datasets beats diverse mixing—format mixtures. We argue this is due to small models (0.6B-4B) cannot deal with the heterogeneous nature of complex mixed datasets under a limited token budget (100M). Third, we argue that data quality and focus matter more than scale. Medium datasets (FiQA 3.6M, FinGPT 4.1M, Alpaca 8.5M, SEC 8.1M) substantially outperform large datasets (News 194M, WikiText 124M) as model scales from 0.6B to 4B.

\newpage

%----------------------------------------------------------------------------------------
% TABLE OF CONTENTS
%----------------------------------------------------------------------------------------
\tableofcontents
\listoffigures
\listoftables

\newpage
\pagenumbering{arabic}

%----------------------------------------------------------------------------------------
% MAIN CHAPTERS
%----------------------------------------------------------------------------------------

\input{chapters/chapter1_introduction}
\input{chapters/chapter2_background}
\input{chapters/chapter3_methodology}
\input{chapters/chapter4_results}
\input{chapters/chapter5_discussion}
\input{chapters/chapter6_conclusion}

%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
\printbibliography

\newpage

%----------------------------------------------------------------------------------------
% DECLARATION
%----------------------------------------------------------------------------------------
\thispagestyle{firststyle}

\section*{Eidesstattliche Erklärung}
Der/Die Verfasser/in erklärt an Eides statt, dass er/sie die vorliegende Arbeit selbständig, ohne fremde Hilfe und ohne Benutzung anderer als die angegebenen Hilfsmittel angefertigt hat. Die aus fremden Quellen (einschliesslich elektronischer Quellen) direkt oder indirekt übernommenen Gedanken sind ausnahmslos als solche kenntlich gemacht. Die Arbeit ist in gleicher oder ähnlicher Form oder auszugsweise im Rahmen einer anderen Prüfung noch nicht vorgelegt worden.\\[2cm]
\dotbox{Ort, Datum} \hfill \dotbox{Unterschrift des/der Verfassers/in}

\end{document}
