python train.py --model "Qwen/Qwen3-0.6B-Base" --dataset "gbharti/finance-alpaca" --mode "pretrain" --attn_implementation "flash_attention_2" --max_steps "12207" --use_lora --lora_r "32" --lora_alpha "64" --lora_target_modules "q_proj" "k_proj" "v_proj" "o_proj" --batch_size "2" --max_length "1024" --gradient_accumulation_steps "1" --use_packing --learning_rate "2e-5" --lr_scheduler_type "cosine" --warmup_steps "1220" --weight_decay "0.01" --bf16 --eval_steps "1000" --eval_max_batches "2" --eval_on_start --save_steps "1000" --save_total_limit "3" --save_strategy "steps" --resume_from_checkpoint "./runs_buggy/phase2b_financial_qwen3_0.6b/alpaca/checkpoints/checkpoint-2000" --output_dir "./runs/phase2b_financial_qwen3_0.6b/alpaca_resumed"