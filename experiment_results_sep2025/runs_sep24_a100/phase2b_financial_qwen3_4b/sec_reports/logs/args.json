{
  "timestamp": "2025-09-24T01:27:39.269592",
  "output_dir": "./runs/phase2b_financial_qwen3_4b/sec_reports",
  "args": {
    "model": "Qwen/Qwen3-4B-Base",
    "dataset": "stanfordnlp/imdb",
    "dataset_config": null,
    "datasets": [
      "virattt/financial-qa-10K",
      "FinGPT/fingpt-sentiment-train",
      "gbharti/finance-alpaca",
      "LLukas22/fiqa",
      "zeroshot/twitter-financial-news-sentiment",
      "JanosAudran/financial-reports-sec",
      "ashraq/financial-news-articles",
      "wikitext"
    ],
    "dataset_configs": [
      null,
      null,
      null,
      null,
      null,
      "small_lite",
      null,
      "wikitext-103-v1"
    ],
    "mixture_rates": [
      0.0,
      0.0,
      0.0,
      0.0,
      0.0,
      1.0,
      0.0,
      0.0
    ],
    "task": "auto",
    "mode": "pretrain",
    "max_length": 1024,
    "batch_size": 2,
    "device": "auto",
    "multi_gpu": false,
    "ddp_backend": "nccl",
    "gradient_accumulation_steps": 4,
    "save_steps": 1000,
    "save_total_limit": 2,
    "save_strategy": "steps",
    "output_dir": "./runs/phase2b_financial_qwen3_4b/sec_reports",
    "eval_steps": 1000,
    "eval_max_batches": 100,
    "logging_steps": null,
    "eval_strategy": "steps",
    "eval_on_start": true,
    "load_best_model_at_end": false,
    "metric_for_best_model": "eval_loss",
    "greater_is_better": false,
    "resume_from_checkpoint": null,
    "num_train_epochs": 1,
    "max_steps": 12207,
    "learning_rate": 2e-05,
    "lr_scheduler_type": "cosine",
    "warmup_steps": 1220,
    "warmup_ratio": 0.0,
    "weight_decay": 0.01,
    "adam_beta1": 0.9,
    "adam_beta2": 0.999,
    "adam_epsilon": 1e-08,
    "max_grad_norm": 1.0,
    "fp16": false,
    "bf16": true,
    "no_mixed_precision": false,
    "reward_model": null,
    "grpo_beta": 0.1,
    "grpo_group_size": 2,
    "max_prompt_length": 512,
    "max_completion_length": 512,
    "rl_learning_rate": 1e-06,
    "rl_warmup_steps": 100,
    "attn_implementation": "flash_attention_2",
    "use_flash_attention": false,
    "moe_load_in_4bit": false,
    "moe_load_in_8bit": false,
    "moe_expert_parallel": false,
    "use_lora": true,
    "lora_r": 32,
    "lora_alpha": 64,
    "lora_dropout": 0.1,
    "lora_target_modules": [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj"
    ],
    "use_packing": true,
    "packing_max_length": null,
    "return_position_ids": true,
    "disable_mps_fix": false,
    "separate_mixture_eval": true,
    "log_eval_spread": true,
    "actual_attn_implementation": "flash_attention_2"
  }
}